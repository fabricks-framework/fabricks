{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fabricks User Guide","text":"<p>Fabricks is a pragmatic framework to build Databricks Lakehouse pipelines using YAML for orchestration and SQL for transformations. It standardizes jobs, steps, schedules, CDC, and checks while keeping development SQL-first.</p>"},{"location":"#steps-overview","title":"Steps Overview","text":"<p>Fabricks organizes your Lakehouse into clear layers. Each step has a dedicated reference with modes, options, and examples.</p>"},{"location":"#bronze","title":"Bronze","text":"<p>Raw ingestion from source systems (files, streams, existing tables). Keep logic light; land data for downstream processing.</p> <ul> <li>Typical modes: <code>memory</code>, <code>append</code>, <code>register</code></li> <li>Focus: lightweight parsing/landing; no business logic</li> <li>Output: raw tables or temporary views</li> </ul> <p>Read the full reference: Bronze Step</p>"},{"location":"#silver","title":"Silver","text":"<p>Standardize, clean, and enrich data; optionally apply CDC (SCD1/SCD2). Produces conformed datasets and convenience views.</p> <ul> <li>Typical modes: <code>memory</code>, <code>append</code>, <code>latest</code>, <code>update</code>, <code>combine</code></li> <li>CDC: <code>nocdc</code>, <code>scd1</code>, <code>scd2</code> with built-in helpers and views</li> <li>Output: conformed tables and curated views</li> </ul> <p>Read the full reference: Silver Step</p>"},{"location":"#gold","title":"Gold","text":"<p>Curated business models for analytics and reporting; dimensional or mart-style outputs. Can also <code>invoke</code> notebooks when needed.</p> <ul> <li>Typical modes: <code>memory</code>, <code>append</code>, <code>complete</code>, <code>update</code>, <code>invoke</code> (notebooks)</li> <li>Focus: dimensional models, marts, KPI-ready data</li> <li>Output: business-consumption tables and views</li> </ul> <p>Read the full reference: Gold Step</p>"},{"location":"#where-to-configure","title":"Where to Configure","text":"<ul> <li>Project configuration, schedules, and runtime structure: Runtime</li> <li>Data quality and rollback behavior: Checks &amp; Data Quality</li> <li>Table properties, clustering, and layout: Table Options</li> <li>Custom logic and reusable SQL assets: Extenders, UDFs &amp; Parsers</li> <li>Change Data Capture (CDC): CDC</li> </ul>"},{"location":"helpers/init/","title":"Initialize Fabricks","text":"<p>This helper explains how to initialize Fabricks on a Databricks cluster or local environment by:</p> <ul> <li>Installing the Fabricks library</li> <li>Pointing Fabricks to your runtime</li> <li>Running the <code>armageddon</code> script to bootstrap metadata and objects</li> </ul>"},{"location":"helpers/init/#1-install-fabricks","title":"1) Install Fabricks","text":"<p>You need the Fabricks package available on your cluster or local environment.</p> <ul> <li>Databricks cluster (recommended)</li> <li>Libraries \u2192 Install new \u2192 Python PyPI \u2192 <code>fabricks</code> (or install a wheel/artifact you build)</li> <li> <p>Alternatively, attach a workspace library artifact built from this repository</p> </li> <li> <p>Local development (optional)</p> </li> <li><code>pip install fabricks</code></li> <li>or from source (for development): <code>pip install -e .[dev,test]</code></li> </ul> <p>Python &gt;=3.9,&lt;4 is recommended; align with your Databricks LTS runtime.</p>"},{"location":"helpers/init/#2-point-fabricks-to-your-runtime","title":"2) Point Fabricks to your runtime","text":"<p>Fabricks discovers its runtime via either environment variables or <code>[tool.fabricks]</code> in your <code>pyproject.toml</code>. The core lookup logic is implemented in <code>fabricks/context/runtime.py</code>.</p> <p>Option A: Configure via <code>pyproject.toml</code> (preferred for repo-managed projects): <pre><code>[tool.fabricks]\nruntime = \"path/to/your/runtime\"                   # e.g., tests/integration/runtime or examples/runtime\nnotebooks = \"fabricks/api/notebooks\"               # optional: helpers shipped with Fabricks\njob_config_from_yaml = true                        # optional\nloglevel = \"info\"                                  # optional: `DEBUG`|`INFO`|`WARNING`|`ERROR`|`CRITICAL`\ndebugmode = false                                  # optional\nconfig = \"path/to/your/runtime/fabricks/conf.fabricks.yml\"  # main runtime YAML\n</code></pre></p> <p>Option B: Configure via environment variables (useful on clusters):</p> <p><pre><code># FABRICKS_RUNTIME: path to your runtime (jobs, SQL, configs)\n# FABRICKS_CONFIG: full path to your main conf.fabricks.yml (if not set, Fabricks tries to infer a conf.uc.&lt;orgId&gt;.yml)\n# FABRICKS_NOTEBOOKS: optional helper notebook path\n# FABRICKS_IS_JOB_CONFIG_FROM_YAML, FABRICKS_LOGLEVEL, FABRICKS_IS_DEBUGMODE: optional toggles\n```Example\\non Databricks (Cluster \u2192 Configuration \u2192 Advanced options \u2192 Environment variables):\n</code></pre> FABRICKS_RUNTIME=/Workspace/Repos/your/repo/examples/runtime FABRICKS_CONFIG=/Workspace/Repos/your/repo/examples/runtime/fabricks/conf.fabricks.yml FABRICKS_LOGLEVEL=INFO ```You\\ncan also set env vars in a notebook before importing Fabricks: <pre><code>import os\nos.environ[\"FABRICKS_RUNTIME\"] = \"/Workspace/Repos/your/repo/examples/runtime\"\nos.environ[\"FABRICKS_CONFIG\"] = \"/Workspace/Repos/your/repo/examples/runtime/fabricks/conf.fabricks.yml\"\n# Optional:\n# os.environ[\"FABRICKS_LOGLEVEL\"] = \"INFO\"\n</code></pre></p>"},{"location":"helpers/init/#3-run-armageddon","title":"3) Run <code>armageddon</code>","text":"<p><code>armageddon</code> performs a one-time setup based on your runtime configuration (e.g., preparing databases/metadata, registering views).</p> <p>Import and call: <pre><code># Databricks or local\nfrom fabricks.core.scripts.`armageddon` import `armageddon`\n\n# You may pass one or more steps (`bronze`, `silver`, `gold`, `semantic`, `transf`, ...)\n# Examples:\n`armageddon`(steps=\"gold\")                      # single step\n`armageddon`(steps=[\"bronze\", \"silver\", \"gold\"])  # multiple steps\n`armageddon`(steps=None)                        # default behavior, follow runtime config\n</code></pre></p>"},{"location":"helpers/init/#example-databricks-notebook-initialize","title":"Example: Databricks Notebook: Initialize","text":"<p>Create a new notebook (Python) named initialize and include:</p> <p>```python</p>"},{"location":"helpers/init/#optional-set-env-vars-if-not-using-pyprojecttoml","title":"(Optional) set env vars if not using pyproject.toml","text":""},{"location":"helpers/init/#import-os","title":"import os","text":""},{"location":"helpers/init/#osenvironfabricks_runtime-workspacereposyourrepoexamplesruntime","title":"os.environ[\"FABRICKS_RUNTIME\"] = \"/Workspace/Repos/your/repo/examples/runtime\"","text":""},{"location":"helpers/init/#osenvironfabricks_config-workspacereposyourrepoexamplesruntimefabricksconffabricksyml","title":"os.environ[\"FABRICKS_CONFIG\"] = \"/Workspace/Repos/your/repo/examples/runtime/fabricks/conf.fabricks.yml\"","text":""},{"location":"helpers/init/#osenvironfabricks_loglevel-info","title":"os.environ[\"FABRICKS_LOGLEVEL\"] = \"INFO\"","text":"<p>from fabricks.core.scripts.<code>armageddon</code> import <code>armageddon</code></p>"},{"location":"helpers/init/#run-for-all-default-steps-from-your-runtime-config","title":"Run for all default steps from your runtime config:","text":"<p><code>armageddon</code>() ```Attach\\nthe Fabricks library to the cluster before running the notebook.</p>"},{"location":"helpers/init/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing env/config:</li> <li><code>ValueError</code>: Must have at least a <code>pyproject.toml</code> or set <code>FABRICKS_RUNTIME</code></li> <li> <p>Fix by setting <code>FABRICKS_RUNTIME</code> or adding <code>[tool.fabricks]</code> to <code>pyproject.toml</code></p> </li> <li> <p>Unity Catalog:</p> </li> <li> <p>Ensure <code>options.unity_catalog</code> is true and <code>options.catalog</code> is set in <code>conf.fabricks.yml</code></p> </li> <li> <p>Paths and storage:</p> </li> <li> <p><code>conf.fabricks.yml</code> must define <code>path_options.storage</code> and per-step runtime/storage paths; Fabricks uses these to resolve <code>PATHS_RUNTIME</code> and <code>PATHS_STORAGE</code></p> </li> <li> <p>Logging:</p> </li> <li>Set <code>FABRICKS_LOGLEVEL</code> or <code>tool.fabricks.loglevel</code> to control verbosity</li> </ul>"},{"location":"helpers/init/#related-topics","title":"Related topics","text":"<ul> <li>Runtime configuration: <code>../helpers/runtime.md</code></li> <li>Step Helper: <code>./step.md</code></li> <li>Job Helper: <code>./job.md</code></li> </ul>"},{"location":"helpers/job/","title":"Job Helper","text":"<p>This helper describes a widget-driven approach to manage a Fabricks job.</p> <p>Core imports used: <pre><code>from fabricks.api import get_job\n</code></pre></p>"},{"location":"helpers/job/#typical-usage","title":"Typical usage","text":"<p>1) Resolve the job: <pre><code>from fabricks.api import get_job\n\nj = get_job(job=\"gold.sales.orders\")  # or pass step/topic/item explicitly\n</code></pre></p> <p>2) Run actions: <pre><code># Examples:\nj.run()\n\n# Other common operations:\nj.update_schema()\nj.overwrite_schema()\nj.`register`()\n\nj.create()\n\n# Cleanup if required:\n# j.truncate()\n# j.drop()\n</code></pre></p> <p>3) Gold jobs: <pre><code># If your job is gold, ensure UDFs are registered before actions:\nif getattr(j, \"expand\", None) == \"gold\":\n    j.register_udfs()\nj.run()\n</code></pre></p>"},{"location":"helpers/job/#helper-notebook","title":"Helper Notebook","text":"<pre><code># Databricks notebook source\nimport os\nfrom multiprocessing import Pool\nfrom typing import Callable, List\n\nfrom databricks.sdk.runtime import dbutils, spark\n\n# COMMAND ----------\n\ndbutils.widgets.text(\"jobs\", \"---\", label=\"6 - Job(s)\")\ndbutils.widgets.multiselect(\n    \"actions\",\n    \"run\",\n    [\n        \"drop\",                   -- Drop the job\n        \"`register`\",               -- Register the table\n        \"create\",                 -- Create the job\n        \"truncate\",               -- Truncate the table\n        \"run\",                    -- Run the job \n        \"for-each-run\",           -- Run the job (excluding the invoker(s) and the check(s))\n        \"overwrite\",              -- Overwrite the schema and run the job\n        \"pre-run-`invoke`\",         -- Run the pre-run invoker(s)\n        \"pre-run-check\",          -- Run the pre-run check(s)\n        \"post-run-`invoke`\",        -- Run the post-run invoker(s)\n        \"post-run-check\",         -- Run the post-run check(s)\n        \"`update`-schema\",          -- Update the schema\n        \"overwrite-schema\",       -- Overwrite the schema\n    ],\n    label=\"5 - Action(s)\"\n)\ndbutils.widgets.dropdown(\"loglevel\", \"info\", [\"debug\", \"info\", \"error\", \"critical\"], label=\"3 - Log Level\")\ndbutils.widgets.dropdown(\"chdir\", \"False\", [\"True\", \"False\"], label=\"1 - Change Directory\")\ndbutils.widgets.dropdown(\"config_from_yaml\", \"True\", [\"True\", \"False\"], label=\"2 - Use YAML\")\ndbutils.widgets.dropdown(\"debugmode\", \"False\", [\"True\", \"False\"], label=\"4 - Debug Mode\")\n\n# COMMAND ----------\n\nif dbutils.widgets.get(\"chdir\").lower() == \"true\":\n    user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()  # type: ignore\n    try:\n        os.chdir(f\"/Workspace/Users/{user}/Fabricks.Runtime\")\n    except FileNotFoundError:\n        os.chdir(f\"/Workspace/Users/{user}/runtime\")\n\n# COMMAND ----------\n\nif dbutils.widgets.get(\"config_from_yaml\").lower() == \"true\":\n    os.environ[\"FABRICKS_IS_JOB_CONFIG_FROM_YAML\"] = \"1\"\nelse:\n    os.environ[\"FABRICKS_IS_JOB_CONFIG_FROM_YAML\"] = \"0\"\n\n# COMMAND ----------\n\nif dbutils.widgets.get(\"debugmode\").lower() == \"true\":\n    os.environ[\"FABRICKS_IS_DEBUGMODE\"] = \"1\"\nelse:\n    os.environ[\"FABRICKS_IS_DEBUGMODE\"] = \"0\"\n\n# COMMAND ----------\n\nloglevel = dbutils.widgets.get(\"loglevel\")\nif loglevel == \"debug\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"DEBUG\"\nelif loglevel == \"info\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"INFO\"\nelif loglevel == \"error\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"ERROR\"\nelif loglevel == \"critical\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"CRITICAL\"\n\n# COMMAND ----------\n\nfrom fabricks.api import get_job  # noqa: E402\nfrom fabricks.api.log import DEFAULT_LOGGER  # noqa: E402\nfrom fabricks.core.jobs import Gold  # noqa: E402\n\n# COMMAND ----------\n\nactions = dbutils.widgets.get(\"actions\").split(\",\")\nactions = [a.strip() for a in actions]\n\n# COMMAND ----------`n`ndef do(job: str) -&gt; None:\n    j = get_job(job=job)\n    todos: dict[str, Callable] = {}\n\n    if j.expand == \"gold\":\n        assert isinstance(j, Gold)\n        j.register_udfs()\n\n    if \"drop\" in actions:\n        todos[\"drop\"] = j.drop\n\n    if \"`register`\" in actions:\n        todos[\"`register`\"] = j.`register`\n\n    if \"pre-run-`invoke`\" in actions:\n        todos[\"pre-run-`invoke`\"] = j.invoke_pre_run\n\n    if \"pre-run-check\" in actions:\n        todos[\"pre-run-check\"] = j.check_pre_run\n\n    if \"create\" in actions:\n        todos[\"create\"] = j.create\n\n    if \"`update`-schema\" in actions:\n        todos[\"`update`-schema\"] = j.update_schema\n\n    if \"overwrite-schema\" in actions:\n        todos[\"overwrite-schema\"] = j.overwrite_schema\n\n    if \"truncate\" in actions:\n        todos[\"truncate\"] = j.truncate\n\n    if \"run\" in actions:\n        todos[\"run\"] = j.run\n\n    if \"for-each-run\" in actions:\n        todos[\"for-each-run\"] = j.for_each_run\n\n    if \"overwrite\" in actions:\n        todos[\"overwrite\"] = j.overwrite\n\n    if \"post-run-check\" in actions:\n        todos[\"post-run-check\"] = j.check_post_run\n\n    if \"post-run-`invoke`\" in actions:\n        todos[\"post-run-`invoke`\"] = j.invoke_post_run\n\n    for key, func in todos.items():\n        func()\n\n# COMMAND ----------\n\nactions = [s.strip() for s in dbutils.widgets.get(\"actions\").split(\",\")]\njobs = [s.strip() for s in dbutils.widgets.get(\"jobs\").split(\",\")]\njobs = [[j.strip() for j in job.split(\"//\")] if \"//\" in job else job for job in jobs]\n\nfor job in jobs:\n    DEFAULT_LOGGER.warning(\", \".join(actions), extra={\"label\": job})\n\n# COMMAND ----------\n\nfor job in jobs:\n    if isinstance(job, List):\n        with Pool() as pool:\n            results = pool.map(do, job)\n    else:\n        do(job)\n\n# COMMAND ----------\n\ndbutils.notebook.exit(\"exit (0)\")  # type: ignore\n</code></pre>"},{"location":"helpers/job/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze | Silver | Gold</li> <li>Runtime overview and sample runtime: Runtime</li> <li>Checks &amp; Data Quality: Checks and Data Quality</li> <li>Table options and storage layout: Table Options</li> <li>Extenders, UDFs &amp; Views: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"helpers/runtime/","title":"Runtime Configuration","text":"<p>The Fabricks runtime is the folder where your Lakehouse project lives (configs, SQL, jobs, UDFs, extenders, views). This page explains how to point Fabricks to your runtime, how to structure it, and how to configure schedules and step paths.</p>"},{"location":"helpers/runtime/#minimal-setup-and-verification","title":"Minimal setup and verification","text":"<p>This quick path gets a working runtime running end-to-end using the sample included with Fabricks.</p> <p>1) Prepare pyproject configuration - Ensure your <code>pyproject.toml</code> contains a <code>[tool.fabricks]</code> block pointing to the sample runtime:   <pre><code>[tool.fabricks]\nruntime = \"examples/runtime\"\nnotebooks = \"fabricks/api/notebooks\"\njob_config_from_yaml = true\nloglevel = \"info\"\ndebugmode = false\nconfig = \"examples/runtime/fabricks/conf.fabricks.yml\"\n</code></pre></p> <p>2) Inspect the sample runtime - Directory: <code>examples/runtime</code> (see structure in \u00e2\u20ac\u0153Sample runtime\u00e2\u20ac\u009d below). - It contains a minimal schedule and a Gold <code>hello_world.sql</code> full-refresh job.</p> <p>3) Run a schedule - Use your Databricks bundle/job or orchestration to run the schedule named <code>example</code> from <code>examples/runtime/fabricks/schedules/schedule.yml</code>. - You can also run step-by-step via the shipped notebooks referenced by <code>notebooks</code>.</p> <p>Expected outputs - Tables/views:   - A Gold table for the <code>hello_world</code> job (full refresh).   - If using <code>memory</code> mode jobs, temporary views are registered for downstream steps. - Logs:   - A completion line indicating job success; warnings/errors surfaced from checks/contracts if configured. - Data quality (if enabled):   - Built-in bound violations or contract <code>__action = 'fail'</code> causes a non\u00e2\u20ac'zero exit and, for physical tables, an automatic rollback to the last successful version.   - Contract <code>__action = 'warning'</code> logs the message and the run continues.</p> <p>Tip: If your environment uses different storage locations or workspace setup, adjust <code>path_options</code> and <code>spark_options</code> in the runtime YAML before running.</p>"},{"location":"helpers/runtime/#pointing-to-your-runtime","title":"Pointing to your runtime","text":"<p>Configure Fabricks in your project's <code>pyproject.toml</code>:</p> <pre><code>[tool.fabricks]\nruntime = \"tests/integration/runtime\"          # Path to your runtime (jobs, SQL, configs)\nnotebooks = \"fabricks/api/notebooks\"           # Notebook helpers shipped with Fabricks\njob_config_from_yaml = true                     # Enable YAML job config\nloglevel = \"info\"\ndebugmode = false\nconfig = \"tests/integration/runtime/fabricks/conf.uc.fabricks.yml\"  # Main runtime config\n</code></pre> <ul> <li>runtime: path to your project's runtime directory</li> <li>config: path to your main runtime YAML configuration (see below)</li> <li>notebooks: optional helpers used by shipped notebooks</li> <li>job_config_from_yaml: enable loading jobs from YAML files</li> <li>loglevel/debugmode: control logging verbosity</li> </ul>"},{"location":"helpers/runtime/#main-runtime-config-yaml","title":"Main runtime config (YAML)","text":"<p>Define project-level options, step defaults, and step path mappings in <code>fabricks/conf.fabricks.yml</code>:</p> <p><pre><code>name: MyFabricksProject\noptions:\n  secret_scope: my_secret_scope\n  timeouts:\n    step: 3600\n    job: 3600\n    pre_run: 3600\n    post_run: 3600\n  workers: 4\npath_options:\n  storage: /mnt/data\nspark_options:\n  sql:\n    spark.sql.parquet.compression.codec: zstd\n\nbronze:\n  - name: bronze\n    path_options:\n      runtime: src/steps/bronze\n      storage: abfss://bronze@youraccount.blob.core.windows.net\n\nsilver:\n  - name: silver\n    path_options:\n      runtime: src/steps/silver\n      storage: abfss://silver@youraccount.blob.core.windows.net\n\ngold:\n  - name: transf\n    path_options:\n      runtime: src/steps/transf\n      storage: abfss://transf@youraccount.blob.core.windows.net\n  - name: gold\n    path_options:\n      runtime: src/steps/gold\n      storage: abfss://gold@youraccount.blob.core.windows.net\n```Key\\nconcepts:\n- options: global project config (secrets, timeouts, worker count)\n- path_options: shared storage/config paths\n- spark_options: default Spark SQL options applied for jobs\n- step sections (bronze/silver/gold/...): list of step instances with their runtime and storage paths\n\n## Configuration precedence\n\nMultiple layers of configuration can influence a run. When the same setting appears in several places, the following order applies (top wins):\n\n1. Job-level options in YAML (per job)\n2. Step-level defaults in runtime YAML (per step instance under bronze/silver/gold/semantic)\n3. Global options in runtime YAML (`options`, `spark_options`, `path_options`)\n4. Project-level defaults in `pyproject.toml` `[tool.fabricks]`\n5. Internal defaults (shipped with Fabricks)\n\nNotes\n- `spark_options` are merged; job-level `spark_options` override keys from step/global levels.\n- `path_options` (e.g., `storage`, `runtime`) can be set globally, then refined per step instance.\n- Some options only make sense at specific levels (e.g., `check_options` at job level).\n- For CDC and write behavior, job `options` (e.g., `mode`, `change_data_capture`) take precedence over step defaults.\n\n## Schedules\n\nSchedules group jobs and define step order. Place schedules in your runtime (commonly under `fabricks/schedules/`):\n\n```yaml\n- schedule:\n    name: test\n    options:\n      tag: test\n      steps: [bronze, silver, transf, gold, semantic]\n      variables:\n        var1: 1\n        var2: 2\n```Pass\\nthe schedule name when running the shipped notebooks or the Databricks bundle job.\n\n## Typical runtime structure\n\nA minimal runtime can look like:\n</code></pre> fabricks/   conf.fabricks.yml   schedules/     schedule.yml bronze/   _config.example.yml silver/   _config.example.yml gold/   gold/     _config.example.yml     hello_world.sql semantic/   _config.example.yml <pre><code>- bronze/silver/gold/semantic: step folders with YAML configs and SQL\n- fabricks/conf.fabricks.yml: main runtime config for paths and defaults\n- fabricks/schedules/: defines one or more schedules for execution\n\n## Sample runtime\n\nThis section contains a minimal, copyable Fabricks runtime you can use to get started quickly. It mirrors the structure used in the integration tests and includes example configs and a working Gold job (`hello_world.sql`).\n\nWhat this is\n- A self-contained runtime layout showing how to organize steps (bronze, silver, gold, semantic)\n- Example YAML configuration files per step\n- A minimal schedule and config to run a simple job end-to-end\n\nDirectory overview\n</code></pre> examples/runtime/   fabricks/     conf.fabricks.yml     schedules/       schedule.yml   bronze/     _config.example.yml   silver/     _config.example.yml   gold/     gold/       _config.example.yml       hello_world.sql   semantic/     _config.example.yml <code>``Key\\nfiles and purpose -</code>fabricks/conf.fabricks.yml<code>: project-level configuration (secret scope, timeouts, workers, storage paths, schedules path) -</code>fabricks/schedules/schedule.yml<code>: minimal schedule to run the gold step -</code>gold/gold/_config.example.yml<code>: defines a simple Gold job -</code>gold/gold/hello_world.sql<code>: example SQL for a Gold job (full refresh mode) -</code>bronze/_config.example.yml<code>,</code>silver/_config.example.yml<code>,</code>semantic/_config.example.yml`: example step configurations</p> <p>How to use this sample 1) Point <code>tool.fabricks.runtime</code> to this folder and set <code>config</code> to <code>fabricks/conf.fabricks.yml</code> in your <code>pyproject.toml</code>. 2) Run the Databricks bundle job with <code>schedule: example</code> (see the main documentation for bundle details). 3) Inspect the materialized outputs, logs, and table/view artifacts.</p>"},{"location":"helpers/runtime/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze | Silver | Gold</li> <li>Data quality checks and contracts: Checks &amp; Data Quality</li> <li>Table properties and physical layout: Table Options</li> <li>Custom logic integration: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"helpers/schedule/","title":"Schedules","text":"<p>Views provide a simple, declarative way to define a set of jobs to run in a schedule. Instead of listing jobs manually, you create a SQL view that filters the canonical <code>fabricks.jobs</code> table, and then reference that view by name from a schedule.</p> <p>Typical use cases: - Run all jobs for a given domain/topic (e.g., monarch) - Run only certain steps (e.g., all Gold jobs) - Run a curated subset of jobs for ad\u00e2\u20ac'hoc backfills or smoke tests</p>"},{"location":"helpers/schedule/#types-of-views-in-fabricks","title":"Types of views in Fabricks","text":"<ul> <li>Data views (user\u00e2\u20ac'authored):</li> <li>You place <code>.sql</code> files under your runtime <code>views</code> path (see below).</li> <li>Each file defines one SQL view created under the <code>fabricks</code> schema.</li> <li> <p>These are typically simple filters over <code>fabricks.jobs</code>.</p> </li> <li> <p>Schedule views (framework\u00e2\u20ac'generated):</p> </li> <li>For each <code>schedule</code> defined in your runtime YAML, Fabricks can generate a companion view named <code>fabricks.&lt;schedule_name&gt;_schedule</code>.</li> <li>A schedule view selects <code>j.*</code> from <code>fabricks.jobs</code> and applies optional filters from the schedule\u00e2\u20ac\u2122s options (<code>view</code>, <code>steps</code>, <code>tag</code>), excluding manual jobs.</li> </ul> <p>Both kinds of views are useful: data views define job subsets; schedule views expose the final, resolved set for each schedule.</p>"},{"location":"helpers/schedule/#how-it-works","title":"How it works","text":"<ul> <li>Source of truth: Fabricks maintains <code>fabricks.jobs</code> with one row per job.</li> <li>Data views: Your SQL selects a subset from <code>fabricks.jobs</code> (recommended: <code>select *</code>).</li> <li>Schedules:</li> <li>In <code>schedule.yml</code>, you can set <code>options.view: &lt;data_view_name&gt;</code>.</li> <li>Fabricks resolves the schedule\u00e2\u20ac\u2122s membership via the data view (and optional <code>steps</code> / <code>tag</code> filters).</li> <li>Fabricks can also materialize a schedule view <code>fabricks.&lt;schedule_name&gt;_schedule</code> for inspection and tooling.</li> </ul>"},{"location":"helpers/schedule/#where-to-put-data-view-sql-files","title":"Where to put data view SQL files","text":"<p>The runtime configuration defines where Fabricks looks for view SQL files:</p> <p>```yaml title:conf.fabricks.yml (excerpt) path_options:   views: fabricks/views <pre><code>- Place your `.sql` files under the runtime `views` directory (e.g., `.../fabricks/views/`).\n- Each file defines one view; the file name (without `.sql`) becomes the view name.\n- During initialization, Fabricks will create or replace these as SQL views in the `fabricks` schema.\n\n---\n\n## Minimal data view example\n\nCreate a file `monarch.sql` in your runtime `views` directory:\n\n```sql title:monarch.sql\n-- Select all jobs with a specific topic\nselect *\nfrom fabricks.jobs j\nwhere j.topic = 'monarch'\n```This\\nwill create a view called `fabricks.monarch`. You can then reference this view in a schedule.\n\n---\n\n## Using a data view in a schedule\n\n```yaml\n- schedule:\n    name: run-monarch\n    options:\n      view: monarch            # join to fabricks.monarch on job_id\n      steps: [silver, gold]    # optional: restrict to these steps\n      tag: nightly             # optional: only jobs having this tag\n      # variables, timeouts, etc. can still be provided as needed\n</code></pre></p> <p>Behavior: - Fabricks loads all jobs returned by <code>fabricks.monarch</code> and further filters by <code>steps</code> and <code>tag</code> if provided. - Schedule\u00e2\u20ac'level options like <code>variables</code>, <code>timeouts</code>, etc., still apply to execution. - In most cases the view alone defines the job set; <code>steps</code>/<code>tag</code> refine it.</p>"},{"location":"helpers/schedule/#schedule-views-generated","title":"Schedule views (generated)","text":"<p>For each schedule, Fabricks can generate a companion SQL view named <code>fabricks.&lt;schedule_name&gt;_schedule</code> that reflects the resolved membership. The core SQL shape (simplified) is:</p> <pre><code>create or replace view fabricks.&lt;schedule_name&gt;_schedule as\nselect j.*\nfrom fabricks.jobs j\n/* optional: if options.view is provided */\ninner join fabricks.&lt;options.view&gt; v on j.job_id = v.job_id\nwhere true\n  /* optional: if options.steps is provided */\n  and j.step in ('bronze','silver','gold' /* ... */)\n  /* optional: if options.tag is provided (array of tags on the job) */\n  and array_contains(j.tags, '&lt;tag&gt;')\n  and j.type not in ('manual');\n</code></pre> <p>Notes: - The inner join requires the data view to expose <code>job_id</code>. Returning <code>j.*</code> from <code>fabricks.jobs</code> guarantees this. - Manual jobs are excluded from schedule views by design.</p> <p>Quick validation: <pre><code>select count(*) from fabricks.run-monarch_schedule;         -- schedule membership size\nselect step, topic, item from fabricks.run-monarch_schedule limit 20;\n</code></pre></p>"},{"location":"helpers/schedule/#registration-and-lifecycle","title":"Registration and lifecycle","text":"<p>You can (re)create both data views and schedule views programmatically.</p> <ul> <li> <p>Data views (from <code>.sql</code> files under <code>path_options.views</code>):   <pre><code>from fabricks.core.views import (\n    create_or_replace_view,   # single data view by name\n    create_or_replace_views,  # all data views under PATH_VIEWS\n)\n\ncreate_or_replace_view(\"monarch\")\ncreate_or_replace_views()\n</code></pre></p> </li> <li> <p>Schedule views (one per schedule defined in runtime):   <pre><code>from fabricks.core.schedules import (\n    create_or_replace_view as create_or_replace_schedule_view,  # single schedule by name\n    create_or_replace_views as create_or_replace_schedule_views # all schedules\n)\n\ncreate_or_replace_schedule_view(\"run-monarch\")\ncreate_or_replace_schedule_views()\n</code></pre></p> </li> <li> <p>Initialization path:</p> </li> <li>The bootstrap script (\u00e2\u20ac\u0153armageddon\u00e2\u20ac\u009d) calls both data view and schedule view builders so your environment is in sync with runtime configs and SQL view files.</li> </ul>"},{"location":"helpers/schedule/#additional-data-view-examples","title":"Additional data view examples","text":"<p>Filter by step: <code>sql title:gold_only.sql select * from fabricks.jobs j where j.step = 'gold'</code>Filter\\nby a set of topics: <code>sql title:core_topics.sql select * from fabricks.jobs j where j.topic in ('sales', 'finance', 'marketing')</code>Filter\\nby both step and topic pattern: <code>sql title:gold_sales_like.sql select * from fabricks.jobs j where j.step = 'gold'   and j.topic like 'sales_%'</code>Curate\\nexplicit jobs: <code>sql title:curated_selection.sql select * from fabricks.jobs j where (j.step = 'silver' and j.topic = 'monarch' and j.item = 'scd1__current')    or (j.step = 'gold'   and j.topic = 'monarch' and j.item = 'orders')</code></p> <p>Tip: - You may project specific columns, but for schedules that use <code>options.view</code>, ensure <code>job_id</code> is present in the projection. Returning <code>j.*</code> is simplest.</p>"},{"location":"helpers/schedule/#best-practices","title":"Best practices","text":"<ul> <li>Keep view logic simple: use filters, avoid heavy joins or transformations.</li> <li>Align views with business domains or execution scopes (by step, topic family, tags).</li> <li>Use explicit OR lists for one\u00e2\u20ac'off backfills to keep intent clear and auditable.</li> <li>Favor stable view names; schedules reference the view name.</li> </ul>"},{"location":"helpers/schedule/#related-topics","title":"Related topics","text":"<ul> <li>Initialization and bootstrap: ../helpers/init.md</li> <li>Job helper and operations: ../helpers/job.md</li> <li>Steps overview (Bronze/Silver/Gold): ../steps/gold.md and siblings</li> <li>Data quality checks: ./checks-data-quality.md</li> <li>Table options and storage layout: ./table-options.md</li> </ul>"},{"location":"helpers/step/","title":"Step Helper (Databricks Notebook)","text":"<p>This helper describes a widget-driven approach in a Databricks notebook to manage a Fabricks step (bronze, silver, gold). </p> <p>Core imports used: <pre><code>from fabricks.api import get_step\n</code></pre></p>"},{"location":"helpers/step/#typical-usage","title":"Typical usage","text":"<p>1) Resolve the step: <pre><code>from fabricks.api import get_step\n\ns = get_step(\"gold\")  # or \"bronze\"/\"silver\"\n</code></pre></p> <p>2) Run actions: <pre><code># Examples:\ns.update_jobs()\ns.update_dependencies(progress_bar=False)\n\n# Cleanup if required:\n# s.drop()\n</code></pre></p>"},{"location":"helpers/step/#helper-notebook","title":"Helper Notebook","text":"<p>```python\\n# Databricks notebook source import os from multiprocessing import Pool from typing import Callable, List</p> <p>from databricks.sdk.runtime import dbutils, spark</p>"},{"location":"helpers/step/#command-","title":"COMMAND ----------","text":"<p>dbutils.widgets.text(\"steps\", \"---\", label=\"6 - Step(s)\") dbutils.widgets.multiselect(     \"actions\",     \"<code>update</code>-configurations\",     [         \"<code>update</code>-configurations\",    -- Update job configurations         \"add-missing-jobs\",         -- Create any missing jobs         \"<code>update</code>-dependencies\",      -- Update job dependencies         \"<code>update</code>-lists\",             -- Update lists of jobs/tables/views         \"<code>update</code>-tables-list\",       -- Update the list of tables         \"<code>update</code>-views-list\",        -- Update the list of views         \"<code>update</code>\"                    -- Update the step     ],     label=\"5 - Action(s)\" ) dbutils.widgets.dropdown(\"loglevel\", \"info\", [\"debug\", \"info\", \"error\", \"critical\"], label=\"3 - Log Level\") dbutils.widgets.dropdown(\"chdir\", \"False\", [\"True\", \"False\"], label=\"1 - Change Directory\") dbutils.widgets.dropdown(\"config_from_yaml\", \"True\", [\"True\", \"False\"], label=\"2 - Use YAML\") dbutils.widgets.dropdown(\"debugmode\", \"False\", [\"True\", \"False\"], label=\"4 - Debug Mode\")</p>"},{"location":"helpers/step/#command-_1","title":"COMMAND ----------","text":"<p>if dbutils.widgets.get(\"chdir\").lower() == \"true\":     user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()  # type: ignore     try:         os.chdir(f\"/Workspace/Users/{user}/Fabricks.Runtime\")     except FileNotFoundError:         os.chdir(f\"/Workspace/Users/{user}/runtime\")</p>"},{"location":"helpers/step/#command-_2","title":"COMMAND ----------","text":"<p>if dbutils.widgets.get(\"config_from_yaml\").lower() == \"true\":     os.environ[\"FABRICKS_IS_JOB_CONFIG_FROM_YAML\"] = \"1\" else:     os.environ[\"FABRICKS_IS_JOB_CONFIG_FROM_YAML\"] = \"0\"</p>"},{"location":"helpers/step/#command-_3","title":"COMMAND ----------","text":"<p>if dbutils.widgets.get(\"debugmode\").lower() == \"true\":     os.environ[\"FABRICKS_IS_DEBUGMODE\"] = \"1\" else:     os.environ[\"FABRICKS_IS_DEBUGMODE\"] = \"0\"</p>"},{"location":"helpers/step/#command-_4","title":"COMMAND ----------","text":"<p>loglevel = dbutils.widgets.get(\"loglevel\") if loglevel == \"debug\":     os.environ[\"FABRICKS_LOGLEVEL\"] = \"DEBUG\" elif loglevel == \"info\":     os.environ[\"FABRICKS_LOGLEVEL\"] = \"INFO\" elif loglevel == \"error\":     os.environ[\"FABRICKS_LOGLEVEL\"] = \"ERROR\" elif loglevel == \"critical\":     os.environ[\"FABRICKS_LOGLEVEL\"] = \"CRITICAL\"</p>"},{"location":"helpers/step/#command-_5","title":"COMMAND ----------","text":"<p>from fabricks.api import get_step  # noqa: E402 from fabricks.api.log import DEFAULT_LOGGER  # noqa: E402</p>"},{"location":"helpers/step/#command-_6","title":"COMMAND ----------","text":"<p>actions = dbutils.widgets.get(\"actions\").split(\",\") actions = [a.strip() for a in actions]</p>"},{"location":"helpers/step/#command-nndef-dostep-str-none","title":"COMMAND ----------<code>n</code>ndef do(step: str) -&gt; None:","text":"<pre><code>s = get_step(step=step)\ntodos: dict[str, Callable] = {}\n\nif \"``update``-configurations\" in actions:\n    todos[\"``update``-configurations\"] = s.update_jobs\n\nif \"add-missing-jobs\" in actions:\n    todos[\"add-missing-jobs\"] = s.create_jobs\n\nif \"``update``-views-list\" in actions or \"``update``-lists\" in actions:\n    todos[\"``update``-views-list\"] = s.update_views\n\nif \"``update``-tables-list\" in actions or \"``update``-lists\" in actions:\n    todos[\"``update``-tables-list\"] = s.update_tables\n\nif \"``update``\" in actions and len(actions) == 1:\n    todos[\"``update``\"] = s.``update``\n\nif \"``update``-dependencies\" in actions:\n    todos[\"``update``-dependencies\"] = s.update_dependencies\n\nfor key, func in todos.items():\n    func()\n</code></pre>"},{"location":"helpers/step/#command-_7","title":"COMMAND ----------","text":"<p>actions = [s.strip() for s in dbutils.widgets.get(\"actions\").split(\",\")] steps = [s.strip() for s in dbutils.widgets.get(\"steps\").split(\",\")] steps = [[j.strip() for j in job.split(\"//\")] if \"//\" in job else job for job in steps]</p> <p>for step in steps:     DEFAULT_LOGGER.warning(\", \".join(actions), extra={\"label\": step})</p>"},{"location":"helpers/step/#command-_8","title":"COMMAND ----------","text":"<p>for step in steps:     if isinstance(step, List):         with Pool() as pool:             results = pool.map(do, step)     else:         do(step)</p>"},{"location":"helpers/step/#command-_9","title":"COMMAND ----------","text":"<p>dbutils.notebook.exit(\"exit (0)\")  # type: ignore</p>"},{"location":"helpers/step/#command-_10","title":"COMMAND ----------","text":"<p>```</p>"},{"location":"helpers/step/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze | Silver | Gold</li> <li>Runtime overview and sample runtime: Runtime</li> <li>Checks &amp; Data Quality: Checks and Data Quality</li> <li>Table options and storage layout: Table Options</li> <li>Extenders, UDFs &amp; Views: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"introduction/layers/","title":"Layers (Bronze \u00e2\u2020' Silver \u00e2\u2020' Gold)","text":"<p>Fabricks organizes pipelines into layered steps to keep responsibilities clear and composable. Use this page to decide where logic belongs and how to structure data flow.</p> <p>Layer summary</p> <ul> <li>Bronze: Land raw data \u00e2\u20ac\u0153as-is\u00e2\u20ac\u009d from files/streams/tables. Keep it light.</li> <li>Silver: Standardize, clean, enrich; optionally apply CDC (SCD1/SCD2). Provide conformed datasets and convenience views.</li> <li>Gold: Build consumption-ready models (facts/dims/marts), possibly with CDC merges or full refreshes.</li> </ul> <p>When to use each layer</p> <ul> <li> <p>Bronze</p> <ul> <li>Ingest from external sources with minimal logic</li> <li>Keep provenance and raw fidelity</li> <li>Typical modes: <code>memory</code>, <code>append</code>, <code>register</code></li> </ul> </li> <li> <p>Silver</p> <ul> <li>Standardize schemas, apply business keys, deduplicate</li> <li>Apply CDC: <code>scd1</code>/<code>scd2</code> with convenience views (e.g., table__current)</li> <li>Typical modes: <code>memory</code>, <code>append</code>, <code>latest</code>, <code>update</code>, <code>combine</code></li> </ul> </li> <li> <p>Gold</p> <ul> <li>Aggregate, reshape, and model data for analytics/reporting</li> <li>Implement SCD patterns at the desired consumption grain</li> <li>Typical modes: <code>memory</code>, <code>append</code>, <code>complete</code>, <code>update</code>, <code>invoke</code> (notebook)</li> </ul> </li> </ul> <p>Data flow</p> <ul> <li>Bronze: raw sources \u00e2\u2020' (light parsing, optional filters/calculated columns) \u00e2\u2020' raw tables/views</li> <li>Silver: bronze tables/views \u00e2\u2020' (standardize/enrich, optional CDC) \u00e2\u2020' conformed tables + {table}__current view</li> <li>Gold: silver conformed outputs \u00e2\u2020' (marts/dim/fact, SCD merges or <code>complete</code> refresh) \u00e2\u2020' business-consumption tables/views</li> </ul> <p>Quality and governance</p> <ul> <li>Configure pre_run/post_run checks and row thresholds in step/job options.</li> <li>Use warning/fail contracts to protect downstream consumers.</li> </ul> <p>Related</p> <ul> <li>Bronze: ../steps/bronze.md</li> <li>Silver: ../steps/silver.md</li> <li>Gold: ../steps/gold.md</li> <li>CDC reference: ../reference/cdc.md</li> <li>Data quality checks: ../reference/checks-data-quality.md</li> <li>Table options: ../reference/table-options.md</li> </ul>"},{"location":"introduction/overview/","title":"Key Concepts Overview","text":"<p>This page introduces Fabricks core concepts so you can read and write pipelines with confidence.</p> <p>What Fabricks is ?</p> <ul> <li>SQL-first transformations</li> <li>YAML-based orchestration for jobs, steps, schedules, and dependencies</li> <li>Built-in Change Data Capture (CDC) patterns (SCD1/SCD2)</li> <li>Data Quality gates via checks and contracts</li> <li>Extensible with UDFs, Parsers, and Python Extenders</li> </ul> <p>Jobs vs Steps</p> <ul> <li>Step: A processing layer/type that defines behavior and defaults (Bronze, Silver, Gold).</li> <li>Job: A concrete unit of work within a step (identified by topic + item) that you schedule/run.</li> </ul> <p>Layers (Bronze \u00e2\u2020' Silver \u00e2\u2020' Gold)</p> <ul> <li>Bronze: Land raw data (<code>append</code>/<code>register</code>/<code>memory</code>). Keep logic light.</li> <li>Silver: Standardize/clean/enrich. Optional CDC (SCD1/SCD2) and convenience views.</li> <li>Gold: Consumption models and marts. Supports merge/<code>update</code> and SCD patterns.</li> </ul> <p>Change Data Capture (CDC)</p> <ul> <li>SCD1: Current state with optional soft deletes (__is_current, __is_deleted) and a {table}__current view.</li> <li>SCD2: Historical validity windows (__valid_from, __valid_to). Gold consumers use (__key, __timestamp, __operation).</li> <li>Reload rectification: A snapshot boundary that reconciles target state (see CDC reference).</li> </ul> <p>Data Quality (Checks)</p> <ul> <li>Pre- and post-run checks, contracts, and row count thresholds.</li> <li>CI-friendly: Fail fast on contract violations or warn when needed.</li> </ul> <p>Extensibility</p> <ul> <li>UDFs: Register functions on the Spark session for use in SQL.</li> <li>Parsers: Custom readers/cleaners for sources (return a DataFrame).</li> <li>Extenders: Small Python transforms applied right before writing.</li> </ul> <p>Navigation</p> <ul> <li>Layers guide: Layers (Bronze/Silver/Gold) \u00e2\u2020' Layers</li> <li>CDC strategies and contracts \u00e2\u2020' CDC</li> <li>Step references \u00e2\u2020' Bronze | Silver | Gold</li> <li>Data Quality \u00e2\u2020' Checks &amp; Data Quality</li> <li>Table and storage options \u00e2\u2020' Table Options</li> <li>Extensibility \u00e2\u2020' Extenders, UDFs &amp; Parsers</li> </ul>"},{"location":"reference/cdc/","title":"Change Data Capture (CDC)","text":"<p>Fabricks provides built-in Change Data Capture (CDC) patterns to keep downstream tables synchronized with upstream changes using SQL-first pipelines. CDC is enabled per job via <code>options.change_data_capture</code> and implemented by generated MERGE/INSERT statements driven by small helper columns.</p> <p>This page explains the supported CDC strategies, required inputs, merge semantics, and examples.</p>"},{"location":"reference/cdc/#strategies","title":"Strategies","text":"Strategy Description Convenience views <code>nocdc</code> No CDC; writes the result as-is. \u00e2\u20ac\" <code>scd1</code> Tracks current vs deleted; maintains flags <code>__is_current</code>, <code>__is_deleted</code>. <code>{table}__current</code> in Silver <code>scd2</code> Slowly Changing Dimension Type 2: validity windows with <code>__valid_from</code>, <code>__valid_to</code>. <code>{table}__current</code> in Silver"},{"location":"reference/cdc/#what-is-scd1","title":"What is SCD1?","text":"<ul> <li>Definition: Slowly Changing Dimension Type 1 keeps only the current state of each business key. Attribute changes overwrite previous values rather than preserving history.</li> <li>Typical columns in Fabricks: <code>__is_current</code>, <code>__is_deleted</code>. A convenience view <code>{table}__current</code> in Silver selects current non-deleted rows.</li> <li>When to use: When downstream consumers only need the <code>latest</code> values and you do not need to answer \u00e2\u20ac\u0153as-of\u00e2\u20ac\u009d questions or audit historical attribute values.</li> </ul>"},{"location":"reference/cdc/#what-is-scd2","title":"What is SCD2?","text":"<ul> <li>Definition: Slowly Changing Dimension Type 2 preserves the full change history by creating a new versioned row each time attributes change. Each row covers a validity window for a given business key.</li> <li>Typical columns in Fabricks: <code>__valid_from</code>, <code>__valid_to</code>, and <code>__is_current</code> (optional <code>__is_deleted</code> if soft-deletes are modeled). Silver also provides <code>{table}__current</code> for <code>latest</code> rows.</li> <li>When to use: When you must answer \u00e2\u20ac\u0153as-of\u00e2\u20ac\u009d queries (e.g., \u00e2\u20ac\u0153What was the customer segment on 2024\u00e2\u20ac'03\u00e2\u20ac'01?\u00e2\u20ac\u009d), analyze changes over time, or maintain auditable history. In Gold SCD2 merges, inputs use <code>__key</code>, <code>__timestamp</code>, <code>__operation</code> to define change points.</li> </ul>"},{"location":"reference/cdc/#how-to-enable-cdc","title":"How to enable CDC","text":"<p>Set the CDC strategy in the job options:</p> <pre><code>- job:\n    step: silver\n    topic: demo\n    item: `scd1`\n    options:\n      mode: `update`\n      change_data_capture: `scd1`\n      parents: [bronze.demo_source]\n```For\\nGold:\n\n```yaml\n- job:\n    step: gold\n    topic: `scd2`\n    item: `update`\n    options:\n      mode: `update`\n      change_data_capture: `scd2`\n```Supported\\nvalues: ``nocdc``, ``scd1``, ``scd2``.\n\n---\n\n## Input contracts\n\nSome helper columns govern CDC behavior. Fabricks generates additional internal helpers during processing.\n\n- Gold jobs (consumer side):\n    - `scd2` (required): `__key`, `__timestamp`, `__operation` with values `'upsert' | 'delete' | 'reload'`.\n    - `scd1` (required): `__key`; optional `__timestamp` / `__operation` (`'upsert' | 'delete' | 'reload'`) for delete/rectify handling.\n    - Note: If `__operation` is absent in Gold SCD `update` jobs, Fabricks auto-injects `__operation = 'reload'` and enables rectification.\n    - Optional helpers used by merges:\n        - `__order_duplicate_by_asc` / `__order_duplicate_by_desc`\n        - `__identity` (only when `table_options.identity` is not true; if `identity: true`, the identity column is auto-created and you should not supply `__identity`)\n        - `__source` (to scope merges by logical source)\n\n- Silver jobs (producer side):\n    - Provide business keys through job-level `keys` (or compute a `__key`) to support downstream CDC.\n    - Silver can apply CDC directly and yields convenience views (e.g., `{table}__current`).\n\nNote\n\n- Memory outputs ignore columns that start with `__`.\n- Special characters in column names are preserved.\n\nSee details in:\n\n- Steps: [Silver](../steps/silver.md) | [Gold](../steps/gold.md)\n- [Table Options](./table-options.md)\n\n---\n\n## Merge semantics (under the hood)\n\nFabricks compiles CDC operations into SQL via Jinja templates at runtime. The core logic lives in `fabricks.cdc`:\n\n- `Merger.get_merge_query` renders `templates/merge.sql.jinja` for the selected `change_data_capture` strategy.\n- The framework computes internal columns such as:\n    - `__merge_condition` \u00e2\u20ac\" one of `'upsert' | 'delete' | '`update`' | 'insert'` depending on strategy and inputs.\n    - `__merge_key` \u00e2\u20ac\" a synthetic key used to join against the target.\n- You usually do not set these internal fields manually; they are derived from your inputs (`__key`, `__operation`, `__timestamp`) and job options.\n\n*Join keys*\n\n- If a `__key` column exists in the target, merges use `t.__key = s.__merge_key`.\n- Otherwise, the configured `keys` option is used to build an equality join on business keys.\n\n*Source scoping*\n\n- If `__source` exists in both sides, merges add `t.__source = s.__source` to support multi-source data in the same table.\n\n*Soft delete vs hard delete (SCD1)*\n\n- If the incoming data contains `__is_deleted`, the SCD1 template performs soft deletes:\n    - Sets `__is_current = false`, `__is_deleted = true` on delete.\n- If `__is_deleted` is absent, deletes are physical for SCD1.\n\n*Timestamps and metadata*\n\n- If the incoming data provides `__timestamp`, it is propagated to the target.\n- If the target has `__metadata`, the `updated` timestamp is set to the current time during updates/deletes.\n\n*Identity and hash*\n\n- If `table_options.identity: true`, the identity column is created automatically when the table is created.\n- If `table_options.identity` is not true and `__identity` is present in the input, it will be written as a regular column.\n- If `__hash` is present, it is updated during upsert operations.\n\n*Update filtering*\n\n- `options.update_where` can constrain rows affected during merges (useful for limiting the scope of updates).\n\n*Internals reference*\n\n- `framework/fabricks/cdc/base/merger.py`\n- Templates under `framework/fabricks/cdc/templates/merge/*.sql.jinja`\n\n---\n\n## SCD1 details\n\n*Behavior* (see `merge/`scd1`.sql.jinja`)\n\n- Upsert (`__merge_condition = 'upsert'`): updates matching rows and inserts non\u00e2\u20ac'matching rows.\n- Delete (`__merge_condition = 'delete'`):\n    - Soft delete if `__is_deleted` is part of the schema: sets `__is_current = false`, `__is_deleted = true`.\n    - Otherwise, performs a physical delete.\n\n*Convenience view (in Silver)*\n\n- `{table}__current`: filters current (non\u00e2\u20ac'deleted) rows for simplified consumption.\n\n*Minimal Silver example*\n\n```yaml\n- job:\n    step: silver\n    topic: monarch\n    item: `scd1`\n    options:\n      mode: `update`\n      change_data_capture: `scd1`\n</code></pre> <p>Gold consumption example</p> <pre><code>-- Example: consuming current rows from SCD1 silver output\nselect id as id, name as monarch\nfrom silver.monarch_scd1__current\n</code></pre>"},{"location":"reference/cdc/#reload-operation","title":"Reload operation","text":"<p><code>'reload'</code> is a CDC input operation used as a reconciliation boundary. It is not a direct MERGE action; instead, it signals Fabricks to rectify the target table using the supplied snapshot at that timestamp.</p> <ul> <li>Purpose: mark a full or partial snapshot boundary so missing keys can be treated as deletes and present keys as upserts as needed.</li> <li>Auto-injection: when a Gold SCD job runs in <code>mode:</code>update<code>` and your SQL does not provide</code>__operation<code>, Fabricks injects</code>__operation = 'reload'` and turns on rectification.</li> <li>Silver behavior:<ul> <li>If a batch contains <code>'reload'</code> after the target\u00e2\u20ac\u2122s max timestamp, Silver enables rectification logic.</li> <li>In <code>mode:</code>latest<code>`,</code>'reload'` is not allowed and will be rejected.</li> </ul> </li> <li>Gold behavior:<ul> <li>Passing <code>reload=True</code> to a Gold job run triggers a full <code>complete</code> write for that run.</li> </ul> </li> <li>Internals: rectification is computed in <code>framework/fabricks/cdc/templates/query/rectify.sql.jinja</code>, which computes next operations/windows around <code>'reload'</code> markers.</li> </ul> <p>Warning</p> <p>In Silver <code>mode:</code>latest<code>, `'reload'` is forbidden and will be rejected.  Use `mode: `update</code> or <code>mode:</code>append<code>` instead if you need reconciliation behavior.</code>n<code>n!!! tip You generally do not need to emit</code>'reload'<code>manually in Gold SCD</code>update<code>jobs; it is injected for you when</code>__operation<code>is missing.  For explicit control, you can produce rows with</code>__operation = 'reload'` at the snapshot timestamp.</p>"},{"location":"reference/cdc/#scd2-details","title":"SCD2 details","text":"<p>Behavior (see <code>merge/</code>scd2<code>.sql.jinja</code>):</p> <ul> <li>Update (<code>__merge_condition = '</code>update<code>'</code>): closes the current row by setting <code>__valid_to = __valid_from - 1 second</code>, <code>__is_current = false</code>. A subsequent insert creates the new current row.</li> <li>Delete (<code>__merge_condition = 'delete'</code>): closes the current row and sets <code>__is_current = false</code> (and <code>__is_deleted = true</code> if soft delete is modeled).</li> <li>Insert (<code>__merge_condition = 'insert'</code>): inserts a new current row.</li> </ul> <p>Required Gold inputs:</p> <ul> <li><code>__key</code>, <code>__timestamp</code>, <code>__operation</code> with values <code>'upsert' | 'delete' | 'reload'</code>.</li> </ul> <p>Reload notes:</p> <ul> <li><code>'reload'</code> marks a reconciliation boundary; Fabricks derives concrete actions (e.g., closing current rows, inserting new ones, deleting missing keys) across that boundary.</li> <li>If you omit <code>__operation</code> in Gold SCD <code>update</code> jobs, Fabricks injects <code>'reload'</code> and enables rectification automatically.</li> <li>In Silver:<ul> <li>Presence of <code>'reload'</code> (beyond target\u00e2\u20ac\u2122s max timestamp) enables rectification.</li> <li><code>'reload'</code> is forbidden in <code>mode:</code>latest``.</li> </ul> </li> </ul> <p>Optional features:</p> <ul> <li><code>options.correct_valid_from</code>: adjusts start timestamps for validity windows.</li> <li><code>options.persist_last_timestamp</code>: persists last processed timestamp for incremental loads.</li> </ul> <p>Convenience view:</p> <ul> <li><code>{table}__current</code>: returns only the <code>latest</code> (current) rows per business key.</li> </ul> <p>Minimal Silver example:</p> <pre><code>- job:\n    step: silver\n    topic: monarch\n    item: `scd2`\n    options:\n      mode: `update`\n      change_data_capture: `scd2`\n</code></pre> <p>Note: Credit \u00e2\u20ac\" Temporal Snapshot Fact Table (SQLBits 2012). Recommended to watch to understand SCD2 snapshot-based modeling concepts. </p> <p>Slides: Temporal Snapshot Fact Tables (slides)</p> <p>Gold input construction example:</p> <pre><code>-- Turn SCD2 changes into Gold input operations\nwith dates as (\n  select id as id, __valid_from as __timestamp, 'upsert' as __operation\n  from silver.monarch_scd2 where __valid_from &gt; '1900-01-02'\n  union\n  select id as id, __valid_to as __timestamp, 'delete' as __operation\n  from silver.monarch_scd2 where __is_deleted\n)\nselect\n  d.id as __key,\n  s.id as id,\n  s.name as monarch,\n  s.doubleField as value,\n  d.__operation,\n  if(d.__operation = 'delete', d.__timestamp + interval 1 second, d.__timestamp) as __timestamp\nfrom dates d\nleft join silver.monarch_scd2 s\n  on d.id = s.id and d.__timestamp between s.__valid_from and s.__valid_to\n</code></pre>"},{"location":"reference/cdc/#keys-and-__key","title":"Keys and <code>__key</code>","text":"<ul> <li>Preferred: produce a stable <code>__key</code> in your SELECTs (e.g., UDF that hashes business keys).</li> <li>Alternative: configure <code>options.keys: [ ... ]</code> to specify business keys. Fabricks derives join predicates from these when <code>__key</code> is not present.</li> </ul> <p>Tip</p> <ul> <li>Only provide <code>__identity</code> when <code>table_options.identity</code> is not true. If <code>identity: true</code>, the identity column is auto-created when the table is created; do not include <code>__identity</code>. See Table Options.</li> </ul>"},{"location":"reference/cdc/#examples","title":"Examples","text":"<p>Silver SCD1 with duplicates handling</p> <pre><code>- job:\n    step: silver\n    topic: princess\n    item: order_duplicate\n    options:\n      mode: `update`\n      change_data_capture: `scd1`\n      order_duplicate_by:\n        order_by: desc\n```Gold\\nSCD1 `update` with incremental timestamp\n\n```yaml\n- job:\n    step: gold\n    topic: `scd1`\n    item: last_timestamp\n    options:\n      change_data_capture: `scd1`\n      mode: `update`\n      persist_last_timestamp: true\n</code></pre>"},{"location":"reference/cdc/#operational-notes","title":"Operational notes","text":"<ul> <li>Streaming: where supported, <code>options.stream: true</code> enables incremental semantics.</li> <li>Parents: use <code>options.parents</code> to order upstream dependencies.</li> <li>Checks: configure quality gates via <code>options.check_options</code>. See Checks &amp; Data Quality.</li> </ul>"},{"location":"reference/cdc/#related","title":"Related","text":"<ul> <li>Steps: Silver | Gold</li> <li>Reference: Table Options, Extenders, UDFs &amp; Parsers</li> <li>Sample runtime: Sample runtime</li> </ul>"},{"location":"reference/checks-data-quality/","title":"Checks and Data Quality","text":"<p>Fabricks provides built-in mechanisms to validate data before and/or after a job runs. Checks can enforce row counts, equality constraints, and custom SQL contracts that return actions and messages.</p>"},{"location":"reference/checks-data-quality/#configuring-checks","title":"Configuring checks","text":"<p>Enable checks per job using <code>check_options</code>:</p> <pre><code>- job:\n    step: gold\n    topic: check\n    item: max_rows\n    options: { mode: `complete` }\n    check_options:\n      pre_run: true         # run checks before writing\n      post_run: true        # run checks after writing\n      min_rows: 10          # minimum expected row count\n      max_rows: 10000       # maximum expected row count\n      count_must_equal: fabricks.dummy  # enforce row count equality with another table\n      skip: false           # skip all checks if true\n```Supported\\noptions:\n- pre_run/post_run: Run checks around the execution of the job.\n- min_rows/max_rows: Row count bounds.\n- count_must_equal: Assert row count equals a reference table.\n- skip: Disable checks for this job.\n\n## Catalog of checks\n\nThese are the built-in switches you can use in `check_options`. Built-in checks evaluate simple properties of the dataset and will fail the job on violation. Use SQL contracts when you need richer logic or to emit warnings instead of hard failures.\n\n| Name              | Type                | Default | Required | Description                                                  | Failure behavior                                      |\n|-------------------|---------------------|---------|----------|--------------------------------------------------------------|-------------------------------------------------------|\n| pre_run           | boolean             | false   | no       | Run SQL contracts and built-in checks before the write.      | Contract: `__action` controls outcome.                |\n| post_run          | boolean             | false   | no       | Run SQL contracts and built-in checks after the write.       | Contract: `__action` controls outcome.                |\n| min_rows          | integer             | unset   | no       | Minimum expected row count of the dataset being written.     | Fail if `count &lt; min_rows`.                           |\n| max_rows          | integer             | unset   | no       | Maximum allowed row count of the dataset being written.      | Fail if `count &gt; max_rows`.                           |\n| count_must_equal  | string (table/view) | unset   | no       | Require row count to equal a reference table or view.        | Fail if counts differ.                                |\n| skip              | boolean             | false   | no       | Disable all checks for this job.                             | No checks executed.                                   |\n\n### Examples\n\nBounds only:\n```yaml\ncheck_options:\n  pre_run: true\n  min_rows: 10\n  max_rows: 100000\n```Equality\\nagainst a reference:\n```yaml\ncheck_options:\n  post_run: true\n  count_must_equal: analytics.dim_customer\n```Mixing\\nbuilt-ins with SQL contracts:\n- Use `pre_run`/`post_run` to enable contract execution and add your contract SQL files (see \u00e2\u20ac\u0153SQL contracts\u00e2\u20ac\u009d below).\n## SQL contracts\n\nWhen `pre_run` or `post_run` checks are enabled, provide SQL files named after the table or job:\n- `table_name.pre_run.sql`\n- `table_name.post_run.sql`\n\nEach SQL must return:\n- `__action`: either `'fail'` or `'warning'`\n- `__message`: human-readable message\n\nExample:\n\n```sql\n-- fail.pre_run.sql\nselect \"fail\" as __action, \"Please don't fail on me :(\" as __message;\n\n-- warning.post_run.sql\nselect \"warning\" as __action, \"I want you to warn me !\" as __message;\n</code></pre>"},{"location":"reference/checks-data-quality/#behavior-and-rollback","title":"Behavior and rollback","text":"<ul> <li>For physical tables (<code>append</code>/<code>complete</code>/<code>update</code> modes), a failed check triggers an automatic rollback to the previous successful version.</li> <li>For <code>memory</code> mode (views), results are not persisted and failures are logged only.</li> </ul>"},{"location":"reference/checks-data-quality/#ci-integration","title":"CI integration","text":"<p>Checks produce two observable signals that are useful in CI and orchestration systems:</p> <ul> <li>Exit code</li> <li>Any hard failure (built-in bound violations or a contract returning <code>__action = 'fail'</code>) causes the job to exit with a non\u00e2\u20ac'zero status.</li> <li>For physical tables (<code>append</code>/<code>complete</code>/<code>update</code> modes), failures also trigger an automatic rollback to the previous successful version.</li> <li> <p>Most CI systems will mark the pipeline as failed automatically on non\u00e2\u20ac'zero exit codes.</p> </li> <li> <p>Log messages</p> </li> <li>Contracts should emit a single row with <code>__action</code> and <code>__message</code>. These are logged so you can surface them in CI logs and artifacts.</li> <li>When <code>__action = 'warning'</code>, the job continues and exits with code 0. Ensure your CI surfaces logs so warnings are visible in pull requests.</li> </ul> <p>Example contract outputs (see \u00e2\u20ac\u0153SQL contracts\u00e2\u20ac\u009d): <pre><code>-- Fails the job and rolls back (physical tables)\nselect 'fail'    as __action, 'Row count below threshold' as __message;\n\n-- Logs a warning; job continues\nselect 'warning' as __action, 'Null rate above target'    as __message;\n</code></pre></p> <p>Tips: - Prefer <code>post_run</code> for checks that must consider the final persisted state. - Use built-in bounds for simple invariants; reserve contracts for multi-table logic and nuanced policies. - If you need to gate on warnings, implement a small CI step that scans logs for known warning markers and decides policy for your repository.</p>"},{"location":"reference/checks-data-quality/#examples","title":"Examples","text":"<ul> <li>Working scenarios are available in the sample runtime:</li> <li>Browsable: Sample runtime</li> <li>Rich integration scenarios (repository layout):</li> <li><code>framework/tests/integration/runtime/gold/gold/</code></li> </ul>"},{"location":"reference/checks-data-quality/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze | Silver | Gold</li> <li>Table properties and physical layout: Table Options</li> <li>Custom logic integration: Extenders, UDFs &amp; Parsers</li> </ul>"},{"location":"reference/extenders-udfs-parsers/","title":"Extenders, UDFs, and Parsers","text":"<p>This reference explains how to extend Fabricks with custom Python code and reusable SQL assets: - Extenders: Python functions that transform a Spark DataFrame before it is written. - UDFs: User-defined functions you <code>register</code> on the Spark session and use in SQL. - Parsers: Source-specific readers/cleaners that return a DataFrame (optional, advanced).</p> <p>Use these to encapsulate business logic, reuse patterns, and keep SQL jobs focused and readable.</p>"},{"location":"reference/extenders-udfs-parsers/#extenders","title":"Extenders","text":"<p>Extenders are Python functions that take a DataFrame and return a transformed DataFrame. They are applied during job execution (typically in Silver/Gold) right before write.</p> <ul> <li>Location: Put Python modules under your runtime, for example <code>fabricks/extenders</code>.</li> <li>Referencing: In job YAML use <code>extender: name</code> and optionally <code>extender_options: { ... }</code> to pass arguments.</li> </ul> <p>Example (adds a <code>country</code> column):</p> <pre><code>from pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom fabricks.core.extenders import extender\n\n@extender(name=\"add_country\")\ndef add_country(df: DataFrame, **kwargs) -&gt; DataFrame:\n    return df.withColumn(\"country\", lit(kwargs.get(\"country\", \"Unknown\")))\n```In\\na job:\n\n```yaml\n- job:\n    step: silver\n    topic: sales_analytics\n    item: daily_summary\n    options:\n      mode: `update`\n    extender: add_country\n    extender_options:\n      country: CH\n</code></pre> <p>Notes: - Extenders should be idempotent and fast. - Avoid heavy I/O; handle source reading in Bronze or via a Parser. - Prefer small, composable transformations.</p>"},{"location":"reference/extenders-udfs-parsers/#udfs","title":"UDFs","text":"<p>UDFs are registered on the Spark session and then callable from SQL. Place the UDF registration code in your runtime (e.g., <code>fabricks/udfs</code>), and ensure it runs before jobs that need it (for example during initialization or via a notebook hook).</p> <p>Example (simple addition UDF):</p> <pre><code>from pyspark.sql import SparkSession\nfrom fabricks.core.udfs import udf\n\n@udf(name=\"addition\")\ndef addition(spark: SparkSession):\n    def _add(a: int, b: int) -&gt; int:\n        return a + b\n    spark.udf.`register`(\"udf_addition\", _add)\n```Using\\nbuilt-in helpers and a custom UDF in SQL:\n\n```sql\nselect\n  udf_identity(s.id, id) as __identity,\n  udf_key(array(s.id)) as __key,\n  s.id as id,\n  s.name as monarch,\n  udf_addition(1, 2) as addition\nfrom silver.monarch_scd1__current s\n</code></pre> <p>Notes: - UDFs should validate inputs and avoid side-effects. - Prefer Spark SQL built-ins and DataFrame functions where possible for performance.</p>"},{"location":"reference/extenders-udfs-parsers/#parsers","title":"Parsers","text":"<p>Parsers read and lightly clean raw data. They return a DataFrame and should not write output or mutate state.</p> <p>What a parser should do: - Read raw data from <code>data_path</code> (batch or stream based on <code>stream</code> flag) - Optionally apply/validate a schema from <code>schema_path</code> - Perform light, source-specific cleanup (drops/renames/type fixes) - Return a Spark DataFrame (no writes, no side effects)</p> <p>Inputs: - <code>data_path</code>: source location (directory or file) - <code>schema_path</code>: optional schema location (e.g., JSON/DDL) - <code>spark</code>: active <code>SparkSession</code> - <code>stream</code>: when true, prefer <code>readStream</code> where supported</p> <p>Reference in a job:</p> <pre><code>- job:\n    step: bronze\n    topic: demo\n    item: source\n    options:\n      mode: `append`\n      uri: /mnt/demo/raw/demo\n      parser: monarch\n    parser_options:\n      file_format: parquet\n      read_options:\n        mergeSchema: true\n```Example\\nimplementation:\n\n```python\nfrom pyspark.sql import DataFrame, SparkSession\nfrom fabricks.core.parsers import parser\nfrom fabricks.utils.path import Path\n\n@parser(name=\"monarch\")\nclass MonarchParser:\n    def parse(self, data_path: Path, schema_path: Path, spark: SparkSession, stream: bool) -&gt; DataFrame:\n        df = spark.read.parquet(data_path.string)\n        cols_to_drop = [c for c in df.columns if c.startswith(\"BEL_\")]\n        if cols_to_drop:\n            df = df.drop(*cols_to_drop)\n        return df\n</code></pre> <pre><code>from pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom fabricks.core.extenders import extender\n\n@extender(name=\"add_country\")\ndef add_country(df: DataFrame, **kwargs) -&gt; DataFrame:\n    return df.withColumn(\"country\", lit(kwargs.get(\"country\")))\n</code></pre>"},{"location":"reference/extenders-udfs-parsers/#related","title":"Related","text":"<ul> <li>Steps: Bronze | Silver | Gold</li> <li>Reference: Checks &amp; Data Quality | Table Options</li> <li>Runtime: Runtime Configuration</li> </ul>"},{"location":"reference/table-options/","title":"Table Options","text":"<p>Table options control how Fabricks creates and manages physical Delta tables across steps. Use these options to define properties, layout, constraints, identity, retention, and more. Options are generally provided under <code>table_options</code> at the job level (and may also be set as step defaults in your runtime config).</p>"},{"location":"reference/table-options/#common-options","title":"Common options","text":"<ul> <li>identity: Enables a Delta identity column on the target table. If <code>identity: true</code>, the identity column is auto-created when the table is created.</li> <li>liquid_clustering: Enables Delta Liquid Clustering where supported by the Databricks runtime.</li> <li>partition_by: List of partition columns (e.g., <code>[date_id, country]</code>).</li> <li>zorder_by: Columns used for Z-Ordering (improves data skipping).</li> <li>cluster_by: Logical clustering keys (materialization depends on runtime features).</li> <li>bloomfilter_by: Columns to maintain Bloom filters on (where supported).</li> <li>constraints: Map of table constraints (e.g., <code>primary key(__key)</code>).</li> <li>properties: Arbitrary Delta table properties (key/value pairs).</li> <li>comment: Table comment/description.</li> <li>calculated_columns: Map of computed columns populated on write.</li> <li>retention_days: Table VACUUM retention period (days).</li> <li>powerbi: true to apply Power BI\u00e2\u20ac\"specific metadata (if supported).</li> </ul>"},{"location":"reference/table-options/#option-matrix-types-defaults-compatibility","title":"Option matrix (types | defaults | compatibility)","text":"Option Type Default Applies to modes Notes identity boolean false <code>append</code>, <code>complete</code>, <code>update</code> Creates a Delta identity column on table create. liquid_clustering boolean false <code>append</code>, <code>complete</code>, <code>update</code> Requires Databricks runtime support. partition_by array[string] \u00e2\u20ac\" <code>append</code>, <code>complete</code>, <code>update</code> Low-cardinality columns recommended. zorder_by array[string] \u00e2\u20ac\" <code>append</code>, <code>complete</code>, <code>update</code> Improves data skipping on frequently filtered columns. cluster_by array[string] \u00e2\u20ac\" <code>append</code>, <code>complete</code>, <code>update</code> Logical clustering; materialization depends on runtime features. bloomfilter_by array[string] \u00e2\u20ac\" <code>append</code>, <code>complete</code>, <code>update</code> Use selectively for point-lookups. constraints map[string,string] \u00e2\u20ac\" <code>append</code>, <code>complete</code>, <code>update</code> E.g., <code>primary key(__key)</code>. properties map[string,string] \u00e2\u20ac\" <code>append</code>, <code>complete</code>, <code>update</code> Arbitrary Delta table properties. comment string \u00e2\u20ac\" <code>append</code>, <code>complete</code>, <code>update</code> Table description. calculated_columns map[string,string] \u00e2\u20ac\" <code>append</code>, <code>complete</code>, <code>update</code> Computed at write time. retention_days integer \u00e2\u20ac\" <code>append</code>, <code>complete</code>, <code>update</code> VACUUM retention (days). powerbi boolean false <code>append</code>, <code>complete</code>, <code>update</code> Applies Power BI\u00e2\u20ac\"specific metadata where supported. <p>Notes: - table_options are ignored in <code>memory</code> mode (view-only). - Defaults may also be set at step level in your runtime and overridden per job.</p>"},{"location":"reference/table-options/#minimal-example","title":"Minimal example","text":"<pre><code>- job:\n    step: gold\n    topic: fact\n    item: option\n    options:\n      mode: `complete`\n    table_options:\n      identity: true\n      cluster_by: [monarch]\n      properties:\n        delta.enableChangeDataFeed: true\n      comment: Strength lies in unity\n</code></pre>"},{"location":"reference/table-options/#extended-example","title":"Extended example","text":"<pre><code>- job:\n    step: gold\n    topic: sales\n    item: curated\n    options:\n      mode: `update`\n      change_data_capture: `scd1`\n    table_options:\n      identity: true\n      partition_by: [date_id]\n      zorder_by: [customer_id, product_id]\n      bloomfilter_by: [order_id]\n      constraints:\n        primary_key: \"primary key(__key)\"\n      properties:\n        delta.minReaderVersion: 2\n        delta.minWriterVersion: 5\n        delta.enableChangeDataFeed: true\n      comment: \"Curated sales model with CDC\"\n      retention_days: 30\n      calculated_columns:\n        ingestion_date: \"current_date()\"\n</code></pre>"},{"location":"reference/table-options/#notes-and-guidance","title":"Notes and guidance","text":"<ul> <li>Scope: <code>table_options</code> apply to physical table modes (<code>append</code>, <code>complete</code>, <code>update</code>). They do not apply to <code>memory</code> mode (view-only).</li> <li>Identity: When <code>identity: true</code> is enabled, the identity column is defined at create time - See Identity.</li> <li>Layout vs performance: Start with <code>partition_by</code> on low-cardinality columns; add <code>zorder_by</code> for frequently filtered high-cardinality columns. Use Bloom filters selectively for point-lookups.</li> <li>Defaults: You can specify step-level defaults in your runtime config and override them per job.</li> </ul>"},{"location":"reference/table-options/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze | Silver | Gold</li> <li>Data quality checks: Checks &amp; Data Quality</li> <li>Custom logic integration: Extenders, UDFs &amp; Parsers</li> <li>Runtime configuration: Runtime</li> </ul>"},{"location":"steps/bronze/","title":"Bronze Step Reference","text":"<p>The Bronze step ingests raw data from files, streams, or existing tables into the Lakehouse. Its purpose is to capture source data with minimal transformation so downstream steps can standardize, enrich, and model it.</p> <p>What Bronze does</p> <ul> <li>Reads data from a <code>uri</code> using a built-in parser (e.g., <code>parquet</code>, <code>csv</code>) or a custom parser/extender.</li> <li>Optionally applies lightweight transformations: filters, calculated columns, or column encryption.</li> <li>Produces one of: a temporary view, an appended Delta table, or registers an existing table/view for downstream steps.</li> </ul> <p>Modes</p> Mode Behavior <code>memory</code> Register a temporary view only; no Delta table is written. Useful for quick testing or transient inputs. <code>append</code> Append records to the target Delta table (no merge/upsert). Typical for landing raw files. <code>register</code> Register an existing Delta table or view available at <code>uri</code>. Parser is not used in this mode. <p>When to use each mode</p> <ul> <li><code>memory</code>: Use when you want a temporary, in-session view (no persisted table).</li> <li><code>append</code>: Use for typical raw file ingestion where new data is appended to a Delta target.</li> <li><code>register</code>: Use when the source is already materialized as a table/view and you only want to reference it.</li> </ul> <p>Minimal example <pre><code>- job:\n    step: bronze\n    topic: demo\n    item: source\n    options:\n      mode: `append`\n      uri: /mnt/demo/raw/demo\n      parser: parquet\n      keys: [id]\n</code></pre></p> <p>Examples</p> <p>Memory (temporary view only) <pre><code>- job:\n    step: bronze\n    topic: demo\n    item: mem_view\n    options:\n      mode: `memory`\n      uri: /mnt/demo/raw/demo\n      parser: parquet\n      keys: [id]   # optional, useful for downstream CDC\n</code></pre></p> <p>Register an existing table/view <pre><code>- job:\n    step: bronze\n    topic: demo\n    item: register_source\n    options:\n      mode: `register`\n      uri: analytics.raw_demo   # existing Delta table or view name\n</code></pre></p> <p>Common options (summary)</p> <p>This section lists the most common options for a Bronze job. Options can be defined at the job level (recommended) or inherited from step-level defaults.</p> Option Type Default Required Purpose / notes <code>type</code> enum: <code>default</code>, <code>manual</code> <code>default</code> no <code>manual</code> disables auto DDL/DML; you manage persistence yourself. <code>mode</code> enum: <code>memory</code>,<code>append</code>,<code>register</code> - yes Controls ingestion behavior. <code>uri</code> string (path or table) - cond. Source location or existing table/view (required for <code>append</code>/<code>register</code>). <code>parser</code> string - cond. Input parser (<code>parquet</code>, <code>csv</code>, <code>json</code>) or custom parser. Not used in <code>register</code>. <code>source</code> string - no Logical source label for lineage/logging. <code>keys</code> array[string] - no Business keys used for dedup and downstream CDC; recommended. <code>parents</code> array[string] - no Upstream job dependencies to enforce ordering. <code>filter_where</code> string (SQL predicate) - no Row-level filter applied during ingestion. <code>calculated_columns</code> map[string,string - no New columns defined as SQL expressions evaluated at load time. <code>encrypted_columns</code> array[string] - no Columns to encrypt during write. <code>extender</code> string - no Python extender to transform the DataFrame (see Extenders). <code>extender_options</code> map[string,any] {} no Arguments passed to the extender. <code>operation</code> enum: <code>upsert</code>,<code>reload</code>,<code>delete</code> - no Changelog semantics for certain feeds. <code>timeout</code> integer (seconds) - no Per-job timeout; overrides step default. <p>Notes</p> <ul> <li><code>uri</code> is required for <code>append</code> and <code>register</code>; optional for <code>memory</code> when an extender or custom parser produces the DataFrame.</li> <li><code>parser</code> is required when reading files/directories and is ignored in <code>register</code> mode.</li> <li><code>keys</code> help with deduplication and enable downstream CDC patterns; provide them when available.</li> <li><code>type: manual</code> is useful when you want full control over how data is persisted (avoid Fabricks auto-DDL).</li> </ul> <p>Extensibility</p> <ul> <li><code>extender</code>: A Python callable that receives the input DataFrame and returns a transformed DataFrame. Use extenders for custom parsing, enrichment, or complex transformations that are difficult in SQL.</li> <li><code>extender_options</code>: Mapping of options passed to the extender.</li> <li>See also: Extenders, UDFs &amp; Parsers</li> </ul> <p>Operational guidance</p> <ul> <li>Data lineage: set <code>source</code> and <code>parents</code> to make job lineage and ordering explicit.</li> <li>Error handling: use <code>timeout</code> and configure <code>Checks &amp; Data Quality</code> to detect and halt on bad input.</li> <li>Security: use <code>encrypted_columns</code> to protect sensitive fields at write time.</li> </ul> <p>Related</p> <ul> <li>Next steps: Silver Step, Table Options</li> <li>Data quality: Checks &amp; Data Quality</li> <li>Extensibility: Extenders, UDFs &amp; Parsers</li> <li>Sample runtime: Sample runtime</li> </ul>"},{"location":"steps/gold/","title":"Gold Step Reference","text":"<p>Gold steps produce consumption-ready models, typically implemented in SQL. Gold focuses on dimensional models, facts and marts, and prepares data for analytics and reporting.</p> <p>What Gold does - Runs curated SQL transformations or notebooks to produce business-ready tables and views. - Supports full-refresh (<code>complete</code>), append, and merge/update semantics; can also invoke notebooks for complex workflows. - Implements CDC logic for downstream consumers when required (SCD1/SCD2 patterns).</p> <p>Modes</p> Mode Behavior <code>memory</code> In-session view only; no persisted table. Useful for testing or transient outputs. <code>append</code> Append-only writes to the target table. <code>complete</code> Full refresh/overwrite of the target table. Use when you need a consistent snapshot. <code>update</code> Merge/upsert semantics for idempotent updates and CDC. <code>invoke</code> Execute a notebook as the job body (configured via <code>invoker_options</code>). <p>CDC and inputs - Gold jobs often receive CDC-style inputs. Required CDC fields include <code>__key</code>, <code>__timestamp</code>, and <code>__operation</code> for change-point driven inputs. - CDC strategies supported: <code>nocdc</code>, <code>scd1</code>, <code>scd2</code>. See the CDC reference for full details: Change Data Capture (CDC)</p> <p>Common options (summary)</p> Option Purpose <code>type</code> <code>default</code> vs <code>manual</code>. <code>manual</code> disables Fabricks auto-DDL/DML. <code>mode</code> One of: <code>memory</code>, <code>append</code>, <code>complete</code>, <code>update</code>, <code>invoke</code>. <code>change_data_capture</code> CDC strategy: <code>nocdc</code> | <code>scd1</code> | <code>scd2</code>. <code>update_where</code> Predicate to limit rows affected during merge/upsert. <code>parents</code> Upstream dependencies for scheduling and recomputation. <code>deduplicate</code> Drop duplicate keys in the result before writing. <code>persist_last_timestamp</code> Persist the last processed timestamp for incremental loads. <code>correct_valid_from</code> Adjust SCD2 start timestamps for sentinel handling. <code>table</code> Target table override (useful for semantic/table-copy scenarios). <code>table_options</code> Delta table options and metadata (identity, clustering, properties, comments). <code>spark_options</code> Per-job Spark SQL/session options. <code>udfs</code> Path/registry of UDFs to load before executing the job. <code>check_options</code> Configure DQ checks (<code>pre_run</code>, <code>post_run</code>, <code>min_rows</code>, <code>max_rows</code>, etc.). <code>notebook</code> / <code>invoker_options</code> Configure notebook invocation for <code>mode: invoke</code>. <code>requirements</code> If true, install/resolve additional dependencies for this job. <code>timeout</code> Per-job timeout seconds (overrides step defaults). <p>Operational guidance - Use <code>type: manual</code> when you need explicit control over table DDL or persistence. - When using SCD patterns, ensure your inputs include the expected <code>__</code> CDC columns. - Use <code>update_where</code> to scope merges and avoid unintended updates. - For heavy operations, configure <code>table_options</code> (clustering, partitioning, properties) and <code>spark_options</code> appropriately.</p> <p>Examples</p> <p>Append example <pre><code>- job:\n    step: gold\n    topic: fact\n    item: append\n    options:\n      mode: `append`\n</code></pre></p> <p>Complete/overwrite example <pre><code>- job:\n    step: gold\n    topic: dim\n    item: overwrite\n    options:\n      mode: `complete`\n      change_data_capture: `scd1`\n    table_options:\n      identity: true\n</code></pre></p> <p>Deduplicate and ordering examples <pre><code>- job:\n    step: gold\n    topic: fact\n    item: deduplicate\n    options:\n      mode: `complete`\n      deduplicate: true\n</code></pre></p> <p>SQL examples (dedupe/order) <pre><code>-- deduplicate.sql\nselect 1 as __key, 2 as dummy\nunion all\nselect 1 as __key, 1 as dummy\n</code></pre></p> <pre><code>-- order_duplicate.sql\nselect 1 as __key, 1 as dummy, 1 as __order_duplicate_by_desc\nunion all\nselect 1 as __key, 2 as dummy, 2 as __order_duplicate_by_desc\n</code></pre> <p>SCD2 header-line change points (example)</p> <p>This example shows how to generate SCD2 change points when combining header and line SCD2 sources. - Emit upserts at each validity start across header and lines. - Emit a delete for the header-only row as soon as a corresponding line exists (to close the sentinel header row). - Emit deletes at validity end for deleted intervals. - Join additional SCD2 streams (e.g., item price) to ensure change points reflect related dimension changes.</p> <pre><code>with\n  hdr_line_dates as (\n    select h.order_id, l.order_line_id, h.__valid_to, h.__valid_from, h.__is_deleted\n    from silver.order_header_scd2 h\n    left join silver.order_line_scd2 l\n      on h.order_id = l.order_id\n      and h.__valid_from between l.__valid_from and l.__valid_to\n\n    union\n\n    select order_id, order_line_id, __valid_to, __valid_from, __is_deleted\n    from silver.order_line_scd2\n\n    union\n\n    select h.order_id, l.order_line_id, ip.__valid_to, ip.__valid_from, false\n    from silver.order_header_scd2__current h\n    inner join silver.order_line_scd2__current l\n      on h.order_id = l.order_id\n    inner join silver.item_price_scd2 ip\n      on ip.item_id = l.item_id\n     and ip.price_list_id = h.price_list_id\n     and ip.__is_current\n  ),\n\n  change_points as (\n    select order_id, order_line_id, __valid_from as __timestamp, 'upsert' as __operation\n    from hdr_line_dates\n\n    union\n\n    select order_id, null as order_line_id, __valid_from as __timestamp, 'delete' as __operation\n    from hdr_line_dates\n    where order_line_id is not null\n\n    union\n\n    select order_id, order_line_id, __valid_to as __timestamp, 'delete' as __operation\n    from hdr_line_dates\n    where __is_deleted\n  )\n\nselect\n  concat_ws('*', p.order_id, coalesce(p.order_line_id, -1)) as __key,\n  p.order_id,\n  p.order_line_id,\n  h.customer_id,\n  h.price_list_id,\n  h.order_date,\n  l.item_id,\n  l.quantity,\n  l.unit_price,\n  p.__operation,\n  p.__timestamp\nfrom change_points p\nleft join silver.order_header_scd2 h\n  on p.order_id = h.order_id\n and p.__timestamp between h.__valid_from and h.__valid_to\nleft join silver.order_line_scd2 l\n  on p.order_id = l.order_id\n and coalesce(p.order_line_id, -1) = coalesce(l.order_line_id, -1)\n and p.__timestamp between l.__valid_from and l.__valid_to\n</code></pre> <p>Notes - The null-sentinel header row (order_line_id = null -&gt; coerced to -1 in __key) is closed via a <code>delete</code> when any line appears at the same boundary. - Additional SCD2 inputs (e.g., item_price_scd2) can be unioned into the dates set to force change points whenever related dimensions change. - The output stream conforms to Gold SCD2 input fields: <code>__key</code>, <code>__timestamp</code>, <code>__operation</code>.</p> <p>Related - Next steps: Table Options - Data quality: Checks &amp; Data Quality - Extensibility: Extenders, UDFs &amp; Parsers - Sample runtime: Sample runtime</p>"},{"location":"steps/silver/","title":"Silver Step Reference","text":"<p>The Silver step standardizes, cleans, and enriches landed Bronze data. It is the layer where business logic, schema conformance, deduplication, and optional Change Data Capture (CDC) are applied so Gold jobs receive consistent, analytics-ready inputs.</p> <p>Key responsibilities</p> <ul> <li>Ingest and unify upstream outputs (from Bronze or other Silver jobs).</li> <li>Enforce schema, apply business keys, and compute derived columns.</li> <li>Deduplicate, order, and resolve conflicting records.</li> <li>Optionally apply CDC semantics (scd1 or scd2) and produce convenience views such as {table}__current.</li> <li>Write conformed Delta tables or register transient views for consumption.</li> </ul> <p>Modes</p> Mode When to use / behavior memory In-session temporary view only. Use for ad-hoc runs and tests; no persisted table is written. append Append-only writes. Ideal for additive feeds where merges are not required. latest Keep only the latest row per business key within the incoming batch (batch-local). update Merge/upsert semantics against an existing target; used for CDC and idempotent updates. combine Union results from multiple parents into a single logical output. Useful for multi-source catalogs. <p>Behavior notes</p> <ul> <li>Deduplication &amp; ordering</li> <li>Use deduplicate to drop duplicate keys in the incoming batch.</li> <li>order_duplicate_by accepts a sort spec to select the preferred row when duplicates exist.</li> <li> <p>latest applies only to the incoming batch; it does not inspect historical rows. Use update to merge winners into history.</p> </li> <li> <p>Streaming</p> </li> <li>stream: true is supported when your connectors and environment support streaming execution.</li> <li> <p>For stable results in streaming, provide deterministic keys and stable ordering columns. Prefer post_run checks for accuracy.</p> </li> <li> <p>Combine</p> </li> <li>combine unions parent outputs. Parents must be schema-compatible; enable deduplicate if you need to collapse duplicates across parents.</li> </ul> <p>CDC strategies</p> Strategy Effect Output convention nocdc No CDC metadata; write the result as-is. \u2014 scd1 Current-state model with __is_current, __is_deleted flags. {table}__current view available. scd2 Full-history model with validity windows __valid_from, __valid_to. {table}__current view available. <p>Common options (summary)</p> <ul> <li>type: default or manual. manual disables Fabricks auto-DDL/DML; you manage persistence.</li> <li>mode: One of memory, append, latest, update, combine (required).</li> <li>change_data_capture: nocdc, scd1, or scd2 (optional).</li> <li>parents: List upstream job identifiers (recommended for lineage and ordering).</li> <li>keys: Business keys used for deduplication and downstream CDC (recommended where available).</li> <li>deduplicate: Boolean to drop duplicates within the batch.</li> <li>order_duplicate_by: Sort specification to pick the preferred record among duplicates.</li> <li>filter_where: SQL predicate applied during transform to filter rows.</li> <li>extender / extender_options: Python extender and its arguments for advanced transforms.</li> <li>check_options: Data-quality checks (pre_run, post_run, min_rows, max_rows, etc.).</li> <li>stream: Enable streaming semantics when supported.</li> <li>timeout: Per-job timeout in seconds (overrides step default).</li> </ul> <p>Options guidance</p> <ul> <li>Always set mode to reflect the write semantics you need.</li> <li>Use change_data_capture when you need SCD semantics; choose scd1 for current-state and scd2 for historical windows.</li> <li>Provide parents to make dependencies explicit and reduce ambiguous scheduling.</li> <li>Use keys, deduplicate, and order_duplicate_by to handle noisy upstream data.</li> <li>Use type: manual for precise control of DDL/DML and when performing backfills or manual migrations.</li> </ul> <p>Examples</p> <p>Basic SCD2 update <code>``yaml - job:     step: silver     topic: orders     item: orders_scd2     options:       mode: update       change_data_capture: scd2       parents: [bronze.orders_raw]       keys: [order_id]</code></p> <p>Latest-per-key within a batch <code>yaml - job:     step: silver     topic: customers     item: customers_latest     options:       mode: latest       deduplicate: true       order_duplicate_by:         order_by: desc       keys: [customer_id]</code></p> <p>Combine multiple parents (union) <code>yaml - job:     step: silver     topic: product_catalog     item: union_all     options:       mode: combine       parents: [bronze.catalog_a, bronze.catalog_b]       deduplicate: false</code></p> <p>Extender example <code>yaml - job:     step: silver     topic: enrich     item: add_geo     options:       mode: update       extender: enrich_with_geo       extender_options:         country_field: country_code</code></p> <p>Quality gate example <code>yaml - job:     step: silver     topic: princess     item: quality_gate     options:       mode: update       change_data_capture: scd1       parents: [silver.princess_latest]     check_options:       post_run: true       min_rows: 1       max_rows: 100000</code></p> <p>Operational guidance</p> <ul> <li>Lineage &amp; observability: set parents, source, and keys to aid debugging and monitoring.</li> <li>Data quality: prefer check_options to fail fast on contract violations.</li> <li>Streaming: ensure deterministic keys and ordering columns for repeatable results.</li> <li>Manual workflows: use     ype: manual to avoid automatic schema or table changes during operational work.</li> </ul> <p>Related - Next steps: Gold Step, Table Options - Data quality: Checks &amp; Data Quality - Extensibility: Extenders, UDFs &amp; Parsers - Sample runtime: Sample runtime</p>"}]}