{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fabricks User Guide","text":"<p>Fabricks is a pragmatic framework to build Databricks Lakehouse pipelines using YAML for orchestration and SQL for transformations.  It standardizes jobs, steps, schedules, CDC, and checks while keeping development SQL\u2011first.</p>"},{"location":"#steps-overview","title":"Steps Overview","text":"<p>Fabricks organizes your Lakehouse into clear layers. Each step has a dedicated reference with modes, options, and examples.</p>"},{"location":"#bronze","title":"Bronze","text":"<p>Raw ingestion from source systems (files, streams, existing tables). Keep logic light; land data for downstream processing.</p> <ul> <li>Typical modes: memory, append, register</li> <li>Focus: lightweight parsing/landing; no business logic</li> <li>Output: raw tables or temporary views</li> </ul> <p>Read the full reference \u2192 Bronze Step</p>"},{"location":"#silver","title":"Silver","text":"<p>Standardize, clean, and enrich data; optionally apply CDC (SCD1/SCD2). Produces conformed datasets and convenience views.</p> <ul> <li>Typical modes: memory, append, latest, update, combine</li> <li>CDC: nocdc, scd1, scd2 with built-in helpers and views</li> <li>Output: conformed tables and curated views</li> </ul> <p>Read the full reference \u2192 Silver Step</p>"},{"location":"#gold","title":"Gold","text":"<p>Curated business models for analytics and reporting; dimensional or mart\u2011style outputs. Can also invoke notebooks when needed.</p> <ul> <li>Typical modes: memory, append, complete, update, invoke (notebooks)</li> <li>Focus: dimensional models, marts, KPI-ready data</li> <li>Output: business-consumption tables and views</li> </ul> <p>Read the  reference \u2192 Gold Step</p>"},{"location":"#where-to-configure","title":"Where to Configure","text":"<ul> <li>Project configuration, schedules, and structure \u2192 Runtime</li> <li>Data quality and rollback behavior \u2192 Checks &amp; Data Quality</li> <li>Table properties, clustering, and layout \u2192 Table Options</li> <li>Custom logic and reusable SQL assets \u2192 Extenders, UDFs &amp; Views</li> <li>Change Data Capture (CDC) \u2192 CDC</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<ul> <li>Copy the minimal sample and adapt it: Sample runtime</li> <li>Point <code>tool.fabricks.runtime</code> and <code>config</code> to your runtime and start adding jobs in SQL and YAML.</li> </ul>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2024 fabricks-framework</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"release-notes/","title":"Release Notes","text":"<p>For the latest releases and detailed changelogs, please visit the Fabricks Releases page on GitHub.</p>"},{"location":"runtime/","title":"Runtime Configuration","text":"<p>The Fabricks runtime is the folder where your Lakehouse project lives (configs, SQL, jobs, UDFs, extenders, views). This page explains how to point Fabricks to your runtime, how to structure it, and how to configure schedules and step paths.</p>"},{"location":"runtime/#pointing-to-your-runtime","title":"Pointing to your runtime","text":"<p>Configure Fabricks in your project's <code>pyproject.toml</code>:</p> <pre><code>[tool.fabricks]\nruntime = \"tests/integration/runtime\"          # Path to your runtime (jobs, SQL, configs)\nnotebooks = \"fabricks/api/notebooks\"           # Notebook helpers shipped with Fabricks\njob_config_from_yaml = true                     # Enable YAML job config\nloglevel = \"info\"\ndebugmode = false\nconfig = \"tests/integration/runtime/fabricks/conf.uc.fabricks.yml\"  # Main runtime config\n</code></pre> <ul> <li>runtime: path to your project's runtime directory</li> <li>config: path to your main runtime YAML configuration (see below)</li> <li>notebooks: optional helpers used by shipped notebooks</li> <li>job_config_from_yaml: enable loading jobs from YAML files</li> <li>loglevel/debugmode: control logging verbosity</li> </ul>"},{"location":"runtime/#main-runtime-config-yaml","title":"Main runtime config (YAML)","text":"<p>Define project-level options, step defaults, and step path mappings in <code>fabricks/conf.fabricks.yml</code>:</p> <pre><code>name: MyFabricksProject\noptions:\n  secret_scope: my_secret_scope\n  timeouts:\n    step: 3600\n    job: 3600\n    pre_run: 3600\n    post_run: 3600\n  workers: 4\npath_options:\n  storage: /mnt/data\nspark_options:\n  sql:\n    spark.sql.parquet.compression.codec: zstd\n\nbronze:\n  - name: bronze\n    path_options:\n      runtime: src/steps/bronze\n      storage: abfss://bronze@youraccount.blob.core.windows.net\n\nsilver:\n  - name: silver\n    path_options:\n      runtime: src/steps/silver\n      storage: abfss://silver@youraccount.blob.core.windows.net\n\ngold:\n  - name: transf\n    path_options:\n      runtime: src/steps/transf\n      storage: abfss://transf@youraccount.blob.core.windows.net\n  - name: gold\n    path_options:\n      runtime: src/steps/gold\n      storage: abfss://gold@youraccount.blob.core.windows.net\n</code></pre> <p>Key concepts: - options: global project config (secrets, timeouts, worker count) - path_options: shared storage/config paths - spark_options: default Spark SQL options applied for jobs - step sections (bronze/silver/gold/...): list of step instances with their runtime and storage paths</p>"},{"location":"runtime/#schedules","title":"Schedules","text":"<p>Schedules group jobs and define step order. Place schedules in your runtime (commonly under <code>fabricks/schedules/</code>):</p> <pre><code>- schedule:\n    name: test\n    options:\n      tag: test\n      steps: [bronze, silver, transf, gold, semantic]\n      variables:\n        var1: 1\n        var2: 2\n</code></pre> <p>Pass the schedule name when running the shipped notebooks or the Databricks bundle job.</p>"},{"location":"runtime/#typical-runtime-structure","title":"Typical runtime structure","text":"<p>A minimal runtime can look like:</p> <pre><code>fabricks/\n  conf.fabricks.yml\n  schedules/\n    schedule.yml\nbronze/\n  _config.example.yml\nsilver/\n  _config.example.yml\ngold/\n  gold/\n    _config.example.yml\n    hello_world.sql\nsemantic/\n  _config.example.yml\n</code></pre> <ul> <li>bronze/silver/gold/semantic: step folders with YAML configs and SQL</li> <li>fabricks/conf.fabricks.yml: main runtime config for paths and defaults</li> <li>fabricks/schedules/: defines one or more schedules for execution</li> </ul>"},{"location":"runtime/#sample-runtime","title":"Sample runtime","text":"<p>This section contains a minimal, copyable Fabricks runtime you can use to get started quickly. It mirrors the structure used in the integration tests and includes example configs and a working Gold job (<code>hello_world.sql</code>).</p> <p>What this is - A self-contained runtime layout showing how to organize steps (bronze, silver, gold, semantic) - Example YAML configuration files per step - A minimal schedule and config to run a simple job end-to-end</p> <p>Directory overview</p> <pre><code>examples/runtime/\n  fabricks/\n    conf.fabricks.yml\n    schedules/\n      schedule.yml\n  bronze/\n    _config.example.yml\n  silver/\n    _config.example.yml\n  gold/\n    gold/\n      _config.example.yml\n      hello_world.sql\n  semantic/\n    _config.example.yml\n</code></pre> <p>Key files and purpose - <code>fabricks/conf.fabricks.yml</code>: project-level configuration (secret scope, timeouts, workers, storage paths, schedules path) - <code>fabricks/schedules/schedule.yml</code>: minimal schedule to run the gold step - <code>gold/gold/_config.example.yml</code>: defines a simple Gold job - <code>gold/gold/hello_world.sql</code>: example SQL for a Gold job (full refresh mode) - <code>bronze/_config.example.yml</code>, <code>silver/_config.example.yml</code>, <code>semantic/_config.example.yml</code>: example step configurations</p> <p>How to use this sample 1) Point <code>tool.fabricks.runtime</code> to this folder and set <code>config</code> to <code>fabricks/conf.fabricks.yml</code> in your <code>pyproject.toml</code>. 2) Run the Databricks bundle job with <code>schedule: example</code> (see the main documentation for bundle details). 3) Inspect the materialized outputs, logs, and table/view artifacts.</p>"},{"location":"runtime/#related-topics","title":"Related topics","text":"<ul> <li>Step overviews and details:</li> <li>Bronze</li> <li>Silver</li> <li>Gold</li> <li>Data quality checks and contracts: Checks &amp; Data Quality</li> <li>Table properties and physical layout: Table Options</li> <li>Custom logic integration: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"helpers/init/","title":"Initialize Fabricks (Cluster/Notebook Helper)","text":"<p>This helper explains how to initialize Fabricks on a Databricks cluster or local environment by: - Installing the Fabricks library - Pointing Fabricks to your runtime - Running the armageddon script to bootstrap metadata and objects</p> <p>References to the core implementation: - armageddon: https://github.com/fabricks-framework/fabricks/tree/main/framework/fabricks/core/scripts/armageddon.py - Runtime detection and configuration: https://github.com/fabricks-framework/fabricks/tree/main/framework/fabricks/context/runtime.py</p>"},{"location":"helpers/init/#1-install-fabricks","title":"1) Install Fabricks","text":"<p>You need the Fabricks package available on your cluster or local environment.</p> <ul> <li>Databricks cluster (recommended)</li> <li>Libraries \u2192 Install new \u2192 Python PyPI \u2192 fabricks (or install a wheel/artifact you build)</li> <li> <p>Alternatively, attach a workspace library artifact built from this repository</p> </li> <li> <p>Local development (optional)</p> </li> <li>pip install fabricks</li> <li>or from source (for development): pip install -e .[dev,test]</li> </ul> <p>Python &gt;=3.9,&lt;4 is recommended; align with your Databricks LTS runtime.</p>"},{"location":"helpers/init/#2-point-fabricks-to-your-runtime","title":"2) Point Fabricks to your runtime","text":"<p>Fabricks discovers its runtime via either environment variables or [tool.fabricks] in your pyproject.toml. The core lookup logic is implemented in fabricks/context/runtime.py.</p> <p>Option A: Configure via pyproject.toml (preferred for repo-managed projects):</p> <pre><code>[tool.fabricks]\nruntime = \"path/to/your/runtime\"                   # e.g., tests/integration/runtime or examples/runtime\nnotebooks = \"fabricks/api/notebooks\"               # optional: helpers shipped with Fabricks\njob_config_from_yaml = true                        # optional\nloglevel = \"info\"                                  # optional: DEBUG|INFO|WARNING|ERROR|CRITICAL\ndebugmode = false                                  # optional\nconfig = \"path/to/your/runtime/fabricks/conf.fabricks.yml\"  # main runtime YAML\n</code></pre> <p>Option B: Configure via environment variables (useful on clusters): - FABRICKS_RUNTIME: path to your runtime (jobs, SQL, configs) - FABRICKS_CONFIG: full path to your main conf.fabricks.yml (if not set, Fabricks tries to infer a conf.uc..yml) - FABRICKS_NOTEBOOKS: optional helper notebook path - FABRICKS_IS_JOB_CONFIG_FROM_YAML, FABRICKS_LOGLEVEL, FABRICKS_IS_DEBUGMODE: optional toggles <p>Example on Databricks (Cluster \u2192 Configuration \u2192 Advanced options \u2192 Environment variables):</p> <pre><code>FABRICKS_RUNTIME=/Workspace/Repos/your/repo/examples/runtime\nFABRICKS_CONFIG=/Workspace/Repos/your/repo/examples/runtime/fabricks/conf.fabricks.yml\nFABRICKS_LOGLEVEL=INFO\n</code></pre> <p>You can also set env vars in a notebook before importing Fabricks:</p> <pre><code>import os\nos.environ[\"FABRICKS_RUNTIME\"] = \"/Workspace/Repos/your/repo/examples/runtime\"\nos.environ[\"FABRICKS_CONFIG\"] = \"/Workspace/Repos/your/repo/examples/runtime/fabricks/conf.fabricks.yml\"\n# Optional:\n# os.environ[\"FABRICKS_LOGLEVEL\"] = \"INFO\"\n</code></pre> <p>Validation snippet:</p> <pre><code>from fabricks.context.runtime import PATH_RUNTIME, PATH_CONFIG\nprint(\"PATH_RUNTIME:\", PATH_RUNTIME)\nprint(\"PATH_CONFIG:\", PATH_CONFIG)\n</code></pre>"},{"location":"helpers/init/#3-run-armageddon","title":"3) Run armageddon","text":"<p>armageddon performs a one-shot setup aligned with your runtime configuration (e.g., preparing databases/metadata, registering views).</p> <p>Import and call:</p> <pre><code># Databricks or local\nfrom fabricks.core.scripts.armageddon import armageddon\n\n# You may pass one or more steps (bronze, silver, gold, semantic, transf, ...)\n# Examples:\narmageddon(steps=\"gold\")                      # single step\narmageddon(steps=[\"bronze\", \"silver\", \"gold\"])  # multiple steps\narmageddon(steps=None)                        # default behavior, follow runtime config\n</code></pre> <p>Notes: - armageddon will log progress using Fabricks DEFAULT_LOGGER. You can set logging level through FABRICKS_LOGLEVEL or tool.fabricks.loglevel. - Ensure your runtime conf (conf.fabricks.yml) is reachable via PATH_CONFIG and defines required path_options (udfs, parsers, extenders, views, schedules, requirements) and options (secret_scope, unity_catalog/catalog if applicable). - For Unity Catalog, set options.unity_catalog: true and options.catalog accordingly.</p>"},{"location":"helpers/init/#example-databricks-notebook-initialize","title":"Example: Databricks Notebook: Initialize","text":"<p>Create a new notebook (Python) named initialize and include:</p> <pre><code># (Optional) set env vars if not using pyproject.toml\n# import os\n# os.environ[\"FABRICKS_RUNTIME\"] = \"/Workspace/Repos/your/repo/examples/runtime\"\n# os.environ[\"FABRICKS_CONFIG\"] = \"/Workspace/Repos/your/repo/examples/runtime/fabricks/conf.fabricks.yml\"\n# os.environ[\"FABRICKS_LOGLEVEL\"] = \"INFO\"\n\nfrom fabricks.core.scripts.armageddon import armageddon\n# Run for all default steps from your runtime config:\narmageddon(steps=None)\n</code></pre> <p>Attach the Fabricks library to the cluster before running the notebook.</p>"},{"location":"helpers/init/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing env/config:</li> <li>ValueError: Must have at least a pyproject.toml or set FABRICKS_RUNTIME</li> <li> <p>Fix by setting FABRICKS_RUNTIME or adding [tool.fabricks] to pyproject.toml</p> </li> <li> <p>Unity Catalog:</p> </li> <li> <p>Ensure options.unity_catalog is true and options.catalog is set in conf.fabricks.yml</p> </li> <li> <p>Paths and storage:</p> </li> <li> <p>conf.fabricks.yml must define path_options.storage and per-step runtime/storage paths; Fabricks uses these to resolve PATHS_RUNTIME and PATHS_STORAGE</p> </li> <li> <p>Logging:</p> </li> <li>Set FABRICKS_LOGLEVEL or tool.fabricks.loglevel to control verbosity</li> </ul>"},{"location":"helpers/init/#related-topics","title":"Related topics","text":"<ul> <li>Runtime configuration: ../runtime.md</li> <li>Step Helper: ./step.md</li> <li>Job Helper: ./job.md</li> </ul>"},{"location":"helpers/job/","title":"Job Helper","text":"<p>This helper describes a widget-driven approach to manage a Fabricks job. It focuses on lifecycle operations driven by the core job implementation: registering/creating a job, running it (with checks and invokers), schema evolution, and housekeeping.</p> <p>The behaviors documented here are grounded in the framework core:</p> <ul> <li>Generator</li> <li>Processor</li> <li>Checker</li> <li>Invoker</li> <li>Gold</li> <li>factory</li> </ul>"},{"location":"helpers/job/#typical-usage","title":"Typical usage","text":"<p>1) Resolve the job:</p> <pre><code>from fabricks.api import get_job\nj = get_job(job=\"gold.sales.orders\")  # or pass step/topic/item explicitly\n</code></pre> <p>2) Run actions based on widgets:</p> <pre><code># Examples:\nj.check_pre_run()\nj.run(schedule=\"daily\", invoke=True, reload=False)\nj.check_post_run()\nj.check_post_run_extra()\n\n# Other common operations:\nj.update_schema()\nj.overwrite_schema()\nj.register()\nj.create()\n# Cleanup if required:\n# j.truncate()\n# j.drop()\n</code></pre> <p>3) Gold jobs:</p> <pre><code># If your job is gold, ensure UDFs are registered before actions:\nif getattr(j, \"expand\", None) == \"gold\":\n    j.register_udfs()\nj.run()\n</code></pre>"},{"location":"helpers/job/#methods-explained-from-core","title":"Methods explained (from core)","text":"<p>Below is a summary of what each job-level method does in the Fabricks core, as implemented by the base classes. References indicate where behaviors are implemented.</p> <ul> <li> <p>drop() \u2014 Generator.drop</p> <ul> <li>Purpose: Remove all artifacts and CDC related to the job.</li> <li>Reference: fabricks/core/jobs/base/generator.py::Generator.drop</li> </ul> </li> <li> <p>register() \u2014 Generator.register</p> <ul> <li>Purpose: Register the job\u2019s output object (table or view).</li> </ul> </li> <li> <p>Reference: fabricks/core/jobs/base/generator.py::Generator.register</p> </li> <li> <p>create() \u2014 Generator.create</p> <ul> <li>Purpose: Create the physical table or virtual view.</li> </ul> </li> <li> <p>Reference: fabricks/core/jobs/base/generator.py::Generator.create, _create_table</p> </li> <li> <p>truncate() \u2014 Generator.truncate</p> <ul> <li>Purpose: Clear data and artifacts without dropping the table definition.</li> </ul> </li> <li> <p>Reference: fabricks/core/jobs/base/generator.py::Generator.truncate</p> </li> <li> <p>update-schema() \u2014 Generator.update_schema</p> <ul> <li>Purpose: Evolve the table schema in-place.</li> </ul> </li> <li> <p>Reference: fabricks/core/jobs/base/generator.py::Generator.update_schema, _update_schema</p> </li> <li> <p>overwrite-schema() \u2014 Generator.overwrite_schema</p> <ul> <li>Purpose: Replace the table schema to match the computed DataFrame.</li> </ul> </li> <li> <p>Reference: fabricks/core/jobs/base/generator.py::Generator.overwrite_schema, _update_schema</p> </li> <li> <p>run(...) \u2014 Processor.run</p> <ul> <li>Purpose: Orchestrate a full run with optional invocations, checks, and rollback semantics.</li> <li>Distinct exceptions:<ul> <li><code>SkipRunCheckWarning</code>, <code>PreRunCheckWarning</code>, <code>PostRunCheckWarning</code></li> <li><code>PreRunInvokeException</code>, <code>PostRunInvokeException</code></li> <li><code>PreRunCheckException</code>, <code>PostRunCheckException</code></li> <li>plus general failures resulting in rollback for persisted jobs</li> </ul> </li> <li>Reference: fabricks/core/jobs/base/processor.py::Processor.run</li> </ul> </li> <li> <p>for-each-run(...) \u2014 Processor.for_each_run</p> <ul> <li>Purpose: Execute the core per-run logic, stream-aware.</li> </ul> </li> <li> <p>Reference: fabricks/core/jobs/base/processor.py::Processor.for_each_run, _for_each_batch; error type: SchemaDriftError</p> </li> <li> <p>overwrite() \u2014 Processor.overwrite (abstract)</p> <ul> <li>Purpose: Replace table/view contents with new results in one operation.</li> </ul> </li> <li> <p>Reference: fabricks/core/jobs/base/processor.py::Processor.overwrite</p> </li> <li> <p>pre-run-invoke() \u2014 Invoker.invoke_pre_run</p> <ul> <li>Purpose: Execute configured invokers before the run.</li> <li>Behavior:<ul> <li>Invokes both job-level and step-level invokers: <code>invoke_job(position=\"pre_run\")</code>, <code>invoke_step(position=\"pre_run\")</code></li> <li>Failures raise <code>PreRunInvokeException</code></li> </ul> </li> <li>Reference: fabricks/core/jobs/base/invoker.py::{invoke_pre_run, invoke_job, invoke_step, invoke}</li> </ul> </li> <li> <p>post-run-invoke() \u2014 Invoker.invoke_post_run</p> <ul> <li>Purpose: Execute configured invokers after the run.</li> <li>Behavior:<ul> <li>Invokes job-level and step-level invokers with <code>position=\"post_run\"</code>.</li> <li>Failures raise <code>PostRunInvokeException</code></li> </ul> </li> </ul> </li> <li> <p>Reference: fabricks/core/jobs/base/invoker.py::{invoke_post_run, invoke_job, invoke_step}</p> </li> <li> <p>pre-run-check() \u2014 Checker.check_pre_run</p> <ul> <li>Purpose: Validate pre-conditions via SQL contracts.</li> <li>Behavior:<ul> <li>If enabled in options, looks for <code>.pre_run.sql</code> in the runtime path and executes it.</li> <li>Expects rows with columns <code>__action</code> in {'fail','warning'} and <code>__message</code>.</li> <li>Failures raise <code>PreRunCheckException</code>; warnings raise <code>PreRunCheckWarning</code>.</li> </ul> </li> <li>Reference: fabricks/core/jobs/base/checker.py::{check_pre_run, _check}</li> </ul> </li> <li> <p>post-run-check() \u2014 Checker.check_post_run (+ extras)</p> <ul> <li>Purpose: Validate post-conditions via SQL contracts and row constraints.</li> <li>Behavior:<ul> <li>Runs <code>.post_run.sql</code> if enabled and enforces fail/warning actions like pre-run.</li> <li>Additionally runs <code>check_post_run_extra()</code> (min/max rows and count equality).</li> <li>Failures raise <code>PostRunCheckException</code>; warnings raise <code>PostRunCheckWarning</code>.</li> </ul> </li> </ul> </li> <li> <p>Reference: fabricks/core/jobs/base/checker.py::{check_post_run, check_post_run_extra}</p> </li> <li> <p>register_udfs() \u2014 Gold.register_udfs (only for Gold jobs)</p> <ul> <li>Purpose: Register UDFs required by Gold transformations before running actions.</li> <li>Reference: fabricks/core/jobs/gold.py (class <code>Gold</code>)</li> </ul> </li> </ul>"},{"location":"helpers/job/#notes-and-recommendations","title":"Notes and recommendations","text":"<ul> <li>On persisted jobs, schema drift during a run can raise <code>SchemaDriftError</code>. Consider passing <code>reload=True</code> to update schema automatically when appropriate, or set <code>self.schema_drift=True</code> in configuration.</li> <li>Use <code>for_each_run</code> for per-batch/stream processing where your concrete job implements <code>for_each_batch</code>.</li> <li>Invokers and checks are optional and driven by configuration; enable or disable them from your job options.</li> <li>Be explicit with <code>invoke</code> and <code>reload</code> flags on <code>run</code> to control invocations and schema handling.</li> </ul>"},{"location":"helpers/job/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze \u2022 Silver \u2022 Gold</li> <li>Runtime overview and sample runtime: Runtime</li> <li>Checks &amp; Data Quality: Checks and Data Quality</li> <li>Table options and storage layout: Table Options</li> <li>Extenders, UDFs &amp; Views: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"helpers/step/","title":"Step Helper (Databricks Notebook)","text":"<p>This helper describes a widget-driven approach in a Databricks notebook to manage a Fabricks step (bronze, silver, gold). It focuses on lifecycle operations driven by the core <code>BaseStep</code> implementation: creating/updating a step, discovering jobs, updating metadata (jobs/tables/views/dependencies), registering objects, and housekeeping.</p> <p>The behaviors documented here are grounded in the framework core:</p> <ul> <li>BaseStep</li> <li>factory</li> <li>timeouts</li> </ul>"},{"location":"helpers/step/#overview","title":"Overview","text":"<p>At a high level, a step helper would:</p> <ul> <li>Present widgets to select a step (bronze/silver/gold), actions, and optional filters (topics)</li> <li>Optionally toggle update/cleanup behaviors and progress bars</li> <li>Resolve the step via <code>fabricks.core.steps.get_step</code></li> <li>Execute selected actions in sequence</li> <li>Report progress/errors</li> </ul> <p>Core imports used:</p> <pre><code>from fabricks.core.steps.get_step import get_step\nfrom fabricks.context.log import DEFAULT_LOGGER\n</code></pre>"},{"location":"helpers/step/#typical-usage-in-a-databricks-notebook","title":"Typical usage in a Databricks notebook","text":"<p>1) Resolve the step:</p> <pre><code>from fabricks.core.steps.get_step import get_step\ns = get_step(\"gold\")  # or \"bronze\"/\"silver\"\n</code></pre> <p>2) Run actions based on widgets:</p> <pre><code># Examples:\ns.update(update_dependencies=True, progress_bar=True)\ns.register(update=True, drop=False)\ns.update_dependencies(topic=[\"sales\", \"finance\"], progress_bar=False)\ns.create()\n# Cleanup if required:\n# s.drop()\n</code></pre> <p>3) To inspect jobs and dependencies:</p> <pre><code>jobs_df = s.get_jobs()\ndeps_df, errors = s.get_dependencies(progress_bar=True, topic=\"sales\")\n</code></pre>"},{"location":"helpers/step/#methods-explained-from-core","title":"Methods explained (from core)","text":"<p>Below is a summary of what each step-level method does in the Fabricks core, as implemented by <code>BaseStep</code>. References indicate where behaviors are implemented.</p> <ul> <li> <p>drop() \u2014 BaseStep.drop</p> <ul> <li>Purpose: Remove all artifacts for a step and drop its database and metadata tables.</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.drop</li> </ul> </li> <li> <p>create() \u2014 BaseStep.create</p> <ul> <li>Purpose: Initialize the step by updating resources if the runtime exists.</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.create</li> </ul> </li> <li> <p>update(update_dependencies=True, progress_bar=False) \u2014 BaseStep.update</p> <ul> <li>Purpose: Synchronize all objects for the step (jobs, dependencies, tables, views).</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.update</li> </ul> </li> <li> <p>get_dependencies(progress_bar=False, topic=None) \u2014 BaseStep.get_dependencies</p> <ul> <li>Purpose: Collect job dependencies across the step.</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.get_dependencies</li> </ul> </li> <li> <p>get_jobs_iter(topic=None) \u2014 BaseStep.get_jobs_iter</p> <ul> <li>Purpose: Iterate YAML job definitions for the step (raw).</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.get_jobs_iter</li> </ul> </li> <li> <p>get_jobs(topic=None) \u2014 BaseStep.get_jobs</p> <ul> <li>Purpose: Load step jobs into a Spark DataFrame with schema, computed <code>job_id</code>, and de-duplication.</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.get_jobs</li> </ul> </li> <li> <p>create_jobs(retry=True) \u2014 BaseStep.create_jobs</p> <ul> <li>Purpose: Create missing jobs (tables/views) physically.</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.create_jobs</li> </ul> </li> <li> <p>update_jobs(drop=False) \u2014 BaseStep.update_jobs</p> <ul> <li>Purpose: Refresh the SCD1 metadata table for jobs.</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.update_jobs</li> </ul> </li> <li> <p>update_tables() \u2014 BaseStep.update_tables</p> <ul> <li>Purpose: Refresh the SCD1 metadata table for physical tables.</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.update_tables</li> </ul> </li> <li> <p>update_views() \u2014 BaseStep.update_views</p> <ul> <li>Purpose: Refresh the SCD1 metadata table for views.</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.update_views</li> </ul> </li> <li> <p>update_dependencies(progress_bar=False, topic=None) \u2014 BaseStep.update_dependencies</p> <ul> <li>Purpose: Refresh the SCD1 metadata table for dependencies.</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.update_dependencies</li> </ul> </li> <li> <p>register(update=False, drop=False) \u2014 BaseStep.register</p> <ul> <li>Purpose: Ensure the step database exists and all job outputs are registered (typically views for virtual jobs or tables where required).</li> <li>Reference: fabricks/core/steps/base.py::BaseStep.register</li> </ul> </li> <li> <p>Properties and configuration \u2014 BaseStep</p> <ul> <li>workers<ul> <li>Resolved from step options or global runtime options; cached.</li> </ul> </li> <li>timeouts: <code>Timeouts(job: int, step: int)</code><ul> <li>Resolved from step options or global runtime options; cached.</li> </ul> </li> <li>conf / options<ul> <li>Step configuration and its <code>options</code> loaded from global <code>STEPS</code> settings and runtime YAML structure.</li> </ul> </li> <li>str \u2192 step name</li> </ul> </li> </ul> <p>References:</p> <ul> <li>fabricks/core/steps/base.py::BaseStep</li> <li>fabricks/core/steps/get_step.py::get_step</li> <li>fabricks/core/steps/_types.py::Timeouts</li> </ul>"},{"location":"helpers/step/#notes-and-recommendations","title":"Notes and recommendations","text":"<ul> <li>Ensure your runtime directory structure for the step exists before calling <code>create()</code> or <code>update()</code>. If not, the helper will log a warning.</li> <li>Consider enabling progress bars only for development/debug runs; for production, keep logs terse.</li> <li>When filtering by <code>topic</code>, pass a string or a list of strings; the helper will scope the updates accordingly.</li> <li>On large repositories, job creation/registration is parallelized with a fixed default of 16 workers in the current implementation.</li> </ul>"},{"location":"helpers/step/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze \u2022 Silver \u2022 Gold</li> <li>Runtime overview and sample runtime: Runtime</li> <li>Checks &amp; Data Quality: Checks and Data Quality</li> <li>Table options and storage layout: Table Options</li> <li>Extenders, UDFs &amp; Views: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"reference/cdc/","title":"Change Data Capture (CDC)","text":"<p>Fabricks provides built-in Change Data Capture (CDC) patterns to keep downstream tables synchronized with upstream changes using SQL-first pipelines. CDC is enabled per job via <code>options.change_data_capture</code> and implemented by generated MERGE/INSERT statements driven by small helper columns.</p> <p>This page explains the supported CDC strategies, required inputs, merge semantics, and examples.</p>"},{"location":"reference/cdc/#strategies","title":"Strategies","text":"Strategy Description Convenience views <code>nocdc</code> No CDC; writes the result as-is. \u2014 <code>scd1</code> Tracks current vs deleted; maintains flags <code>__is_current</code>, <code>__is_deleted</code>. <code>{table}__current</code> in Silver <code>scd2</code> Slowly Changing Dimension Type 2: validity windows with <code>__valid_from</code>, <code>__valid_to</code>. <code>{table}__current</code> in Silver <ul> <li>Silver jobs commonly produce conformed datasets with optional CDC applied.</li> <li>Gold jobs typically consume CDC tables and may perform additional merges.</li> </ul> <p>See also: - Silver step reference: Silver - Gold step reference: Gold</p>"},{"location":"reference/cdc/#how-to-enable-cdc","title":"How to enable CDC","text":"<p>Set the CDC strategy in the job options:</p> <pre><code>- job:\n    step: silver\n    topic: demo\n    item: scd1\n    options:\n      mode: update\n      change_data_capture: scd1\n      parents: [bronze.demo_source]\n</code></pre> <p>For Gold:</p> <pre><code>- job:\n    step: gold\n    topic: scd2\n    item: update\n    options:\n      mode: update\n      change_data_capture: scd2\n</code></pre> <p>Supported values: <code>nocdc</code>, <code>scd1</code>, <code>scd2</code>.</p>"},{"location":"reference/cdc/#input-contracts","title":"Input contracts","text":"<p>Some helper columns govern CDC behavior. Fabricks generates additional internal helpers during processing.</p> <ul> <li> <p>Gold jobs (consumer side):</p> <ul> <li>scd2 (required): <code>__key</code>, <code>__timestamp</code>, <code>__operation</code> with values <code>'upsert' | 'delete' | 'reload'</code>.</li> <li>scd1 (required): <code>__key</code>; optional <code>__timestamp</code> / <code>__operation</code> (<code>'upsert' | 'delete' | 'reload'</code>) for delete/rectify handling.</li> <li>Note: If <code>__operation</code> is absent in Gold SCD update jobs, Fabricks auto-injects <code>__operation = 'reload'</code> and enables rectification.</li> <li>Optional helpers used by merges:<ul> <li><code>__order_duplicate_by_asc</code> / <code>__order_duplicate_by_desc</code></li> <li><code>__identity</code> (only when <code>table_options.identity</code> is not true; if <code>identity: true</code>, the identity column is auto-created and you should not supply <code>__identity</code>)</li> <li><code>__source</code> (to scope merges by logical source)</li> </ul> </li> </ul> </li> <li> <p>Silver jobs (producer side):</p> <ul> <li>Provide business keys through job-level <code>keys</code> (or compute a <code>__key</code>) to support downstream CDC.</li> <li>Silver can apply CDC directly and yields convenience views (e.g., <code>{table}__current</code>).</li> </ul> </li> </ul> <p>Note</p> <ul> <li>Memory outputs ignore columns that start with <code>__</code>.</li> <li>Special characters in column names are preserved.</li> </ul> <p>See details in:</p> <ul> <li>Gold</li> <li>Silver</li> <li>Table Options</li> </ul>"},{"location":"reference/cdc/#merge-semantics-under-the-hood","title":"Merge semantics (under the hood)","text":"<p>Fabricks compiles CDC operations into SQL via Jinja templates at runtime. The core logic lives in <code>fabricks.cdc</code>:</p> <ul> <li><code>Merger.get_merge_query</code> renders <code>templates/merge.sql.jinja</code> for the selected <code>change_data_capture</code> strategy.</li> <li>The framework computes internal columns such as:<ul> <li><code>__merge_condition</code> \u2014 one of <code>'upsert' | 'delete' | 'update' | 'insert'</code> depending on strategy and inputs.</li> <li><code>__merge_key</code> \u2014 a synthetic key used to join against the target.</li> </ul> </li> <li>You usually do not set these internal fields manually; they are derived from your inputs (<code>__key</code>, <code>__operation</code>, <code>__timestamp</code>) and job options.</li> </ul> <p>Join keys</p> <ul> <li>If a <code>__key</code> column exists in the target, merges use <code>t.__key = s.__merge_key</code>.</li> <li>Otherwise, the configured <code>keys</code> option is used to build an equality join on business keys.</li> </ul> <p>Source scoping</p> <ul> <li>If <code>__source</code> exists in both sides, merges add <code>t.__source = s.__source</code> to support multi-source data in the same table.</li> </ul> <p>Soft delete vs hard delete (SCD1)</p> <ul> <li>If the incoming data contains <code>__is_deleted</code>, the SCD1 template performs soft deletes:<ul> <li>Sets <code>__is_current = false</code>, <code>__is_deleted = true</code> on delete.</li> </ul> </li> <li>If <code>__is_deleted</code> is absent, deletes are physical for SCD1.</li> </ul> <p>Timestamps and metadata</p> <ul> <li>If the incoming data provides <code>__timestamp</code>, it is propagated to the target.</li> <li>If the target has <code>__metadata</code>, the <code>updated</code> timestamp is set to the current time during updates/deletes.</li> </ul> <p>Identity and hash</p> <ul> <li>If <code>table_options.identity: true</code>, the identity column is created automatically when the table is created.</li> <li>If <code>table_options.identity</code> is not true and <code>__identity</code> is present in the input, it will be written as a regular column.</li> <li>If <code>__hash</code> is present, it is updated during upsert operations.</li> </ul> <p>Update filtering</p> <ul> <li><code>options.update_where</code> can constrain rows affected during merges (useful for limiting the scope of updates).</li> </ul> <p>Internals reference</p> <ul> <li><code>framework/fabricks/cdc/base/merger.py</code></li> <li>Templates under <code>framework/fabricks/cdc/templates/merge/*.sql.jinja</code></li> </ul>"},{"location":"reference/cdc/#scd1-details","title":"SCD1 details","text":"<p>Behavior (see <code>merge/scd1.sql.jinja</code>)</p> <ul> <li>Upsert (<code>__merge_condition = 'upsert'</code>): updates matching rows and inserts non\u2011matching rows.</li> <li>Delete (<code>__merge_condition = 'delete'</code>):<ul> <li>Soft delete if <code>__is_deleted</code> is part of the schema: sets <code>__is_current = false</code>, <code>__is_deleted = true</code>.</li> <li>Otherwise, performs a physical delete.</li> </ul> </li> </ul> <p>Convenience view (in Silver)</p> <ul> <li><code>{table}__current</code>: filters current (non\u2011deleted) rows for simplified consumption.</li> </ul> <p>Minimal Silver example</p> <pre><code>- job:\n    step: silver\n    topic: monarch\n    item: scd1\n    options:\n      mode: update\n      change_data_capture: scd1\n</code></pre> <p>Gold consumption example</p> <pre><code>-- Example: consuming current rows from SCD1 silver output\nselect id as id, name as monarch\nfrom silver.monarch_scd1__current\n</code></pre>"},{"location":"reference/cdc/#reload-operation","title":"Reload operation","text":"<p><code>'reload'</code> is a CDC input operation used as a reconciliation boundary. It is not a direct MERGE action; instead, it signals Fabricks to rectify the target table using the supplied snapshot at that timestamp.</p> <ul> <li>Purpose: mark a full or partial snapshot boundary so missing keys can be treated as deletes and present keys as upserts as needed.</li> <li>Auto-injection: when a Gold SCD job runs in <code>mode: update</code> and your SQL does not provide <code>__operation</code>, Fabricks injects <code>__operation = 'reload'</code> and turns on rectification.</li> <li>Silver behavior:<ul> <li>If a batch contains <code>'reload'</code> after the target\u2019s max timestamp, Silver enables rectification logic.</li> <li>In <code>mode: latest</code>, <code>'reload'</code> is not allowed and will be rejected.</li> </ul> </li> <li>Gold behavior:<ul> <li>Passing <code>reload=True</code> to a Gold job run triggers a full <code>complete</code> write for that run.</li> </ul> </li> <li>Internals: rectification is computed in <code>framework/fabricks/cdc/templates/query/rectify.sql.jinja</code>, which computes next operations/windows around <code>'reload'</code> markers.</li> </ul> <p>Tip:</p> <p>You generally do not need to emit <code>'reload'</code> manually in Gold SCD update jobs; it is injected for you when <code>__operation</code> is missing.  For explicit control, you can produce rows with <code>__operation = 'reload'</code> at the snapshot timestamp.</p>"},{"location":"reference/cdc/#scd2-details","title":"SCD2 details","text":"<p>Behavior (see <code>merge/scd2.sql.jinja</code>):</p> <ul> <li>Update (<code>__merge_condition = 'update'</code>): closes the current row by setting <code>__valid_to = __valid_from - 1 second</code>, <code>__is_current = false</code>. A subsequent insert creates the new current row.</li> <li>Delete (<code>__merge_condition = 'delete'</code>): closes the current row and sets <code>__is_current = false</code> (and <code>__is_deleted = true</code> if soft delete is modeled).</li> <li>Insert (<code>__merge_condition = 'insert'</code>): inserts a new current row.</li> </ul> <p>Required Gold inputs:</p> <ul> <li><code>__key</code>, <code>__timestamp</code>, <code>__operation</code> with values <code>'upsert' | 'delete' | 'reload'</code>.</li> </ul> <p>Reload notes:</p> <ul> <li><code>'reload'</code> marks a reconciliation boundary; Fabricks derives concrete actions (e.g., closing current rows, inserting new ones, deleting missing keys) across that boundary.</li> <li>If you omit <code>__operation</code> in Gold SCD update jobs, Fabricks injects <code>'reload'</code> and enables rectification automatically.</li> <li>In Silver:<ul> <li>Presence of <code>'reload'</code> (beyond target\u2019s max timestamp) enables rectification.</li> <li><code>'reload'</code> is forbidden in <code>mode: latest</code>.</li> </ul> </li> </ul> <p>Optional features:</p> <ul> <li><code>options.correct_valid_from</code>: adjusts start timestamps for validity windows.</li> <li><code>options.persist_last_timestamp</code>: persists last processed timestamp for incremental loads.</li> </ul> <p>Convenience view:</p> <ul> <li><code>{table}__current</code>: returns only the latest (current) rows per business key.</li> </ul> <p>Minimal Silver example:</p> <pre><code>- job:\n    step: silver\n    topic: monarch\n    item: scd2\n    options:\n      mode: update\n      change_data_capture: scd2\n</code></pre> <p>Gold input construction example:</p> <pre><code>-- Turn SCD2 changes into Gold input operations\nwith dates as (\n  select id as id, __valid_from as __timestamp, 'upsert' as __operation\n  from silver.monarch_scd2 where __valid_from &gt; '1900-01-02'\n  union\n  select id as id, __valid_to as __timestamp, 'delete' as __operation\n  from silver.monarch_scd2 where __is_deleted\n)\nselect\n  d.id as __key,\n  s.id as id,\n  s.name as monarch,\n  s.doubleField as value,\n  d.__operation,\n  if(d.__operation = 'delete', d.__timestamp + interval 1 second, d.__timestamp) as __timestamp\nfrom dates d\nleft join silver.monarch_scd2 s\n  on d.id = s.id and d.__timestamp between s.__valid_from and s.__valid_to\n</code></pre>"},{"location":"reference/cdc/#keys-and-__key","title":"Keys and <code>__key</code>","text":"<ul> <li>Preferred: produce a stable <code>__key</code> in your SELECTs (e.g., UDF that hashes business keys).</li> <li>Alternative: configure <code>options.keys: [ ... ]</code> to specify business keys. Fabricks derives join predicates from these when <code>__key</code> is not present.</li> </ul> <p>Tip</p> <ul> <li>Only provide <code>__identity</code> when <code>table_options.identity</code> is not true. If <code>identity: true</code>, the identity column is auto-created when the table is created; do not include <code>__identity</code>. See Table Options.</li> </ul>"},{"location":"reference/cdc/#examples","title":"Examples","text":"<p>Silver SCD1 with duplicates handling</p> <pre><code>- job:\n    step: silver\n    topic: princess\n    item: order_duplicate\n    options:\n      mode: update\n      change_data_capture: scd1\n      order_duplicate_by:\n        order_by: desc\n</code></pre> <p>Gold SCD1 update with incremental timestamp</p> <pre><code>- job:\n    step: gold\n    topic: scd1\n    item: last_timestamp\n    options:\n      change_data_capture: scd1\n      mode: update\n      persist_last_timestamp: true\n</code></pre>"},{"location":"reference/cdc/#operational-notes","title":"Operational notes","text":"<ul> <li>Streaming: where supported, <code>options.stream: true</code> enables incremental semantics.</li> <li>Parents: use <code>options.parents</code> to order upstream dependencies.</li> <li>Checks: configure quality gates via <code>options.check_options</code>. See Checks &amp; Data Quality.</li> </ul>"},{"location":"reference/cdc/#related","title":"Related","text":"<ul> <li>Steps: Silver, Gold</li> <li>Reference: Table Options, Extenders, UDFs &amp; Views</li> <li>Sample runtime: Sample runtime</li> </ul>"},{"location":"reference/checks-data-quality/","title":"Checks and Data Quality","text":"<p>Fabricks provides built-in mechanisms to validate data before and/or after a job runs. Checks can enforce row counts, equality constraints, and custom SQL contracts that return actions and messages.</p>"},{"location":"reference/checks-data-quality/#configuring-checks","title":"Configuring checks","text":"<p>Enable checks per job using <code>check_options</code>:</p> <pre><code>- job:\n    step: gold\n    topic: check\n    item: max_rows\n    options: { mode: complete }\n    check_options:\n      pre_run: true         # run checks before writing\n      post_run: true        # run checks after writing\n      min_rows: 10          # minimum expected row count\n      max_rows: 10000       # maximum expected row count\n      count_must_equal: fabricks.dummy  # enforce row count equality with another table\n      skip: false           # skip all checks if true\n</code></pre> <p>Supported options: - pre_run/post_run: Run checks around the execution of the job. - min_rows/max_rows: Row count bounds. - count_must_equal: Assert row count equals a reference table. - skip: Disable checks for this job.</p>"},{"location":"reference/checks-data-quality/#sql-contracts","title":"SQL contracts","text":"<p>When <code>pre_run</code> or <code>post_run</code> checks are enabled, provide SQL files named after the table or job: - <code>table_name.pre_run.sql</code> - <code>table_name.post_run.sql</code></p> <p>Each SQL must return: - <code>__action</code>: either <code>'fail'</code> or <code>'warning'</code> - <code>__message</code>: human-readable message</p> <p>Example:</p> <pre><code>-- fail.pre_run.sql\nselect \"fail\" as __action, \"Please don't fail on me :(\" as __message;\n\n-- warning.post_run.sql\nselect \"warning\" as __action, \"I want you to warn me !\" as __message;\n</code></pre>"},{"location":"reference/checks-data-quality/#behavior-and-rollback","title":"Behavior and rollback","text":"<ul> <li>For physical tables (append/complete/update modes), a failed check triggers an automatic rollback to the previous successful version.</li> <li>For <code>memory</code> mode (views), results are not persisted and failures are logged only.</li> </ul>"},{"location":"reference/checks-data-quality/#examples","title":"Examples","text":"<ul> <li>Working scenarios are available in the sample runtime:</li> <li>Browsable: Sample runtime</li> <li>Rich integration scenarios (repository layout):</li> <li><code>framework/tests/integration/runtime/gold/gold/</code></li> </ul>"},{"location":"reference/checks-data-quality/#related-topics","title":"Related topics","text":"<ul> <li>Step references:</li> <li>Bronze</li> <li>Silver</li> <li>Gold</li> <li>Table properties and physical layout: Table Options</li> <li>Custom logic integration: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"reference/extenders-udfs-views/","title":"Extenders, UDFs, and Views","text":"<p>This reference explains how to extend Fabricks with custom Python code and reusable SQL assets: - Extenders: Python functions that transform a Spark DataFrame before it is written. - UDFs: User-defined functions you register on the Spark session and use in SQL. - Views: Reusable SQL views you ship with your runtime and reference from jobs. - Parsers: Source-specific readers/cleaners that return a DataFrame (optional, advanced).</p> <p>Use these to encapsulate business logic, reuse patterns, and keep SQL jobs focused and readable.</p>"},{"location":"reference/extenders-udfs-views/#extenders","title":"Extenders","text":"<p>Extenders are Python functions that take a DataFrame and return a transformed DataFrame. They are applied during job execution (typically in Silver/Gold) right before write.</p> <ul> <li>Location: Put Python modules under your runtime, for example <code>fabricks/extenders</code>.</li> <li>Referencing: In job YAML use <code>extender: name</code> and optionally <code>extender_options: { ... }</code> to pass arguments.</li> </ul> <p>Example (adds a <code>country</code> column):</p> <pre><code>from pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom fabricks.core.extenders import extender\n\n@extender(name=\"add_country\")\ndef add_country(df: DataFrame, **kwargs) -&gt; DataFrame:\n    return df.withColumn(\"country\", lit(kwargs.get(\"country\", \"Unknown\")))\n</code></pre> <p>In a job:</p> <pre><code>- job:\n    step: silver\n    topic: sales_analytics\n    item: daily_summary\n    options:\n      mode: update\n    extender: add_country\n    extender_options:\n      country: CH\n</code></pre> <p>Notes: - Extenders should be idempotent and fast. - Avoid heavy I/O; handle source reading in Bronze or via a Parser. - Prefer small, composable transformations.</p>"},{"location":"reference/extenders-udfs-views/#udfs","title":"UDFs","text":"<p>UDFs are registered on the Spark session and then callable from SQL. Place the UDF registration code in your runtime (e.g., <code>fabricks/udfs</code>), and ensure it runs before jobs that need it (for example during initialization or via a notebook hook).</p> <p>Example (simple addition UDF):</p> <pre><code>from pyspark.sql import SparkSession\nfrom fabricks.core.udfs import udf\n\n@udf(name=\"additition\")\ndef additition(spark: SparkSession):\n    def _add(a: int, b: int) -&gt; int:\n        return a + b\n    spark.udf.register(\"udf_additition\", _add)\n</code></pre> <p>Using built-in helpers and a custom UDF in SQL:</p> <pre><code>select\n  udf_identity(s.id, id) as __identity,\n  udf_key(array(s.id)) as __key,\n  s.id as id,\n  s.name as monarch,\n  udf_additition(1, 2) as addition\nfrom silver.monarch_scd1__current s\n</code></pre> <p>Notes: - UDFs should validate inputs and avoid side-effects. - Prefer Spark SQL built-ins and DataFrame functions where possible for performance.</p>"},{"location":"reference/extenders-udfs-views/#views","title":"Views","text":"<p>Views package reusable SQL logic you can reference across jobs.</p> <ul> <li>Location: Place them under your runtime, e.g., <code>fabricks/views</code>.</li> <li>Usage: Reference them directly in SQL of downstream jobs:   <code>sql   select * from fabricks.views.dim_calendar</code></li> <li>Keep views small and well-documented; use them to standardize joins, filters, and derivations.</li> </ul>"},{"location":"reference/extenders-udfs-views/#parsers-optional","title":"Parsers (optional)","text":"<p>Parsers read and lightly clean raw data. They return a DataFrame and should not write output or mutate state.</p> <p>What a parser should do: - Read raw data from <code>data_path</code> (batch or stream based on <code>stream</code> flag) - Optionally apply/validate a schema from <code>schema_path</code> - Perform light, source-specific cleanup (drops/renames/type fixes) - Return a Spark DataFrame (no writes, no side effects)</p> <p>Inputs: - <code>data_path</code>: source location (directory or file) - <code>schema_path</code>: optional schema location (e.g., JSON/DDL) - <code>spark</code>: active <code>SparkSession</code> - <code>stream</code>: when true, prefer <code>readStream</code> where supported</p> <p>Reference in a job:</p> <pre><code>- job:\n    step: bronze\n    topic: demo\n    item: source\n    options:\n      mode: append\n      uri: /mnt/demo/raw/demo\n      parser: monarch\n    parser_options:\n      file_format: parquet\n      read_options:\n        mergeSchema: true\n</code></pre> <p>Example implementation:</p> <pre><code>from pyspark.sql import DataFrame, SparkSession\nfrom fabricks.core.parsers import parser\nfrom fabricks.utils.path import Path\n\n@parser(name=\"monarch\")\nclass MonarchParser:\n    def parse(self, data_path: Path, schema_path: Path, spark: SparkSession, stream: bool) -&gt; DataFrame:\n        df = spark.read.parquet(data_path.string)\n        cols_to_drop = [c for c in df.columns if c.startswith(\"BEL_\")]\n        if cols_to_drop:\n            df = df.drop(*cols_to_drop)\n        return df\n</code></pre> <pre><code>from pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom fabricks.core.extenders import extender\n\n@extender(name=\"add_country\")\ndef add_country(df: DataFrame, **kwargs) -&gt; DataFrame:\n    return df.withColumn(\"country\", lit(kwargs.get(\"country\")))\n</code></pre>"},{"location":"reference/extenders-udfs-views/#related","title":"Related","text":"<ul> <li>Steps: Bronze \u2022 Silver \u2022 Gold</li> <li>Reference: Checks &amp; Data Quality \u2022 Table Options</li> <li>Runtime: Runtime Configuration</li> </ul>"},{"location":"reference/table-options/","title":"Table Options","text":"<p>Table options control how Fabricks creates and manages physical Delta tables across steps. Use these options to define properties, layout, constraints, identity, retention, and more. Options are generally provided under <code>table_options</code> at the job level (and may also be set as step defaults in your runtime config).</p>"},{"location":"reference/table-options/#common-options","title":"Common options","text":"<ul> <li>identity: Enables a Delta identity column on the target table. If <code>identity: true</code>, the identity column is auto-created when the table is created.</li> <li>liquid_clustering: Enables Delta Liquid Clustering where supported by the Databricks runtime.</li> <li>partition_by: List of partition columns (e.g., <code>[date_id, country]</code>).</li> <li>zorder_by: Columns used for Z-Ordering (improves data skipping).</li> <li>cluster_by: Logical clustering keys (materialization depends on runtime features).</li> <li>bloomfilter_by: Columns to maintain Bloom filters on (where supported).</li> <li>constraints: Map of table constraints (e.g., <code>primary key(__key)</code>).</li> <li>properties: Arbitrary Delta table properties (key/value pairs).</li> <li>comment: Table comment/description.</li> <li>calculated_columns: Map of computed columns populated on write.</li> <li>retention_days: Table VACUUM retention period (days).</li> <li>powerbi: true to apply Power BI\u2013specific metadata (if supported).</li> </ul>"},{"location":"reference/table-options/#minimal-example","title":"Minimal example","text":"<pre><code>- job:\n    step: gold\n    topic: fact\n    item: option\n    options:\n      mode: complete\n    table_options:\n      identity: true\n      cluster_by: [monarch]\n      properties:\n        delta.enableChangeDataFeed: true\n      comment: Strength lies in unity\n</code></pre>"},{"location":"reference/table-options/#extended-example","title":"Extended example","text":"<pre><code>- job:\n    step: gold\n    topic: sales\n    item: curated\n    options:\n      mode: update\n      change_data_capture: scd1\n    table_options:\n      identity: true\n      partition_by: [date_id]\n      zorder_by: [customer_id, product_id]\n      bloomfilter_by: [order_id]\n      constraints:\n        primary_key: \"primary key(__key)\"\n      properties:\n        delta.minReaderVersion: 2\n        delta.minWriterVersion: 5\n        delta.enableChangeDataFeed: true\n      comment: \"Curated sales model with CDC\"\n      retention_days: 30\n      calculated_columns:\n        ingestion_date: \"current_date()\"\n</code></pre>"},{"location":"reference/table-options/#notes-and-guidance","title":"Notes and guidance","text":"<ul> <li>Scope: <code>table_options</code> apply to physical table modes (<code>append</code>, <code>complete</code>, <code>update</code>). They do not apply to <code>memory</code> mode (view-only).</li> <li>CDC compatibility: Properties like <code>delta.enableChangeDataFeed</code> can be useful for change data capture downstream; set under <code>properties</code>.</li> <li>Identity: When <code>identity: true</code> is enabled, the identity column is defined at create time - See Identity.</li> <li>Layout vs performance: Start with <code>partition_by</code> on low-cardinality columns; add <code>zorder_by</code> for frequently filtered high-cardinality columns. Use Bloom filters selectively for point-lookups.</li> <li>Defaults: You can specify step-level defaults in your runtime config and override them per job.</li> </ul>"},{"location":"reference/table-options/#related-topics","title":"Related topics","text":"<ul> <li>Step references:</li> <li>Bronze</li> <li>Silver</li> <li>Gold</li> <li>Data quality checks: Checks &amp; Data Quality</li> <li>Custom logic integration: Extenders, UDFs &amp; Views</li> <li>Runtime configuration: Runtime</li> </ul>"},{"location":"steps/bronze/","title":"Bronze Step Reference","text":"<p>The Bronze step ingests raw data from files/streams/sources into the Lakehouse.</p> <p>What it does - Reads from a URI using a parser (e.g., parquet) or custom parser - Optionally filters, calculates, or encrypts columns - Appends or registers data for downstream steps</p>"},{"location":"steps/bronze/#modes","title":"Modes","text":"Mode Description memory Register a temporary view only; no Delta table is written. append Append records to the target table (no merge/upsert). register Register an existing Delta table/view at <code>uri</code>. <p>Minimal example</p> <pre><code>- job:\n    step: bronze\n    topic: demo\n    item: source\n    options:\n      mode: append\n      uri: /mnt/demo/raw/demo\n      parser: parquet\n      keys: [id]\n</code></pre> <p>More examples - Example config: <code>framework/examples/runtime/bronze/_config.example.yml</code> - Integration scenarios: <code>framework/tests/integration/runtime/bronze/</code></p>"},{"location":"steps/bronze/#bronze-options","title":"Bronze options","text":""},{"location":"steps/bronze/#all-options-at-a-glance","title":"All Options at a glance","text":"Option Purpose type <code>default</code> vs <code>manual</code> (manual disables auto DDL/DML; you manage persistence). mode One of: <code>memory</code>, <code>append</code>, <code>register</code>. uri Source location (e.g., <code>/mnt/...</code>, <code>abfss://...</code>) used by the parser. parser Input parser (e.g., <code>parquet</code>) or the name of a custom parser. source Logical source label for lineage/logging. keys Business keys used for dedup and downstream CDC. parents Upstream jobs to enforce dependencies and ordering. filter_where SQL predicate applied during ingestion. encrypted_columns Columns to encrypt at write time. calculated_columns Derived columns defined as SQL expressions. extender Name of a Python extender to apply (see Extenders). extender_options Arguments for the extender (mapping). operation Changelog semantics for certain feeds: <code>upsert</code> | <code>reload</code> | <code>delete</code>. timeout Per-job timeout seconds (overrides step default)."},{"location":"steps/bronze/#bronze-dedicated-options","title":"Bronze dedicated options","text":"<ul> <li> <p>Core</p> <ul> <li>type: <code>default</code> vs <code>manual</code>. Manual means Fabricks will not auto-generate DDL/DML; you control persistence.</li> <li>mode: Controls ingestion behavior (<code>memory</code>, <code>append</code>, <code>register</code>).</li> <li>timeout: Per-job timeout seconds; overrides step defaults.</li> </ul> </li> <li> <p>Source</p> <ul> <li>uri: Filesystem or table/view location resolved by the parser.</li> <li>parser: Name of the parser to read the source (e.g., <code>parquet</code>) or a custom parser.</li> <li>source: Optional logical label used for lineage/logging.</li> </ul> </li> <li> <p>Dependencies &amp; ordering</p> <ul> <li>parents: Explicit upstream jobs that must complete before this job runs.</li> <li>keys: Natural/business keys used for deduplication and for downstream CDC.</li> </ul> </li> <li> <p>Transformations &amp; security</p> <ul> <li>filter_where: SQL predicate to filter rows during ingestion.</li> <li>calculated_columns: Mapping of new columns to SQL expressions evaluated at load time.</li> <li>encrypted_columns: List of columns to encrypt during write.</li> </ul> </li> <li> <p>Changelog semantics</p> <ul> <li>operation: For change-log style feeds, indicates whether incoming rows should be treated (<code>upsert</code>, <code>reload</code>, <code>delete</code>).</li> </ul> </li> </ul>"},{"location":"steps/bronze/#extensibility","title":"Extensibility","text":"<ul> <li>extender: Apply a Python extender to the ingested DataFrame.</li> <li>extender_options: Mapping of arguments passed to the extender.</li> <li>See also: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"steps/bronze/#related","title":"Related","text":"<ul> <li>Next steps: Silver Step, Table Options</li> <li>Data quality: Checks &amp; Data Quality</li> <li>Extensibility: Extenders, UDFs &amp; Views</li> <li>Sample runtime: Sample runtime</li> </ul>"},{"location":"steps/gold/","title":"Gold Step Reference","text":"<p>Gold steps produce consumption-ready models, usually via SQL. Semantic applies table properties/metadata.</p>"},{"location":"steps/gold/#modes","title":"Modes","text":"Mode Description memory View-only result; no table is written. append Append rows to the target table. complete Full refresh/overwrite of the target table. update Merge/upsert semantics (typically used with CDC). invoke Run a notebook instead of SQL (configure via <code>invoker_options</code>). <p>See Gold CDC input fields below for required <code>__</code> columns when using SCD.</p> <p>Quick CDC overview - Strategies: <code>nocdc</code>, <code>scd1</code>, <code>scd2</code> - SCD1: uses soft-delete flags <code>__is_current</code>, <code>__is_deleted</code> - SCD2: uses validity windows <code>__valid_from</code>, <code>__valid_to</code> - Gold inputs and operations: <code>__operation</code> values are <code>'upsert' | 'delete' | 'reload'</code>. If omitted for SCD in <code>mode: update</code>, Fabricks injects <code>'reload'</code> and enables rectification automatically</p> <p>See the CDC reference for details and examples: Change Data Capture (CDC)</p>"},{"location":"steps/gold/#options-at-a-glance","title":"Options at a glance","text":"Option Purpose type <code>default</code> vs <code>manual</code> (manual: you manage persistence yourself). mode One of: <code>memory</code>, <code>append</code>, <code>complete</code>, <code>update</code>, <code>invoke</code>. change_data_capture CDC strategy: <code>nocdc</code> | <code>scd1</code> | <code>scd2</code>. update_where Additional predicate limiting updates during merges. parents Explicit upstream dependencies for scheduling/recomputation. deduplicate Drop duplicate keys in the result before writing. persist_last_timestamp Persist the last processed timestamp for incremental loads. correct_valid_from Adjust SCD2 timestamps that would otherwise start at a sentinel date. table Target table override (useful for semantic/table-copy scenarios). table_options Delta table options and metadata (identity, clustering, properties, comments). spark_options Per-job Spark session/SQL options mapping. udfs Path to UDFs registry to load before executing the job. check_options Configure DQ checks (pre_run, post_run, max_rows, min_rows, count_must_equal, skip). notebook Mark job to run as a notebook (used with <code>mode: invoke</code>). invoker_options Configure <code>pre_run</code> / <code>run</code> / <code>post_run</code> notebook execution and arguments. requirements If true, install/resolve additional requirements for this job. timeout Per-job timeout seconds (overrides step defaults)."},{"location":"steps/gold/#field-reference","title":"Field reference","text":"<ul> <li> <p>Core</p> <ul> <li>type: <code>default</code> vs <code>manual</code>. Manual means Fabricks will not auto-generate DDL/DML; you control persistence.</li> <li>mode: Processing behavior (<code>memory</code>, <code>append</code>, <code>complete</code>, <code>update</code>, <code>invoke</code>).</li> <li>timeout: Per-job timeout seconds; overrides step defaults.</li> <li>requirements: Resolve/install additional requirements for this job.</li> </ul> </li> <li> <p>CDC</p> <ul> <li>change_data_capture: <code>nocdc</code>, <code>scd1</code>, or <code>scd2</code>. Governs merge semantics when <code>mode: update</code>.</li> </ul> </li> <li> <p>Incremental &amp; merge</p> <ul> <li>update_where: Predicate that constrains rows affected during merge/upsert.</li> <li>deduplicate: Drop duplicate keys prior to write.</li> <li>persist_last_timestamp: Persist last processed timestamp to support incremental loads.</li> <li>correct_valid_from: Adjusts start timestamps for SCD2 validity windows when needed.</li> </ul> </li> <li> <p>Dependencies &amp; targets</p> <ul> <li>parents: Explicit upstream jobs that must complete before this job runs.</li> <li>table: Target table override (commonly used when coordinating with Semantic/table-copy flows).</li> </ul> </li> <li> <p>Table &amp; Spark</p> <ul> <li>table_options: Delta table options and metadata (e.g., identity, clustering, properties, comments).</li> <li>spark_options: Per-job Spark session/SQL options mapping.</li> </ul> </li> <li>UDFs<ul> <li>udfs: Path to UDFs registry to load before executing the job.</li> </ul> </li> <li>Checks<ul> <li>check_options: Configure DQ checks (e.g., <code>pre_run</code>, <code>post_run</code>, <code>max_rows</code>, <code>min_rows</code>, <code>count_must_equal</code>, <code>skip</code>).</li> </ul> </li> <li>Notebook invocation<ul> <li>notebook: When coupled with <code>mode: invoke</code>, mark this job to run a notebook.</li> <li>invoker_options: Configure <code>pre_run</code>, <code>run</code>, and <code>post_run</code> notebooks and pass arguments.</li> </ul> </li> </ul> <p>Notes:</p> <ul> <li>Memory outputs ignore columns starting with <code>__</code> (e.g., <code>__it_should_not_be_found</code>).</li> </ul> <p>Minimal examples</p> <p>Gold SCD1 update</p> <pre><code>- job:\n    step: gold\n    topic: scd1\n    item: update\n    options:\n      change_data_capture: scd1\n      mode: update\n</code></pre> <p>Gold full refresh</p> <pre><code>- job:\n    step: gold\n    topic: demo\n    item: hello_world\n    options:\n      mode: complete\n</code></pre> <p>Semantic table</p> <pre><code>- job:\n    step: semantic\n    topic: fact\n    item: job_option\n    options: { mode: complete }\n    table_options:\n      properties:\n        delta.minReaderVersion: 2\n        delta.minWriterVersion: 5\n</code></pre> <p>More examples - Example config/SQL: <code>framework/examples/runtime/gold/gold/_config.example.yml</code>, <code>framework/examples/runtime/gold/gold/hello_world.sql</code> - Integration scenarios: <code>framework/tests/integration/runtime/gold/gold/</code> and <code>.../semantic/</code></p> <p>Dependencies</p> <pre><code>with cte as (select d.time, d.hour from gold.dim_time d)\nselect\n  udf_key(array(f.id, d.time)) as __key,\n  f.id as id,\n  f.monarch as monarch,\n  s.__source as role,\n  f.value as value,\n  d.time as time\nfrom cte d\ncross join transf.fact_memory f\nleft join silver.king_and_queen_scd1__current s on f.id = s.id\nwhere d.hour = 10\n</code></pre> <p>Invoke (notebooks)</p> <pre><code>- job:\n    step: gold\n    topic: invoke\n    item: post_run\n    options: { mode: memory }\n    invoker_options:\n      post_run:\n          - notebook: gold/gold/invoke/post_run/exe\n          arguments: { arg1: 1, arg2: 2, arg3: 3 }\n- job:\n    step: gold\n    topic: invoke\n    item: notebook\n    options: { mode: invoke }\n    invoker_options:\n      run:\n        notebook: gold/gold/invoke/notebook\n        arguments: { arg1: 1, arg2: 2, arg3: 3 }\n</code></pre> <p>Checks and DQ</p> <pre><code>- job:\n    step: gold\n    topic: check\n    item: max_rows\n    options: { mode: complete }\n    check_options: { max_rows: 2 }\n</code></pre> <p>Check SQL contracts</p> <pre><code>-- fail.pre_run.sql\nselect \"fail\" as __action, \"Please don't fail on me :(\" as __message\n\n-- warning.post_run.sql\nselect \"I want you to warn me !\" as __message, \"warning\" as __action\n</code></pre> <p>Additional examples</p> <pre><code># Fact with table options and Spark options\n- job:\n    step: gold\n    topic: fact\n    item: option\n    options:\n      mode: complete\n    table_options:\n      identity: true\n      liquid_clustering: true\n      cluster_by: [monarch]\n      properties:\n        country: Belgium\n      comment: Strength lies in unity\n    spark_options:\n      sql:\n        spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite: false\n        spark.databricks.delta.properties.defaults.enableChangeDataFeed: true\n</code></pre> <pre><code>-- order_duplicate.sql\nselect 1 as __key, 1 as dummy, 1 as __order_duplicate_by_desc\nunion all\nselect 1 as __key, 2 as dummy, 2 as __order_duplicate_by_desc\n</code></pre>"},{"location":"steps/gold/#gold-options","title":"Gold options","text":"<p>See Options at a glance and Field reference above.</p>"},{"location":"steps/gold/#cdc-input-fields-for-gold-jobs","title":"CDC input fields for gold jobs","text":"<ul> <li>scd2 (required): <code>__key</code>, <code>__timestamp</code>, <code>__operation</code> ('upsert'|'delete'|'reload').</li> <li>scd1 (required): <code>__key</code>; optional <code>__timestamp</code>/<code>__operation</code> ('upsert'|'delete'|'reload') for delete/rectify handling. If <code>__operation</code> is omitted for SCD in <code>mode: update</code>, Fabricks injects <code>'reload'</code> and enables rectification.</li> <li>Optional helpers: <code>__order_duplicate_by_asc</code> / <code>__order_duplicate_by_desc</code>, <code>__identity</code>.</li> <li>See: Change Data Capture (CDC) for full details and examples.</li> </ul>"},{"location":"steps/gold/#gold-jobs","title":"Gold jobs","text":"<ul> <li>SCD1/SCD2 with modes and options:</li> </ul> <pre><code>- job:\n    step: gold\n    topic: scd1\n    item: update\n    options:\n      change_data_capture: scd1\n      mode: update\n- job:\n    step: gold\n    topic: scd1\n    item: last_timestamp\n    options:\n      change_data_capture: scd1\n      mode: update\n      persist_last_timestamp: true\n- job:\n    step: gold\n    topic: scd2\n    item: correct_valid_from\n    options:\n      change_data_capture: scd2\n      mode: memory\n      correct_valid_from: false\n</code></pre> <p>Example SCD2 SQL using <code>__key</code>, <code>__timestamp</code>, <code>__operation</code>:</p> <pre><code>with dates as (\n  select id as id, __valid_from as __timestamp, 'upsert' as __operation from silver.monarch_scd2 where __valid_from &gt; '1900-01-02'\n  union\n  select id as id, __valid_to as __timestamp, 'delete' as __operation from silver.monarch_scd2 where __is_deleted\n)\nselect\n  d.id as __key,\n  s.id as id,\n  s.name as monarch,\n  s.doubleField as value,\n  d.__operation,\n  if(d.__operation = 'delete', d.__timestamp + interval 1 second, d.__timestamp) as __timestamp\nfrom dates d\nleft join silver.monarch_scd2 s on d.id = s.id and d.__timestamp between s.__valid_from and s.__valid_to\n</code></pre>"},{"location":"steps/gold/#when-to-model-scd2-in-gold-different-grain","title":"When to model SCD2 in Gold (different grain)","text":"<ul> <li>Use SCD2 in Gold when the base SCD2 tables do not align with the required consumption grain, and you need to derive a slowly changing history at a coarser/different grain (e.g., roll up line-item SCD2 to order-level SCD2, or derive a customer segment SCD2 from underlying attribute histories).</li> </ul> <p>Example: roll up line-item SCD2 (silver.order_item_scd2) to order-level SCD2 (total_amount by order_id)</p> <pre><code>- job:\n    step: gold\n    topic: order\n    item: total_amount_scd2\n    options:\n      mode: update\n      change_data_capture: scd2\n</code></pre> <pre><code>-- Derive Gold SCD2 change points (upserts/deletes) at the order_id grain\nwith change_points as (\n  select order_id as __key, __valid_from as __timestamp, 'upsert' as __operation\n  from silver.order_item_scd2\n  union\n  select order_id as __key, __valid_to as __timestamp, 'delete' as __operation\n  from silver.order_item_scd2\n  where __is_deleted\n  union\n  -- article-level price change boundaries that fall within the order_item validity window\n  select oi.order_id as __key, a.__valid_from as __timestamp, 'upsert' as __operation\n  from silver.order_item_scd2 oi\n  join silver.article_scd2 a\n    on oi.article_id = a.article_id\n    and a.__valid_from between oi.__valid_from and oi.__valid_to\n  union\n  select oi.order_id as __key, a.__valid_to as __timestamp, 'upsert' as __operation\n  from silver.order_item_scd2 oi\n  join silver.article_scd2 a\n    on oi.article_id = a.article_id\n    and a.__valid_to between oi.__valid_from and oi.__valid_to\n)\n-- Compute the order-level attributes as of each change point\nselect\n  cp.__key,\n  oh.customer_id as customer_id,\n  seg.segment as customer_segment,\n  sum(oi.amount) as total_amount,\n  cp.__operation,\n  if(cp.__operation = 'delete', cp.__timestamp + interval 1 second, cp.__timestamp) as __timestamp\nfrom change_points cp\nleft join silver.order_item_scd2 oi\n  on cp.__key = oi.order_id\n  and cp.__timestamp between oi.__valid_from and oi.__valid_to\nleft join silver.order_header oh\n  on cp.__key = oh.order_id\nleft join silver.customer_segment_scd2 seg\n  on oh.customer_id = seg.customer_id\n  and cp.__timestamp between seg.__valid_from and seg.__valid_to\n</code></pre> <p>Notes</p> <ul> <li>The change_points CTE promotes underlying SCD2 intervals to the desired Gold grain by emitting 'upsert' at each interval start and 'delete' at interval end (or for deleted keys).</li> <li>At each change point, compute the Gold attributes as-of that timestamp. Fabricks will render the SCD2 merge using <code>__key</code>, <code>__timestamp</code>, and <code>__operation</code>.</li> <li>Article/price changes: extra unions in change_points add <code>silver.article_scd2</code> validity boundaries (within each item window) so totals recompute when article prices change.</li> <li>Joining extra tables:</li> <li>Static/non-SCD2 tables (e.g., <code>silver.order_header</code>) can be joined directly on business keys (e.g., <code>order_id</code>).</li> <li>SCD2 dimensions (e.g., <code>silver.customer_segment_scd2</code>) must be joined as-of the change point using a validity-window predicate: <code>cp.__timestamp between seg.__valid_from and seg.__valid_to</code>.</li> <li>If your derived attribute can also disappear (e.g., no remaining items), the 'delete' operation correctly closes the last interval for that Gold key.</li> </ul> <ul> <li>Fact options: table options, clustering, properties, and Spark options:</li> </ul> <pre><code>- job:\n    step: gold\n    topic: fact\n    item: option\n    options:\n      mode: complete\n    table_options:\n      identity: true\n      liquid_clustering: true\n      cluster_by: [monarch]\n      properties:\n        country: Belgium\n      comment: Strength lies in unity\n    spark_options:\n      sql:\n        spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite: false\n        spark.databricks.delta.properties.defaults.enableChangeDataFeed: true\n</code></pre> <p>Example SQL for this job:</p> <pre><code>select id as id, name as monarch, doubleField as value from silver.monarch_scd1__current\n</code></pre> <ul> <li>Manual, memory, append, overwrite:</li> </ul> <pre><code>- job: { step: gold, topic: fact, item: manual, options: { type: manual, mode: complete } }\n- job: { step: gold, topic: fact, item: memory, options: { mode: memory } }\n- job: { step: gold, topic: fact, item: append, options: { mode: append } }\n- job: { step: gold, topic: dim, item: overwrite, options: { change_data_capture: scd1, mode: update }, table_options: { identity: true } }\n</code></pre> <p>Memory outputs ignore columns starting with <code>__</code> (e.g., <code>__it_should_not_be_found</code>).</p> <ul> <li>Deduplicate and order-by-duplicate examples:</li> </ul> <pre><code>- job:\n    step: gold\n    topic: fact\n    item: deduplicate\n    options:\n      mode: complete\n      deduplicate: true\n</code></pre> <pre><code>-- deduplicate.sql\nselect 1 as __key, 2 as dummy\nunion all\nselect 1 as __key, 1 as dummy\n</code></pre> <pre><code>-- order_duplicate.sql\nselect 1 as __key, 1 as dummy, 1 as __order_duplicate_by_desc\nunion all\nselect 1 as __key, 2 as dummy, 2 as __order_duplicate_by_desc\n</code></pre>"},{"location":"steps/gold/#related","title":"Related","text":"<ul> <li>Next steps: Table Options</li> <li>Data quality: Checks &amp; Data Quality</li> <li>Extensibility: Extenders, UDFs &amp; Views</li> <li>Sample runtime: Sample runtime</li> </ul>"},{"location":"steps/silver/","title":"Silver Step Reference","text":"<p>The Silver step standardizes and enriches data, and applies CDC (SCD1/SCD2) if configured.</p>"},{"location":"steps/silver/#modes","title":"Modes","text":"Mode Description memory View-only; no table written. append Append-only table. latest Keep only the latest row per key within the batch. update Merge/upsert semantics; typically used with CDC. combine Combine/union outputs from parent jobs into one result."},{"location":"steps/silver/#cdc-strategies","title":"CDC strategies","text":"Strategy What it does Convenience views nocdc No CDC flags; writes the result as-is. \u2014 scd1 Tracks current vs deleted; adds flags <code>__is_current</code>, <code>__is_deleted</code>. <code>TABLE__current</code> scd2 Maintains validity windows with <code>__valid_from</code>, <code>__valid_to</code> and historical rows. <code>TABLE__current</code>"},{"location":"steps/silver/#options-at-a-glance","title":"Options at a glance","text":"Option Purpose type <code>default</code> vs <code>manual</code> (manual: you manage persistence yourself). mode One of the modes listed above. change_data_capture CDC strategy: <code>nocdc</code> | <code>scd1</code> | <code>scd2</code>. parents Upstream job identifiers (e.g., <code>bronze.demo_source</code>). filter_where SQL predicate applied at transform time. deduplicate Drop duplicate keys within the batch. stream Enable streaming semantics where supported. order_duplicate_by Choose preferred row when duplicates exist (e.g., <code>{ order_by: desc }</code>). timeout Per-job timeout seconds (overrides step default). extender Name of a Python extender to apply (see Extenders). extender_options Arguments for the extender (mapping). check_options Configure DQ checks (pre_run, post_run, max_rows, min_rows, count_must_equal, skip)."},{"location":"steps/silver/#field-reference","title":"Field reference","text":"<ul> <li> <p>Core</p> <ul> <li>type: <code>default</code> vs <code>manual</code>. Manual means Fabricks won\u2019t auto-generate DDL/DML; you control persistence.</li> <li>mode: Processing behavior (<code>memory</code>, <code>append</code>, <code>latest</code>, <code>update</code>, <code>combine</code>).</li> <li>timeout: Per-job timeout; overrides step defaults.</li> </ul> </li> <li> <p>CDC</p> <ul> <li>change_data_capture: <code>nocdc</code>, <code>scd1</code>, or <code>scd2</code>.</li> <li>scd1 flags: <code>__is_current</code>, <code>__is_deleted</code> and a <code>{table}__current</code> convenience view.</li> <li>scd2 windows: <code>__valid_from</code>, <code>__valid_to</code> and a <code>{table}__current</code> convenience view.</li> </ul> </li> <li> <p>Dependencies &amp; ordering</p> <ul> <li>parents: Explicit upstream jobs (e.g., <code>bronze.topic_item</code>).</li> <li>deduplicate: Drop duplicate keys within the batch.</li> <li>order_duplicate_by: Pick a preferred row when duplicates exist (e.g., <code>{ order_by: desc }</code>).</li> </ul> </li> <li> <p>Streaming</p> <ul> <li>stream: Where supported, use streaming semantics for incremental processing.</li> </ul> </li> <li> <p>Extensibility</p> <ul> <li>extender: Apply a Python extender to the DataFrame.</li> <li>extender_options: Mapping of arguments passed to the extender.</li> </ul> </li> <li> <p>See also: Extenders, UDFs &amp; Views</p> </li> <li> <p>Checks</p> </li> <li>check_options: Configure DQ checks (e.g., <code>pre_run</code>, <code>post_run</code>, <code>max_rows</code>, <code>min_rows</code>, <code>count_must_equal</code>, <code>skip</code>). See Checks &amp; Data Quality</li> </ul> <p>Minimal example</p> <pre><code>- job:\n    step: silver\n    topic: demo\n    item: scd1\n    options:\n      mode: update\n      change_data_capture: scd1\n      parents: [bronze.demo_source]\n</code></pre> <p>More examples - Example config: <code>framework/examples/runtime/silver/_config.example.yml</code> - Integration scenarios: <code>framework/tests/integration/runtime/silver/</code></p>"},{"location":"steps/silver/#silver-options","title":"Silver options","text":"<p>See Options at a glance and Field reference above.</p>"},{"location":"steps/silver/#silver-jobs","title":"Silver jobs","text":"<ul> <li>Combine outputs example:   ```yaml</li> <li> <p>job:       step: silver       topic: princess       item: combine       options:         mode: combine         parents:           - silver.princess_extend           - silver.princess_latest   ```</p> </li> <li> <p>Monarchs with SCD1/SCD2 and delta/memory flavors:</p> </li> </ul> <pre><code>- job:\n    step: silver\n    topic: monarch\n    item: scd1\n    options:\n      mode: update\n      change_data_capture: scd1\n- job:\n    step: silver\n    topic: monarch\n    item: scd2\n    options:\n      mode: update\n      change_data_capture: scd2\n- job:\n    step: silver\n    topic: monarch\n    item: delta\n    options:\n      mode: update\n      change_data_capture: scd2\n- job:\n    step: silver\n    topic: monarch\n    item: memory\n    options:\n      mode: memory\n      change_data_capture: nocdc\n</code></pre> <ul> <li>Multi-parent example (kings and queens):</li> </ul> <pre><code>- job:\n    step: silver\n    topic: king_and_queen\n    item: scd1\n    options:\n      mode: update\n      change_data_capture: scd1\n      parents: [bronze.queen_scd1, bronze.king_scd1]\n</code></pre> <ul> <li>Princess patterns: latest, append, calculated columns, duplicate ordering, manual type, schema drift, extend with extenders:</li> </ul> <pre><code>- job:\n    step: silver\n    topic: princess\n    item: latest\n    options:\n      mode: latest\n- job:\n    step: silver\n    topic: princess\n    item: append\n    options:\n      mode: append\n- job:\n    step: silver\n    topic: princess\n    item: order_duplicate\n    options:\n      mode: update\n      change_data_capture: scd1\n      order_duplicate_by:\n        order_by: desc\n- job:\n    step: silver\n    topic: princess\n    item: calculated_column\n    options:\n      mode: update\n      change_data_capture: scd1\n      order_duplicate_by:\n        order_by: asc\n- job:\n    step: silver\n    topic: princess\n    item: manual\n    options:\n      type: manual\n      mode: update\n      change_data_capture: scd1\n- job:\n    step: silver\n    topic: princess\n    item: extend\n    options:\n      mode: update\n      change_data_capture: scd1\n    extender_options:\n        - extender: add_country\n        arguments:\n          country: Belgium\n</code></pre>"},{"location":"steps/silver/#related","title":"Related","text":"<ul> <li>Next steps: Gold Step, Table Options</li> <li>Data quality: Checks &amp; Data Quality</li> <li>Extensibility: Extenders, UDFs &amp; Views</li> <li>Sample runtime: Sample runtime</li> </ul> <p>Special characters in column names are preserved (see <code>prince.special_char</code>).</p>"}]}