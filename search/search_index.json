{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fabricks User Guide","text":"<p>Fabricks is a pragmatic framework to build Databricks Lakehouse pipelines using YAML for orchestration and SQL for transformations.  It standardizes jobs, steps, schedules, CDC, and checks while keeping development SQL\u2011first.</p>"},{"location":"#steps-overview","title":"Steps Overview","text":"<p>Fabricks organizes your Lakehouse into clear layers. Each step has a dedicated reference with modes, options, and examples.</p>"},{"location":"#bronze","title":"Bronze","text":"<p>Raw ingestion from source systems (files, streams, existing tables). Keep logic light; land data for downstream processing.</p> <ul> <li>Typical modes: memory, append, register</li> <li>Focus: lightweight parsing/landing; no business logic</li> <li>Output: raw tables or temporary views</li> </ul> <p>Read the full reference \u2192 Bronze Step</p>"},{"location":"#silver","title":"Silver","text":"<p>Standardize, clean, and enrich data; optionally apply CDC (SCD1/SCD2). Produces conformed datasets and convenience views.</p> <ul> <li>Typical modes: memory, append, latest, update, combine</li> <li>CDC: nocdc, scd1, scd2 with built-in helpers and views</li> <li>Output: conformed tables and curated views</li> </ul> <p>Read the full reference \u2192 Silver Step</p>"},{"location":"#gold","title":"Gold","text":"<p>Curated business models for analytics and reporting; dimensional or mart\u2011style outputs. Can also invoke notebooks when needed.</p> <ul> <li>Typical modes: memory, append, complete, update, invoke (notebooks)</li> <li>Focus: dimensional models, marts, KPI-ready data</li> <li>Output: business-consumption tables and views</li> </ul> <p>Read the full reference \u2192 Gold Step</p>"},{"location":"#where-to-configure","title":"Where to Configure","text":"<ul> <li>Project configuration, schedules, and structure \u2192 Runtime</li> <li>Data quality and rollback behavior \u2192 Checks &amp; Data Quality</li> <li>Table properties, clustering, and layout \u2192 Table Options</li> <li>Custom logic and reusable SQL assets \u2192 Extenders, UDFs &amp; Parsers</li> <li>Change Data Capture (CDC) \u2192 CDC</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<ul> <li>Start with the Quickstart: Getting Started</li> <li>Copy the minimal sample and adapt it: Sample runtime</li> <li>Point <code>tool.fabricks.runtime</code> and <code>config</code> to your runtime and start adding jobs in SQL and YAML.</li> </ul>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2024 fabricks-framework</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"release-notes/","title":"Release Notes","text":"<p>For the latest releases and detailed changelogs, please visit the Fabricks Releases page on GitHub.</p>"},{"location":"helpers/getting-started/","title":"Getting Started","text":"<p>This quickstart guides you through running a minimal Fabricks pipeline end-to-end in minutes. It focuses on essentials: project setup, runtime layout, a schedule, and executing a simple job.</p> <p>What you\u2019ll do - Configure Fabricks in your project (pyproject.toml) - Create a minimal runtime with step folders and config - Run a simple Gold job and inspect outputs - Learn where to go next</p> <p>Prerequisites - Python 3.9+ and a working Databricks/Spark environment - Access to a storage location (e.g., abfss:// or dbfs:/) if you plan to persist tables - Recommended: mkdocs-material to preview docs (optional)</p> <p>Install Use your project\u2019s environment. If you vendor Fabricks as a library: <pre><code>pip install fabricks\n</code></pre></p> <p>If Fabricks is part of your mono-repo, ensure your environment resolves it (editable install or workspace).</p> <p>1) Configure your project Add Fabricks configuration to pyproject.toml: <pre><code>[tool.fabricks]\nruntime = \"examples/runtime\"                          # Path to your runtime (jobs, SQL, configs)\nnotebooks = \"fabricks/api/notebooks\"                  # Notebook helpers shipped with Fabricks (optional)\njob_config_from_yaml = true                           # Enable YAML job config\nloglevel = \"info\"\ndebugmode = false\nconfig = \"examples/runtime/fabricks/conf.fabricks.yml\"  # Main runtime config\n</code></pre> Key fields: - runtime: points to your project\u2019s Fabricks runtime (see layout below) - config: path to the main runtime YAML (project options, step paths, defaults) - job_config_from_yaml: when true, loads job definitions from step folders - notebooks: optional helper notebooks path (used by invoke/notebook modes)</p> <p>2) Create a minimal runtime You can copy the structure below. This mirrors the integration-test sample, with a working Gold job.</p> <p>Directory layout: <pre><code>examples/runtime/\n  fabricks/\n    conf.fabricks.yml\n    schedules/\n      schedule.yml\n  bronze/\n    _config.example.yml\n  silver/\n    _config.example.yml\n  gold/\n    gold/\n      _config.example.yml\n      hello_world.sql\n  semantic/\n    _config.example.yml\n</code></pre></p> <p>Minimal contents:</p> <p>examples/runtime/fabricks/conf.fabricks.yml <pre><code>name: MyFabricksProject\noptions:\n  secret_scope: my_secret_scope\n  timeouts:\n    step: 3600\n    job: 3600\n    pre_run: 3600\n    post_run: 3600\n  workers: 2\npath_options:\n  storage: /mnt/data  # adjust to your env (dbfs:/mnt/... or abfss://...)\nspark_options:\n  sql:\n    spark.sql.parquet.compression.codec: zstd\n\nbronze:\n  - name: bronze\n    path_options:\n      runtime: bronze\n      storage: /mnt/data/bronze\n\nsilver:\n  - name: silver\n    path_options:\n      runtime: silver\n      storage: /mnt/data/silver\n\ngold:\n  - name: gold\n    path_options:\n      runtime: gold\n      storage: /mnt/data/gold\n</code></pre></p> <p>examples/runtime/fabricks/schedules/schedule.yml <pre><code>- schedule:\n    name: example\n    options:\n      tag: example\n      steps: [gold]\n      variables: {}\n</code></pre></p> <p>examples/runtime/gold/gold/_config.example.yml <pre><code>- job:\n    step: gold\n    topic: demo\n    item: hello_world\n    options:\n      mode: complete\n</code></pre></p> <p>examples/runtime/gold/gold/hello_world.sql <pre><code>select 1 as id, 'hello' as monarch, 1.0 as value\n</code></pre></p> <p>3) Run the sample Fabricks can run via Databricks bundle jobs or notebooks depending on your deployment. Typical paths:</p> <p>A) Databricks job (bundle) - Configure your bundle to pass the schedule name: example - Trigger the job from your CI/CD or Databricks UI</p> <p>B) Notebook helpers - Use the shipped notebooks in notebooks = \"fabricks/api/notebooks\" to invoke a run with schedule: example</p> <p>On successful execution, expect: - A materialized Gold table (or view) for demo.hello_world (location and format depend on your environment) - Logs indicating step/job completion - If using complete mode, table is fully refreshed</p> <p>4) Verify outputs Run simple SQL in your environment: <pre><code>select * from gold.demo_hello_world\n</code></pre> Or if running in memory mode: <pre><code>select * from gold.demo_hello_world__view\n</code></pre> (Note: memory mode produces views only and ignores columns starting with __)</p> <p>Next steps - Build your first pipeline (Bronze \u2192 Silver \u2192 Gold): Tutorials \u2192 Your First Pipeline - Understand runtime structure and schedules: Runtime - Learn about CDC and SCD1/SCD2 patterns: Change Data Capture (CDC) - Add checks and contracts for data quality: Checks &amp; Data Quality - Apply table properties and clustering: Table Options - Extend with UDFs, Parsers, and Python Extenders: Extenders, UDFs &amp; Parsers</p> <p>Related - Runtime: ../helpers/runtime.md - Steps: Bronze \u2022 Silver \u2022 Gold \u2022 Semantic - CDC: ../reference/cdc.md - DQ Checks: ../reference/checks-data-quality.md - Table Options: ../reference/table-options.md - Extenders/UDFs/Parsers: ../reference/extenders-udfs-parsers.md - Views: ../reference/views.md</p>"},{"location":"helpers/init/","title":"Initialize Fabricks","text":"<p>This helper explains how to initialize Fabricks on a Databricks cluster or local environment by:</p> <ul> <li>Installing the Fabricks library</li> <li>Pointing Fabricks to your runtime</li> <li>Running the armageddon script to bootstrap metadata and objects</li> </ul>"},{"location":"helpers/init/#1-install-fabricks","title":"1) Install Fabricks","text":"<p>You need the Fabricks package available on your cluster or local environment.</p> <ul> <li>Databricks cluster (recommended)</li> <li>Libraries \u2192 Install new \u2192 Python PyPI \u2192 fabricks (or install a wheel/artifact you build)</li> <li> <p>Alternatively, attach a workspace library artifact built from this repository</p> </li> <li> <p>Local development (optional)</p> </li> <li>pip install fabricks</li> <li>or from source (for development): pip install -e .[dev,test]</li> </ul> <p>Python &gt;=3.9,&lt;4 is recommended; align with your Databricks LTS runtime.</p>"},{"location":"helpers/init/#2-point-fabricks-to-your-runtime","title":"2) Point Fabricks to your runtime","text":"<p>Fabricks discovers its runtime via either environment variables or [tool.fabricks] in your pyproject.toml. The core lookup logic is implemented in fabricks/context/runtime.py.</p> <p>Option A: Configure via pyproject.toml (preferred for repo-managed projects): <pre><code>[tool.fabricks]\nruntime = \"path/to/your/runtime\"                   # e.g., tests/integration/runtime or examples/runtime\nnotebooks = \"fabricks/api/notebooks\"               # optional: helpers shipped with Fabricks\njob_config_from_yaml = true                        # optional\nloglevel = \"info\"                                  # optional: DEBUG|INFO|WARNING|ERROR|CRITICAL\ndebugmode = false                                  # optional\nconfig = \"path/to/your/runtime/fabricks/conf.fabricks.yml\"  # main runtime YAML\n</code></pre></p> <p>Option B: Configure via environment variables (useful on clusters):</p> <pre><code># FABRICKS_RUNTIME: path to your runtime (jobs, SQL, configs)\n# FABRICKS_CONFIG: full path to your main conf.fabricks.yml (if not set, Fabricks tries to infer a conf.uc.&lt;orgId&gt;.yml)\n# FABRICKS_NOTEBOOKS: optional helper notebook path\n# FABRICKS_IS_JOB_CONFIG_FROM_YAML, FABRICKS_LOGLEVEL, FABRICKS_IS_DEBUGMODE: optional toggles\n</code></pre> <p>Example on Databricks (Cluster \u2192 Configuration \u2192 Advanced options \u2192 Environment variables): <pre><code>FABRICKS_RUNTIME=/Workspace/Repos/your/repo/examples/runtime\nFABRICKS_CONFIG=/Workspace/Repos/your/repo/examples/runtime/fabricks/conf.fabricks.yml\nFABRICKS_LOGLEVEL=INFO\n</code></pre></p> <p>You can also set env vars in a notebook before importing Fabricks: <pre><code>import os\nos.environ[\"FABRICKS_RUNTIME\"] = \"/Workspace/Repos/your/repo/examples/runtime\"\nos.environ[\"FABRICKS_CONFIG\"] = \"/Workspace/Repos/your/repo/examples/runtime/fabricks/conf.fabricks.yml\"\n# Optional:\n# os.environ[\"FABRICKS_LOGLEVEL\"] = \"INFO\"\n</code></pre></p>"},{"location":"helpers/init/#3-run-armageddon","title":"3) Run armageddon","text":"<p>armageddon performs a one-shot setup aligned with your runtime configuration (e.g., preparing databases/metadata, registering views).</p> <p>Import and call: <pre><code># Databricks or local\nfrom fabricks.core.scripts.armageddon import armageddon\n\n# You may pass one or more steps (bronze, silver, gold, semantic, transf, ...)\n# Examples:\narmageddon(steps=\"gold\")                      # single step\narmageddon(steps=[\"bronze\", \"silver\", \"gold\"])  # multiple steps\narmageddon(steps=None)                        # default behavior, follow runtime config\n</code></pre></p>"},{"location":"helpers/init/#example-databricks-notebook-initialize","title":"Example: Databricks Notebook: Initialize","text":"<p>Create a new notebook (Python) named initialize and include:</p> <pre><code># (Optional) set env vars if not using pyproject.toml\n# import os\n# os.environ[\"FABRICKS_RUNTIME\"] = \"/Workspace/Repos/your/repo/examples/runtime\"\n# os.environ[\"FABRICKS_CONFIG\"] = \"/Workspace/Repos/your/repo/examples/runtime/fabricks/conf.fabricks.yml\"\n# os.environ[\"FABRICKS_LOGLEVEL\"] = \"INFO\"\n\nfrom fabricks.core.scripts.armageddon import armageddon\n# Run for all default steps from your runtime config:\narmageddon()\n</code></pre> <p>Attach the Fabricks library to the cluster before running the notebook.</p>"},{"location":"helpers/init/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing env/config:</li> <li>ValueError: Must have at least a pyproject.toml or set FABRICKS_RUNTIME</li> <li> <p>Fix by setting FABRICKS_RUNTIME or adding [tool.fabricks] to pyproject.toml</p> </li> <li> <p>Unity Catalog:</p> </li> <li> <p>Ensure options.unity_catalog is true and options.catalog is set in conf.fabricks.yml</p> </li> <li> <p>Paths and storage:</p> </li> <li> <p>conf.fabricks.yml must define path_options.storage and per-step runtime/storage paths; Fabricks uses these to resolve PATHS_RUNTIME and PATHS_STORAGE</p> </li> <li> <p>Logging:</p> </li> <li>Set FABRICKS_LOGLEVEL or tool.fabricks.loglevel to control verbosity</li> </ul>"},{"location":"helpers/init/#related-topics","title":"Related topics","text":"<ul> <li>Runtime configuration: ../helpers/runtime.md</li> <li>Step Helper: ./step.md</li> <li>Job Helper: ./job.md</li> </ul>"},{"location":"helpers/job/","title":"Job Helper","text":"<p>This helper describes a widget-driven approach to manage a Fabricks job.</p> <p>Core imports used: <pre><code>from fabricks.api import get_job\n</code></pre></p>"},{"location":"helpers/job/#typical-usage","title":"Typical usage","text":"<p>1) Resolve the job: <pre><code>from fabricks.api import get_job\n\nj = get_job(job=\"gold.sales.orders\")  # or pass step/topic/item explicitly\n</code></pre></p> <p>2) Run actions: <pre><code># Examples:\nj.run()\n\n# Other common operations:\nj.update_schema()\nj.overwrite_schema()\nj.register()\n\nj.create()\n\n# Cleanup if required:\n# j.truncate()\n# j.drop()\n</code></pre></p> <p>3) Gold jobs: <pre><code># If your job is gold, ensure UDFs are registered before actions:\nif getattr(j, \"expand\", None) == \"gold\":\n    j.register_udfs()\nj.run()\n</code></pre></p>"},{"location":"helpers/job/#helper-notebook","title":"Helper Notebook","text":"<pre><code># Databricks notebook source\nimport os\nfrom multiprocessing import Pool\nfrom typing import Callable, List\n\nfrom databricks.sdk.runtime import dbutils, spark\n\n# COMMAND ----------\n\ndbutils.widgets.text(\"jobs\", \"---\", label=\"6 - Job(s)\")\ndbutils.widgets.multiselect(\n    \"actions\",\n    \"run\",\n    [\n        \"drop\",                   -- Drop the job\n        \"register\",               -- Register the table\n        \"create\",                 -- Create the job\n        \"truncate\",               -- Truncate the table\n        \"run\",                    -- Run the job \n        \"for-each-run\",           -- Run the job (excluding the invoker(s) and the check(s))\n        \"overwrite\",              -- Overwrite the schema and run the job\n        \"pre-run-invoke\",         -- Run the pre-run invoker(s)\n        \"pre-run-check\",          -- Run the pre-run check(s)\n        \"post-run-invoke\",        -- Run the post-run invoker(s)\n        \"post-run-check\",         -- Run the post-run check(s)\n        \"update-schema\",          -- Update the schema\n        \"overwrite-schema\",       -- Overwrite the schema\n    ],\n    label=\"5 - Action(s)\"\n)\ndbutils.widgets.dropdown(\"loglevel\", \"info\", [\"debug\", \"info\", \"error\", \"critical\"], label=\"3 - Log Level\")\ndbutils.widgets.dropdown(\"chdir\", \"False\", [\"True\", \"False\"], label=\"1 - Change Directory\")\ndbutils.widgets.dropdown(\"config_from_yaml\", \"True\", [\"True\", \"False\"], label=\"2 - Use YAML\")\ndbutils.widgets.dropdown(\"debugmode\", \"False\", [\"True\", \"False\"], label=\"4 - Debug Mode\")\n\n# COMMAND ----------\n\nif dbutils.widgets.get(\"chdir\").lower() == \"true\":\n    user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()  # type: ignore\n    try:\n        os.chdir(f\"/Workspace/Users/{user}/Fabricks.Runtime\")\n    except FileNotFoundError:\n        os.chdir(f\"/Workspace/Users/{user}/runtime\")\n\n# COMMAND ----------\n\nif dbutils.widgets.get(\"config_from_yaml\").lower() == \"true\":\n    os.environ[\"FABRICKS_IS_JOB_CONFIG_FROM_YAML\"] = \"1\"\nelse:\n    os.environ[\"FABRICKS_IS_JOB_CONFIG_FROM_YAML\"] = \"0\"\n\n# COMMAND ----------\n\nif dbutils.widgets.get(\"debugmode\").lower() == \"true\":\n    os.environ[\"FABRICKS_IS_DEBUGMODE\"] = \"1\"\nelse:\n    os.environ[\"FABRICKS_IS_DEBUGMODE\"] = \"0\"\n\n# COMMAND ----------\n\nloglevel = dbutils.widgets.get(\"loglevel\")\nif loglevel == \"debug\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"DEBUG\"\nelif loglevel == \"info\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"INFO\"\nelif loglevel == \"error\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"ERROR\"\nelif loglevel == \"critical\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"CRITICAL\"\n\n# COMMAND ----------\n\nfrom fabricks.api import get_job  # noqa: E402\nfrom fabricks.api.log import DEFAULT_LOGGER  # noqa: E402\nfrom fabricks.core.jobs import Gold  # noqa: E402\n\n# COMMAND ----------\n\nactions = dbutils.widgets.get(\"actions\").split(\",\")\nactions = [a.strip() for a in actions]\n\n# COMMAND ----------\n\n\ndef do(job: str) -&gt; None:\n    j = get_job(job=job)\n    todos: dict[str, Callable] = {}\n\n    if j.expand == \"gold\":\n        assert isinstance(j, Gold)\n        j.register_udfs()\n\n    if \"drop\" in actions:\n        todos[\"drop\"] = j.drop\n\n    if \"register\" in actions:\n        todos[\"register\"] = j.register\n\n    if \"pre-run-invoke\" in actions:\n        todos[\"pre-run-invoke\"] = j.invoke_pre_run\n\n    if \"pre-run-check\" in actions:\n        todos[\"pre-run-check\"] = j.check_pre_run\n\n    if \"create\" in actions:\n        todos[\"create\"] = j.create\n\n    if \"update-schema\" in actions:\n        todos[\"update-schema\"] = j.update_schema\n\n    if \"overwrite-schema\" in actions:\n        todos[\"overwrite-schema\"] = j.overwrite_schema\n\n    if \"truncate\" in actions:\n        todos[\"truncate\"] = j.truncate\n\n    if \"run\" in actions:\n        todos[\"run\"] = j.run\n\n    if \"for-each-run\" in actions:\n        todos[\"for-each-run\"] = j.for_each_run\n\n    if \"overwrite\" in actions:\n        todos[\"overwrite\"] = j.overwrite\n\n    if \"post-run-check\" in actions:\n        todos[\"post-run-check\"] = j.check_post_run\n\n    if \"post-run-invoke\" in actions:\n        todos[\"post-run-invoke\"] = j.invoke_post_run\n\n    for key, func in todos.items():\n        func()\n\n# COMMAND ----------\n\nactions = [s.strip() for s in dbutils.widgets.get(\"actions\").split(\",\")]\njobs = [s.strip() for s in dbutils.widgets.get(\"jobs\").split(\",\")]\njobs = [[j.strip() for j in job.split(\"//\")] if \"//\" in job else job for job in jobs]\n\nfor job in jobs:\n    DEFAULT_LOGGER.warning(\", \".join(actions), extra={\"job\": job})\n\n# COMMAND ----------\n\nfor job in jobs:\n    if isinstance(job, List):\n        with Pool() as pool:\n            results = pool.map(do, job)\n    else:\n        do(job)\n\n# COMMAND ----------\n\ndbutils.notebook.exit(\"exit (0)\")  # type: ignore\n</code></pre>"},{"location":"helpers/job/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze \u2022 Silver \u2022 Gold</li> <li>Runtime overview and sample runtime: Runtime</li> <li>Checks &amp; Data Quality: Checks and Data Quality</li> <li>Table options and storage layout: Table Options</li> <li>Extenders, UDFs &amp; Views: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"helpers/runtime/","title":"Runtime Configuration","text":"<p>The Fabricks runtime is the folder where your Lakehouse project lives (configs, SQL, jobs, UDFs, extenders, views). This page explains how to point Fabricks to your runtime, how to structure it, and how to configure schedules and step paths.</p>"},{"location":"helpers/runtime/#minimal-setup-and-verification","title":"Minimal setup and verification","text":"<p>This quick path gets a working runtime running end-to-end using the sample included with Fabricks.</p> <p>1) Prepare pyproject configuration - Ensure your <code>pyproject.toml</code> contains a <code>[tool.fabricks]</code> block pointing to the sample runtime:   <pre><code>[tool.fabricks]\nruntime = \"examples/runtime\"\nnotebooks = \"fabricks/api/notebooks\"\njob_config_from_yaml = true\nloglevel = \"info\"\ndebugmode = false\nconfig = \"examples/runtime/fabricks/conf.fabricks.yml\"\n</code></pre></p> <p>2) Inspect the sample runtime - Directory: <code>examples/runtime</code> (see structure in \u201cSample runtime\u201d below). - It contains a minimal schedule and a Gold <code>hello_world.sql</code> full-refresh job.</p> <p>3) Run a schedule - Use your Databricks bundle/job or orchestration to run the schedule named <code>example</code> from <code>examples/runtime/fabricks/schedules/schedule.yml</code>. - You can also run step-by-step via the shipped notebooks referenced by <code>notebooks</code>.</p> <p>Expected outputs - Tables/views:   - A Gold table for the <code>hello_world</code> job (full refresh).   - If using memory mode jobs, temporary views are registered for downstream steps. - Logs:   - A completion line indicating job success; warnings/errors surfaced from checks/contracts if configured. - Data quality (if enabled):   - Built-in bound violations or contract <code>__action = 'fail'</code> causes a non\u2011zero exit and, for physical tables, an automatic rollback to the last successful version.   - Contract <code>__action = 'warning'</code> logs the message and the run continues.</p> <p>Tip: If your environment uses different storage locations or workspace setup, adjust <code>path_options</code> and <code>spark_options</code> in the runtime YAML before running.</p>"},{"location":"helpers/runtime/#pointing-to-your-runtime","title":"Pointing to your runtime","text":"<p>Configure Fabricks in your project's <code>pyproject.toml</code>:</p> <pre><code>[tool.fabricks]\nruntime = \"tests/integration/runtime\"          # Path to your runtime (jobs, SQL, configs)\nnotebooks = \"fabricks/api/notebooks\"           # Notebook helpers shipped with Fabricks\njob_config_from_yaml = true                     # Enable YAML job config\nloglevel = \"info\"\ndebugmode = false\nconfig = \"tests/integration/runtime/fabricks/conf.uc.fabricks.yml\"  # Main runtime config\n</code></pre> <ul> <li>runtime: path to your project's runtime directory</li> <li>config: path to your main runtime YAML configuration (see below)</li> <li>notebooks: optional helpers used by shipped notebooks</li> <li>job_config_from_yaml: enable loading jobs from YAML files</li> <li>loglevel/debugmode: control logging verbosity</li> </ul>"},{"location":"helpers/runtime/#main-runtime-config-yaml","title":"Main runtime config (YAML)","text":"<p>Define project-level options, step defaults, and step path mappings in <code>fabricks/conf.fabricks.yml</code>:</p> <pre><code>name: MyFabricksProject\noptions:\n  secret_scope: my_secret_scope\n  timeouts:\n    step: 3600\n    job: 3600\n    pre_run: 3600\n    post_run: 3600\n  workers: 4\npath_options:\n  storage: /mnt/data\nspark_options:\n  sql:\n    spark.sql.parquet.compression.codec: zstd\n\nbronze:\n  - name: bronze\n    path_options:\n      runtime: src/steps/bronze\n      storage: abfss://bronze@youraccount.blob.core.windows.net\n\nsilver:\n  - name: silver\n    path_options:\n      runtime: src/steps/silver\n      storage: abfss://silver@youraccount.blob.core.windows.net\n\ngold:\n  - name: transf\n    path_options:\n      runtime: src/steps/transf\n      storage: abfss://transf@youraccount.blob.core.windows.net\n  - name: gold\n    path_options:\n      runtime: src/steps/gold\n      storage: abfss://gold@youraccount.blob.core.windows.net\n</code></pre> <p>Key concepts: - options: global project config (secrets, timeouts, worker count) - path_options: shared storage/config paths - spark_options: default Spark SQL options applied for jobs - step sections (bronze/silver/gold/...): list of step instances with their runtime and storage paths</p>"},{"location":"helpers/runtime/#configuration-precedence","title":"Configuration precedence","text":"<p>Multiple layers of configuration can influence a run. When the same setting appears in several places, the following order applies (top wins):</p> <ol> <li>Job-level options in YAML (per job)</li> <li>Step-level defaults in runtime YAML (per step instance under bronze/silver/gold/semantic)</li> <li>Global options in runtime YAML (<code>options</code>, <code>spark_options</code>, <code>path_options</code>)</li> <li>Project-level defaults in <code>pyproject.toml</code> <code>[tool.fabricks]</code></li> <li>Internal defaults (shipped with Fabricks)</li> </ol> <p>Notes - <code>spark_options</code> are merged; job-level <code>spark_options</code> override keys from step/global levels. - <code>path_options</code> (e.g., <code>storage</code>, <code>runtime</code>) can be set globally, then refined per step instance. - Some options only make sense at specific levels (e.g., <code>check_options</code> at job level). - For CDC and write behavior, job <code>options</code> (e.g., <code>mode</code>, <code>change_data_capture</code>) take precedence over step defaults.</p>"},{"location":"helpers/runtime/#schedules","title":"Schedules","text":"<p>Schedules group jobs and define step order. Place schedules in your runtime (commonly under <code>fabricks/schedules/</code>):</p> <pre><code>- schedule:\n    name: test\n    options:\n      tag: test\n      steps: [bronze, silver, transf, gold, semantic]\n      variables:\n        var1: 1\n        var2: 2\n</code></pre> <p>Pass the schedule name when running the shipped notebooks or the Databricks bundle job.</p>"},{"location":"helpers/runtime/#typical-runtime-structure","title":"Typical runtime structure","text":"<p>A minimal runtime can look like:</p> <pre><code>fabricks/\n  conf.fabricks.yml\n  schedules/\n    schedule.yml\nbronze/\n  _config.example.yml\nsilver/\n  _config.example.yml\ngold/\n  gold/\n    _config.example.yml\n    hello_world.sql\nsemantic/\n  _config.example.yml\n</code></pre> <ul> <li>bronze/silver/gold/semantic: step folders with YAML configs and SQL</li> <li>fabricks/conf.fabricks.yml: main runtime config for paths and defaults</li> <li>fabricks/schedules/: defines one or more schedules for execution</li> </ul>"},{"location":"helpers/runtime/#sample-runtime","title":"Sample runtime","text":"<p>This section contains a minimal, copyable Fabricks runtime you can use to get started quickly. It mirrors the structure used in the integration tests and includes example configs and a working Gold job (<code>hello_world.sql</code>).</p> <p>What this is - A self-contained runtime layout showing how to organize steps (bronze, silver, gold, semantic) - Example YAML configuration files per step - A minimal schedule and config to run a simple job end-to-end</p> <p>Directory overview <pre><code>examples/runtime/\n  fabricks/\n    conf.fabricks.yml\n    schedules/\n      schedule.yml\n  bronze/\n    _config.example.yml\n  silver/\n    _config.example.yml\n  gold/\n    gold/\n      _config.example.yml\n      hello_world.sql\n  semantic/\n    _config.example.yml\n</code></pre></p> <p>Key files and purpose - <code>fabricks/conf.fabricks.yml</code>: project-level configuration (secret scope, timeouts, workers, storage paths, schedules path) - <code>fabricks/schedules/schedule.yml</code>: minimal schedule to run the gold step - <code>gold/gold/_config.example.yml</code>: defines a simple Gold job - <code>gold/gold/hello_world.sql</code>: example SQL for a Gold job (full refresh mode) - <code>bronze/_config.example.yml</code>, <code>silver/_config.example.yml</code>, <code>semantic/_config.example.yml</code>: example step configurations</p> <p>How to use this sample 1) Point <code>tool.fabricks.runtime</code> to this folder and set <code>config</code> to <code>fabricks/conf.fabricks.yml</code> in your <code>pyproject.toml</code>. 2) Run the Databricks bundle job with <code>schedule: example</code> (see the main documentation for bundle details). 3) Inspect the materialized outputs, logs, and table/view artifacts.</p>"},{"location":"helpers/runtime/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze \u2022 Silver \u2022 Gold</li> <li>Data quality checks and contracts: Checks &amp; Data Quality</li> <li>Table properties and physical layout: Table Options</li> <li>Custom logic integration: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"helpers/schedule/","title":"Schedules","text":"<p>Views provide a simple, declarative way to define a set of jobs to run in a schedule. Instead of listing jobs manually, you create a SQL view that filters the canonical <code>fabricks.jobs</code> table, and then reference that view by name from a schedule.</p> <p>Typical use cases: - Run all jobs for a given domain/topic (e.g., monarch) - Run only certain steps (e.g., all Gold jobs) - Run a curated subset of jobs for ad\u2011hoc backfills or smoke tests</p>"},{"location":"helpers/schedule/#types-of-views-in-fabricks","title":"Types of views in Fabricks","text":"<ul> <li>Data views (user\u2011authored):</li> <li>You place <code>.sql</code> files under your runtime <code>views</code> path (see below).</li> <li>Each file defines one SQL view created under the <code>fabricks</code> schema.</li> <li> <p>These are typically simple filters over <code>fabricks.jobs</code>.</p> </li> <li> <p>Schedule views (framework\u2011generated):</p> </li> <li>For each <code>schedule</code> defined in your runtime YAML, Fabricks can generate a companion view named <code>fabricks.&lt;schedule_name&gt;_schedule</code>.</li> <li>A schedule view selects <code>j.*</code> from <code>fabricks.jobs</code> and applies optional filters from the schedule\u2019s options (<code>view</code>, <code>steps</code>, <code>tag</code>), excluding manual jobs.</li> </ul> <p>Both kinds of views are useful: data views define job subsets; schedule views expose the final, resolved set for each schedule.</p>"},{"location":"helpers/schedule/#how-it-works","title":"How it works","text":"<ul> <li>Source of truth: Fabricks maintains <code>fabricks.jobs</code> with one row per job.</li> <li>Data views: Your SQL selects a subset from <code>fabricks.jobs</code> (recommended: <code>select *</code>).</li> <li>Schedules:</li> <li>In <code>schedule.yml</code>, you can set <code>options.view: &lt;data_view_name&gt;</code>.</li> <li>Fabricks resolves the schedule\u2019s membership via the data view (and optional <code>steps</code> / <code>tag</code> filters).</li> <li>Fabricks can also materialize a schedule view <code>fabricks.&lt;schedule_name&gt;_schedule</code> for inspection and tooling.</li> </ul>"},{"location":"helpers/schedule/#where-to-put-data-view-sql-files","title":"Where to put data view SQL files","text":"<p>The runtime configuration defines where Fabricks looks for view SQL files:</p> <p>```yaml title:conf.fabricks.yml (excerpt) path_options:   views: fabricks/views <pre><code>- Place your `.sql` files under the runtime `views` directory (e.g., `.../fabricks/views/`).\n- Each file defines one view; the file name (without `.sql`) becomes the view name.\n- During initialization, Fabricks will create or replace these as SQL views in the `fabricks` schema.\n\n---\n\n## Minimal data view example\n\nCreate a file `monarch.sql` in your runtime `views` directory:\n\n```sql title:monarch.sql\n-- Select all jobs with a specific topic\nselect *\nfrom fabricks.jobs j\nwhere j.topic = 'monarch'\n</code></pre></p> <p>This will create a view called <code>fabricks.monarch</code>. You can then reference this view in a schedule.</p>"},{"location":"helpers/schedule/#using-a-data-view-in-a-schedule","title":"Using a data view in a schedule","text":"<pre><code>- schedule:\n    name: run-monarch\n    options:\n      view: monarch            # join to fabricks.monarch on job_id\n      steps: [silver, gold]    # optional: restrict to these steps\n      tag: nightly             # optional: only jobs having this tag\n      # variables, timeouts, etc. can still be provided as needed\n</code></pre> <p>Behavior: - Fabricks loads all jobs returned by <code>fabricks.monarch</code> and further filters by <code>steps</code> and <code>tag</code> if provided. - Schedule\u2011level options like <code>variables</code>, <code>timeouts</code>, etc., still apply to execution. - In most cases the view alone defines the job set; <code>steps</code>/<code>tag</code> refine it.</p>"},{"location":"helpers/schedule/#schedule-views-generated","title":"Schedule views (generated)","text":"<p>For each schedule, Fabricks can generate a companion SQL view named <code>fabricks.&lt;schedule_name&gt;_schedule</code> that reflects the resolved membership. The core SQL shape (simplified) is:</p> <pre><code>create or replace view fabricks.&lt;schedule_name&gt;_schedule as\nselect j.*\nfrom fabricks.jobs j\n/* optional: if options.view is provided */\ninner join fabricks.&lt;options.view&gt; v on j.job_id = v.job_id\nwhere true\n  /* optional: if options.steps is provided */\n  and j.step in ('bronze','silver','gold' /* ... */)\n  /* optional: if options.tag is provided (array of tags on the job) */\n  and array_contains(j.tags, '&lt;tag&gt;')\n  and j.type not in ('manual');\n</code></pre> <p>Notes: - The inner join requires the data view to expose <code>job_id</code>. Returning <code>j.*</code> from <code>fabricks.jobs</code> guarantees this. - Manual jobs are excluded from schedule views by design.</p> <p>Quick validation: <pre><code>select count(*) from fabricks.run-monarch_schedule;         -- schedule membership size\nselect step, topic, item from fabricks.run-monarch_schedule limit 20;\n</code></pre></p>"},{"location":"helpers/schedule/#registration-and-lifecycle","title":"Registration and lifecycle","text":"<p>You can (re)create both data views and schedule views programmatically.</p> <ul> <li> <p>Data views (from <code>.sql</code> files under <code>path_options.views</code>):   <pre><code>from fabricks.core.views import (\n    create_or_replace_view,   # single data view by name\n    create_or_replace_views,  # all data views under PATH_VIEWS\n)\n\ncreate_or_replace_view(\"monarch\")\ncreate_or_replace_views()\n</code></pre></p> </li> <li> <p>Schedule views (one per schedule defined in runtime):   <pre><code>from fabricks.core.schedules import (\n    create_or_replace_view as create_or_replace_schedule_view,  # single schedule by name\n    create_or_replace_views as create_or_replace_schedule_views # all schedules\n)\n\ncreate_or_replace_schedule_view(\"run-monarch\")\ncreate_or_replace_schedule_views()\n</code></pre></p> </li> <li> <p>Initialization path:</p> </li> <li>The bootstrap script (\u201carmageddon\u201d) calls both data view and schedule view builders so your environment is in sync with runtime configs and SQL view files.</li> </ul>"},{"location":"helpers/schedule/#additional-data-view-examples","title":"Additional data view examples","text":"<p>Filter by step: ```sql title:gold_only.sql select * from fabricks.jobs j where j.step = 'gold' <pre><code>Filter by a set of topics:\n```sql title:core_topics.sql\nselect *\nfrom fabricks.jobs j\nwhere j.topic in ('sales', 'finance', 'marketing')\n</code></pre></p> <p>Filter by both step and topic pattern: ```sql title:gold_sales_like.sql select * from fabricks.jobs j where j.step = 'gold'   and j.topic like 'sales_%' <pre><code>Curate explicit jobs:\n```sql title:curated_selection.sql\nselect *\nfrom fabricks.jobs j\nwhere (j.step = 'silver' and j.topic = 'monarch' and j.item = 'scd1__current')\n   or (j.step = 'gold'   and j.topic = 'monarch' and j.item = 'orders')\n</code></pre></p> <p>Tip: - You may project specific columns, but for schedules that use <code>options.view</code>, ensure <code>job_id</code> is present in the projection. Returning <code>j.*</code> is simplest.</p>"},{"location":"helpers/schedule/#best-practices","title":"Best practices","text":"<ul> <li>Keep view logic simple: use filters, avoid heavy joins or transformations.</li> <li>Align views with business domains or execution scopes (by step, topic family, tags).</li> <li>Use explicit OR lists for one\u2011off backfills to keep intent clear and auditable.</li> <li>Favor stable view names; schedules reference the view name.</li> </ul>"},{"location":"helpers/schedule/#related-topics","title":"Related topics","text":"<ul> <li>Initialization and bootstrap: ../helpers/init.md</li> <li>Job helper and operations: ../helpers/job.md</li> <li>Steps overview (Bronze/Silver/Gold): ../steps/gold.md and siblings</li> <li>Data quality checks: ./checks-data-quality.md</li> <li>Table options and storage layout: ./table-options.md</li> </ul>"},{"location":"helpers/step/","title":"Step Helper (Databricks Notebook)","text":"<p>This helper describes a widget-driven approach in a Databricks notebook to manage a Fabricks step (bronze, silver, gold). </p> <p>Core imports used: <pre><code>from fabricks.api import get_step\n</code></pre></p>"},{"location":"helpers/step/#typical-usage","title":"Typical usage","text":"<p>1) Resolve the step: <pre><code>from fabricks.api import get_step\n\ns = get_step(\"gold\")  # or \"bronze\"/\"silver\"\n</code></pre></p> <p>2) Run actions: <pre><code># Examples:\ns.update_jobs()\ns.update_dependencies(progress_bar=False)\n\n# Cleanup if required:\n# s.drop()\n</code></pre></p>"},{"location":"helpers/step/#helper-notebook","title":"Helper Notebook","text":"<pre><code># Databricks notebook source\nimport os\nfrom multiprocessing import Pool\nfrom typing import Callable, List\n\nfrom databricks.sdk.runtime import dbutils, spark\n\n# COMMAND ----------\n\ndbutils.widgets.text(\"steps\", \"---\", label=\"6 - Step(s)\")\ndbutils.widgets.multiselect(\n    \"actions\",\n    \"update-configurations\",\n    [\n        \"update-configurations\",    -- Update job configurations\n        \"add-missing-jobs\",         -- Create any missing jobs\n        \"update-dependencies\",      -- Update job dependencies\n        \"update-lists\",             -- Update lists of jobs/tables/views\n        \"update-tables-list\",       -- Update the list of tables\n        \"update-views-list\",        -- Update the list of views\n        \"update\"                    -- Update the step\n    ],\n    label=\"5 - Action(s)\"\n)\ndbutils.widgets.dropdown(\"loglevel\", \"info\", [\"debug\", \"info\", \"error\", \"critical\"], label=\"3 - Log Level\")\ndbutils.widgets.dropdown(\"chdir\", \"False\", [\"True\", \"False\"], label=\"1 - Change Directory\")\ndbutils.widgets.dropdown(\"config_from_yaml\", \"True\", [\"True\", \"False\"], label=\"2 - Use YAML\")\ndbutils.widgets.dropdown(\"debugmode\", \"False\", [\"True\", \"False\"], label=\"4 - Debug Mode\")\n\n# COMMAND ----------\n\nif dbutils.widgets.get(\"chdir\").lower() == \"true\":\n    user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()  # type: ignore\n    try:\n        os.chdir(f\"/Workspace/Users/{user}/Fabricks.Runtime\")\n    except FileNotFoundError:\n        os.chdir(f\"/Workspace/Users/{user}/runtime\")\n\n# COMMAND ----------\n\nif dbutils.widgets.get(\"config_from_yaml\").lower() == \"true\":\n    os.environ[\"FABRICKS_IS_JOB_CONFIG_FROM_YAML\"] = \"1\"\nelse:\n    os.environ[\"FABRICKS_IS_JOB_CONFIG_FROM_YAML\"] = \"0\"\n\n# COMMAND ----------\n\nif dbutils.widgets.get(\"debugmode\").lower() == \"true\":\n    os.environ[\"FABRICKS_IS_DEBUGMODE\"] = \"1\"\nelse:\n    os.environ[\"FABRICKS_IS_DEBUGMODE\"] = \"0\"\n\n# COMMAND ----------\n\nloglevel = dbutils.widgets.get(\"loglevel\")\nif loglevel == \"debug\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"DEBUG\"\nelif loglevel == \"info\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"INFO\"\nelif loglevel == \"error\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"ERROR\"\nelif loglevel == \"critical\":\n    os.environ[\"FABRICKS_LOGLEVEL\"] = \"CRITICAL\"\n\n# COMMAND ----------\n\nfrom fabricks.api import get_step  # noqa: E402\nfrom fabricks.api.log import DEFAULT_LOGGER  # noqa: E402\n\n# COMMAND ----------\n\nactions = dbutils.widgets.get(\"actions\").split(\",\")\nactions = [a.strip() for a in actions]\n\n# COMMAND ----------\n\n\ndef do(step: str) -&gt; None:\n    s = get_step(step=step)\n    todos: dict[str, Callable] = {}\n\n    if \"update-configurations\" in actions:\n        todos[\"update-configurations\"] = s.update_jobs\n\n    if \"add-missing-jobs\" in actions:\n        todos[\"add-missing-jobs\"] = s.create_jobs\n\n    if \"update-views-list\" in actions or \"update-lists\" in actions:\n        todos[\"update-views-list\"] = s.update_views\n\n    if \"update-tables-list\" in actions or \"update-lists\" in actions:\n        todos[\"update-tables-list\"] = s.update_tables\n\n    if \"update\" in actions and len(actions) == 1:\n        todos[\"update\"] = s.update\n\n    if \"update-dependencies\" in actions:\n        todos[\"update-dependencies\"] = s.update_dependencies\n\n    for key, func in todos.items():\n        func()\n\n# COMMAND ----------\n\nactions = [s.strip() for s in dbutils.widgets.get(\"actions\").split(\",\")]\nsteps = [s.strip() for s in dbutils.widgets.get(\"steps\").split(\",\")]\nsteps = [[j.strip() for j in job.split(\"//\")] if \"//\" in job else job for job in steps]\n\nfor step in steps:\n    DEFAULT_LOGGER.warning(\", \".join(actions), extra={\"step\": step})\n\n# COMMAND ----------\n\nfor step in steps:\n    if isinstance(step, List):\n        with Pool() as pool:\n            results = pool.map(do, step)\n    else:\n        do(step)\n\n# COMMAND ----------\n\ndbutils.notebook.exit(\"exit (0)\")  # type: ignore\n\n# COMMAND ----------\n</code></pre>"},{"location":"helpers/step/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze \u2022 Silver \u2022 Gold</li> <li>Runtime overview and sample runtime: Runtime</li> <li>Checks &amp; Data Quality: Checks and Data Quality</li> <li>Table options and storage layout: Table Options</li> <li>Extenders, UDFs &amp; Views: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"introduction/layers/","title":"Layers (Bronze \u2192 Silver \u2192 Gold)","text":"<p>Fabricks organizes pipelines into layered steps to keep responsibilities clear and composable. Use this page to decide where logic belongs and how to structure data flow.</p> <p>Layer summary</p> <ul> <li>Bronze: Land raw data \u201cas-is\u201d from files/streams/tables. Keep it light.</li> <li>Silver: Standardize, clean, enrich; optionally apply CDC (SCD1/SCD2). Provide conformed datasets and convenience views.</li> <li>Gold: Build consumption-ready models (facts/dims/marts), possibly with CDC merges or full refreshes.</li> </ul> <p>When to use each layer</p> <ul> <li> <p>Bronze</p> <ul> <li>Ingest from external sources with minimal logic</li> <li>Keep provenance and raw fidelity</li> <li>Typical modes: memory, append, register</li> </ul> </li> <li> <p>Silver</p> <ul> <li>Standardize schemas, apply business keys, deduplicate</li> <li>Apply CDC: scd1/scd2 with convenience views (e.g., table__current)</li> <li>Typical modes: memory, append, latest, update, combine</li> </ul> </li> <li> <p>Gold</p> <ul> <li>Aggregate, reshape, and model data for analytics/reporting</li> <li>Implement SCD patterns at the desired consumption grain</li> <li>Typical modes: memory, append, complete, update, invoke (notebook)</li> </ul> </li> </ul> <p>Data flow</p> <ul> <li>Bronze: raw sources \u2192 (light parsing, optional filters/calculated columns) \u2192 raw tables/views</li> <li>Silver: bronze tables/views \u2192 (standardize/enrich, optional CDC) \u2192 conformed tables + {table}__current view</li> <li>Gold: silver conformed outputs \u2192 (marts/dim/fact, SCD merges or complete refresh) \u2192 business-consumption tables/views</li> </ul> <p>Quality and governance</p> <ul> <li>Configure pre_run/post_run checks and row thresholds in step/job options.</li> <li>Use warning/fail contracts to protect downstream consumers.</li> </ul> <p>Related</p> <ul> <li>Bronze: ../steps/bronze.md</li> <li>Silver: ../steps/silver.md</li> <li>Gold: ../steps/gold.md</li> <li>CDC reference: ../reference/cdc.md</li> <li>Data quality checks: ../reference/checks-data-quality.md</li> <li>Table options: ../reference/table-options.md</li> </ul>"},{"location":"introduction/overview/","title":"Key Concepts Overview","text":"<p>This page introduces Fabricks core concepts so you can read and write pipelines with confidence.</p> <p>What Fabricks is ?</p> <ul> <li>SQL-first transformations</li> <li>YAML-based orchestration for jobs, steps, schedules, and dependencies</li> <li>Built-in Change Data Capture (CDC) patterns (SCD1/SCD2)</li> <li>Data Quality gates via checks and contracts</li> <li>Extensible with UDFs, Parsers, and Python Extenders</li> </ul> <p>Jobs vs Steps</p> <ul> <li>Step: A processing layer/type that defines behavior and defaults (Bronze, Silver, Gold).</li> <li>Job: A concrete unit of work within a step (identified by topic + item) that you schedule/run.</li> </ul> <p>Layers (Bronze \u2192 Silver \u2192 Gold)</p> <ul> <li>Bronze: Land raw data (append/register/memory). Keep logic light.</li> <li>Silver: Standardize/clean/enrich. Optional CDC (SCD1/SCD2) and convenience views.</li> <li>Gold: Consumption models and marts. Supports merge/update and SCD patterns.</li> </ul> <p>Change Data Capture (CDC)</p> <ul> <li>SCD1: Current state with optional soft deletes (__is_current, __is_deleted) and a {table}__current view.</li> <li>SCD2: Historical validity windows (__valid_from, __valid_to). Gold consumers use (__key, __timestamp, __operation).</li> <li>Reload rectification: A snapshot boundary that reconciles target state (see CDC reference).</li> </ul> <p>Data Quality (Checks)</p> <ul> <li>Pre- and post-run checks, contracts, and row count thresholds.</li> <li>CI-friendly: Fail fast on contract violations or warn when needed.</li> </ul> <p>Extensibility</p> <ul> <li>UDFs: Register functions on the Spark session for use in SQL.</li> <li>Parsers: Custom readers/cleaners for sources (return a DataFrame).</li> <li>Extenders: Small Python transforms applied right before writing.</li> </ul> <p>Navigation</p> <ul> <li>Layers guide: Layers (Bronze/Silver/Gold) \u2192 Layers</li> <li>CDC strategies and contracts \u2192 CDC</li> <li>Step references \u2192 Bronze \u2022 Silver \u2022 Gold</li> <li>Data Quality \u2192 Checks &amp; Data Quality</li> <li>Table and storage options \u2192 Table Options</li> <li>Extensibility \u2192 Extenders, UDFs &amp; Parsers</li> </ul>"},{"location":"reference/cdc/","title":"Change Data Capture (CDC)","text":"<p>Fabricks provides built-in Change Data Capture (CDC) patterns to keep downstream tables synchronized with upstream changes using SQL-first pipelines. CDC is enabled per job via <code>options.change_data_capture</code> and implemented by generated MERGE/INSERT statements driven by small helper columns.</p> <p>This page explains the supported CDC strategies, required inputs, merge semantics, and examples.</p>"},{"location":"reference/cdc/#strategies","title":"Strategies","text":"Strategy Description Convenience views <code>nocdc</code> No CDC; writes the result as-is. \u2014 <code>scd1</code> Tracks current vs deleted; maintains flags <code>__is_current</code>, <code>__is_deleted</code>. <code>{table}__current</code> in Silver <code>scd2</code> Slowly Changing Dimension Type 2: validity windows with <code>__valid_from</code>, <code>__valid_to</code>. <code>{table}__current</code> in Silver"},{"location":"reference/cdc/#what-is-scd1","title":"What is SCD1?","text":"<ul> <li>Definition: Slowly Changing Dimension Type 1 keeps only the current state of each business key. Attribute changes overwrite previous values rather than preserving history.</li> <li>Typical columns in Fabricks: <code>__is_current</code>, <code>__is_deleted</code>. A convenience view <code>{table}__current</code> in Silver selects current non-deleted rows.</li> <li>When to use: When downstream consumers only need the latest values and you do not need to answer \u201cas-of\u201d questions or audit historical attribute values.</li> </ul>"},{"location":"reference/cdc/#what-is-scd2","title":"What is SCD2?","text":"<ul> <li>Definition: Slowly Changing Dimension Type 2 preserves the full change history by creating a new versioned row each time attributes change. Each row covers a validity window for a given business key.</li> <li>Typical columns in Fabricks: <code>__valid_from</code>, <code>__valid_to</code>, and <code>__is_current</code> (optional <code>__is_deleted</code> if soft-deletes are modeled). Silver also provides <code>{table}__current</code> for latest rows.</li> <li>When to use: When you must answer \u201cas-of\u201d queries (e.g., \u201cWhat was the customer segment on 2024\u201103\u201101?\u201d), analyze changes over time, or maintain auditable history. In Gold SCD2 merges, inputs use <code>__key</code>, <code>__timestamp</code>, <code>__operation</code> to define change points.</li> </ul>"},{"location":"reference/cdc/#how-to-enable-cdc","title":"How to enable CDC","text":"<p>Set the CDC strategy in the job options:</p> <pre><code>- job:\n    step: silver\n    topic: demo\n    item: scd1\n    options:\n      mode: update\n      change_data_capture: scd1\n      parents: [bronze.demo_source]\n</code></pre> <p>For Gold:</p> <pre><code>- job:\n    step: gold\n    topic: scd2\n    item: update\n    options:\n      mode: update\n      change_data_capture: scd2\n</code></pre> <p>Supported values: <code>nocdc</code>, <code>scd1</code>, <code>scd2</code>.</p>"},{"location":"reference/cdc/#input-contracts","title":"Input contracts","text":"<p>Some helper columns govern CDC behavior. Fabricks generates additional internal helpers during processing.</p> <ul> <li> <p>Gold jobs (consumer side):</p> <ul> <li>scd2 (required): <code>__key</code>, <code>__timestamp</code>, <code>__operation</code> with values <code>'upsert' | 'delete' | 'reload'</code>.</li> <li>scd1 (required): <code>__key</code>; optional <code>__timestamp</code> / <code>__operation</code> (<code>'upsert' | 'delete' | 'reload'</code>) for delete/rectify handling.</li> <li>Note: If <code>__operation</code> is absent in Gold SCD update jobs, Fabricks auto-injects <code>__operation = 'reload'</code> and enables rectification.</li> <li>Optional helpers used by merges:<ul> <li><code>__order_duplicate_by_asc</code> / <code>__order_duplicate_by_desc</code></li> <li><code>__identity</code> (only when <code>table_options.identity</code> is not true; if <code>identity: true</code>, the identity column is auto-created and you should not supply <code>__identity</code>)</li> <li><code>__source</code> (to scope merges by logical source)</li> </ul> </li> </ul> </li> <li> <p>Silver jobs (producer side):</p> <ul> <li>Provide business keys through job-level <code>keys</code> (or compute a <code>__key</code>) to support downstream CDC.</li> <li>Silver can apply CDC directly and yields convenience views (e.g., <code>{table}__current</code>).</li> </ul> </li> </ul> <p>Note</p> <ul> <li>Memory outputs ignore columns that start with <code>__</code>.</li> <li>Special characters in column names are preserved.</li> </ul> <p>See details in:</p> <ul> <li>Steps: Silver \u2022 Gold</li> <li>Table Options</li> </ul>"},{"location":"reference/cdc/#merge-semantics-under-the-hood","title":"Merge semantics (under the hood)","text":"<p>Fabricks compiles CDC operations into SQL via Jinja templates at runtime. The core logic lives in <code>fabricks.cdc</code>:</p> <ul> <li><code>Merger.get_merge_query</code> renders <code>templates/merge.sql.jinja</code> for the selected <code>change_data_capture</code> strategy.</li> <li>The framework computes internal columns such as:<ul> <li><code>__merge_condition</code> \u2014 one of <code>'upsert' | 'delete' | 'update' | 'insert'</code> depending on strategy and inputs.</li> <li><code>__merge_key</code> \u2014 a synthetic key used to join against the target.</li> </ul> </li> <li>You usually do not set these internal fields manually; they are derived from your inputs (<code>__key</code>, <code>__operation</code>, <code>__timestamp</code>) and job options.</li> </ul> <p>Join keys</p> <ul> <li>If a <code>__key</code> column exists in the target, merges use <code>t.__key = s.__merge_key</code>.</li> <li>Otherwise, the configured <code>keys</code> option is used to build an equality join on business keys.</li> </ul> <p>Source scoping</p> <ul> <li>If <code>__source</code> exists in both sides, merges add <code>t.__source = s.__source</code> to support multi-source data in the same table.</li> </ul> <p>Soft delete vs hard delete (SCD1)</p> <ul> <li>If the incoming data contains <code>__is_deleted</code>, the SCD1 template performs soft deletes:<ul> <li>Sets <code>__is_current = false</code>, <code>__is_deleted = true</code> on delete.</li> </ul> </li> <li>If <code>__is_deleted</code> is absent, deletes are physical for SCD1.</li> </ul> <p>Timestamps and metadata</p> <ul> <li>If the incoming data provides <code>__timestamp</code>, it is propagated to the target.</li> <li>If the target has <code>__metadata</code>, the <code>updated</code> timestamp is set to the current time during updates/deletes.</li> </ul> <p>Identity and hash</p> <ul> <li>If <code>table_options.identity: true</code>, the identity column is created automatically when the table is created.</li> <li>If <code>table_options.identity</code> is not true and <code>__identity</code> is present in the input, it will be written as a regular column.</li> <li>If <code>__hash</code> is present, it is updated during upsert operations.</li> </ul> <p>Update filtering</p> <ul> <li><code>options.update_where</code> can constrain rows affected during merges (useful for limiting the scope of updates).</li> </ul> <p>Internals reference</p> <ul> <li><code>framework/fabricks/cdc/base/merger.py</code></li> <li>Templates under <code>framework/fabricks/cdc/templates/merge/*.sql.jinja</code></li> </ul>"},{"location":"reference/cdc/#scd1-details","title":"SCD1 details","text":"<p>Behavior (see <code>merge/scd1.sql.jinja</code>)</p> <ul> <li>Upsert (<code>__merge_condition = 'upsert'</code>): updates matching rows and inserts non\u2011matching rows.</li> <li>Delete (<code>__merge_condition = 'delete'</code>):<ul> <li>Soft delete if <code>__is_deleted</code> is part of the schema: sets <code>__is_current = false</code>, <code>__is_deleted = true</code>.</li> <li>Otherwise, performs a physical delete.</li> </ul> </li> </ul> <p>Convenience view (in Silver)</p> <ul> <li><code>{table}__current</code>: filters current (non\u2011deleted) rows for simplified consumption.</li> </ul> <p>Minimal Silver example</p> <pre><code>- job:\n    step: silver\n    topic: monarch\n    item: scd1\n    options:\n      mode: update\n      change_data_capture: scd1\n</code></pre> <p>Gold consumption example</p> <pre><code>-- Example: consuming current rows from SCD1 silver output\nselect id as id, name as monarch\nfrom silver.monarch_scd1__current\n</code></pre>"},{"location":"reference/cdc/#reload-operation","title":"Reload operation","text":"<p><code>'reload'</code> is a CDC input operation used as a reconciliation boundary. It is not a direct MERGE action; instead, it signals Fabricks to rectify the target table using the supplied snapshot at that timestamp.</p> <ul> <li>Purpose: mark a full or partial snapshot boundary so missing keys can be treated as deletes and present keys as upserts as needed.</li> <li>Auto-injection: when a Gold SCD job runs in <code>mode: update</code> and your SQL does not provide <code>__operation</code>, Fabricks injects <code>__operation = 'reload'</code> and turns on rectification.</li> <li>Silver behavior:<ul> <li>If a batch contains <code>'reload'</code> after the target\u2019s max timestamp, Silver enables rectification logic.</li> <li>In <code>mode: latest</code>, <code>'reload'</code> is not allowed and will be rejected.</li> </ul> </li> <li>Gold behavior:<ul> <li>Passing <code>reload=True</code> to a Gold job run triggers a full <code>complete</code> write for that run.</li> </ul> </li> <li>Internals: rectification is computed in <code>framework/fabricks/cdc/templates/query/rectify.sql.jinja</code>, which computes next operations/windows around <code>'reload'</code> markers.</li> </ul> <p>Warning</p> <p>In Silver <code>mode: latest</code>, <code>'reload'</code> is forbidden and will be rejected.  Use <code>mode: update</code> or <code>mode: append</code> instead if you need reconciliation behavior.</p> <p>Tip</p> <p>You generally do not need to emit <code>'reload'</code> manually in Gold SCD update jobs; it is injected for you when <code>__operation</code> is missing.  For explicit control, you can produce rows with <code>__operation = 'reload'</code> at the snapshot timestamp.</p>"},{"location":"reference/cdc/#scd2-details","title":"SCD2 details","text":"<p>Behavior (see <code>merge/scd2.sql.jinja</code>):</p> <ul> <li>Update (<code>__merge_condition = 'update'</code>): closes the current row by setting <code>__valid_to = __valid_from - 1 second</code>, <code>__is_current = false</code>. A subsequent insert creates the new current row.</li> <li>Delete (<code>__merge_condition = 'delete'</code>): closes the current row and sets <code>__is_current = false</code> (and <code>__is_deleted = true</code> if soft delete is modeled).</li> <li>Insert (<code>__merge_condition = 'insert'</code>): inserts a new current row.</li> </ul> <p>Required Gold inputs:</p> <ul> <li><code>__key</code>, <code>__timestamp</code>, <code>__operation</code> with values <code>'upsert' | 'delete' | 'reload'</code>.</li> </ul> <p>Reload notes:</p> <ul> <li><code>'reload'</code> marks a reconciliation boundary; Fabricks derives concrete actions (e.g., closing current rows, inserting new ones, deleting missing keys) across that boundary.</li> <li>If you omit <code>__operation</code> in Gold SCD update jobs, Fabricks injects <code>'reload'</code> and enables rectification automatically.</li> <li>In Silver:<ul> <li>Presence of <code>'reload'</code> (beyond target\u2019s max timestamp) enables rectification.</li> <li><code>'reload'</code> is forbidden in <code>mode: latest</code>.</li> </ul> </li> </ul> <p>Optional features:</p> <ul> <li><code>options.correct_valid_from</code>: adjusts start timestamps for validity windows.</li> <li><code>options.persist_last_timestamp</code>: persists last processed timestamp for incremental loads.</li> </ul> <p>Convenience view:</p> <ul> <li><code>{table}__current</code>: returns only the latest (current) rows per business key.</li> </ul> <p>Minimal Silver example:</p> <pre><code>- job:\n    step: silver\n    topic: monarch\n    item: scd2\n    options:\n      mode: update\n      change_data_capture: scd2\n</code></pre> <p>Note: Credit \u2014 Temporal Snapshot Fact Table (SQLBits 2012). Recommended to watch to understand SCD2 snapshot-based modeling concepts. </p> <p>Slides: Temporal Snapshot Fact Tables (slides)</p> <p>Gold input construction example:</p> <pre><code>-- Turn SCD2 changes into Gold input operations\nwith dates as (\n  select id as id, __valid_from as __timestamp, 'upsert' as __operation\n  from silver.monarch_scd2 where __valid_from &gt; '1900-01-02'\n  union\n  select id as id, __valid_to as __timestamp, 'delete' as __operation\n  from silver.monarch_scd2 where __is_deleted\n)\nselect\n  d.id as __key,\n  s.id as id,\n  s.name as monarch,\n  s.doubleField as value,\n  d.__operation,\n  if(d.__operation = 'delete', d.__timestamp + interval 1 second, d.__timestamp) as __timestamp\nfrom dates d\nleft join silver.monarch_scd2 s\n  on d.id = s.id and d.__timestamp between s.__valid_from and s.__valid_to\n</code></pre>"},{"location":"reference/cdc/#keys-and-__key","title":"Keys and <code>__key</code>","text":"<ul> <li>Preferred: produce a stable <code>__key</code> in your SELECTs (e.g., UDF that hashes business keys).</li> <li>Alternative: configure <code>options.keys: [ ... ]</code> to specify business keys. Fabricks derives join predicates from these when <code>__key</code> is not present.</li> </ul> <p>Tip</p> <ul> <li>Only provide <code>__identity</code> when <code>table_options.identity</code> is not true. If <code>identity: true</code>, the identity column is auto-created when the table is created; do not include <code>__identity</code>. See Table Options.</li> </ul>"},{"location":"reference/cdc/#examples","title":"Examples","text":"<p>Silver SCD1 with duplicates handling</p> <pre><code>- job:\n    step: silver\n    topic: princess\n    item: order_duplicate\n    options:\n      mode: update\n      change_data_capture: scd1\n      order_duplicate_by:\n        order_by: desc\n</code></pre> <p>Gold SCD1 update with incremental timestamp</p> <pre><code>- job:\n    step: gold\n    topic: scd1\n    item: last_timestamp\n    options:\n      change_data_capture: scd1\n      mode: update\n      persist_last_timestamp: true\n</code></pre>"},{"location":"reference/cdc/#operational-notes","title":"Operational notes","text":"<ul> <li>Streaming: where supported, <code>options.stream: true</code> enables incremental semantics.</li> <li>Parents: use <code>options.parents</code> to order upstream dependencies.</li> <li>Checks: configure quality gates via <code>options.check_options</code>. See Checks &amp; Data Quality.</li> </ul>"},{"location":"reference/cdc/#related","title":"Related","text":"<ul> <li>Steps: Silver \u2022 Gold</li> <li>Reference: Table Options, Extenders, UDFs &amp; Parsers</li> <li>Sample runtime: Sample runtime</li> </ul>"},{"location":"reference/checks-data-quality/","title":"Checks and Data Quality","text":"<p>Fabricks provides built-in mechanisms to validate data before and/or after a job runs. Checks can enforce row counts, equality constraints, and custom SQL contracts that return actions and messages.</p>"},{"location":"reference/checks-data-quality/#configuring-checks","title":"Configuring checks","text":"<p>Enable checks per job using <code>check_options</code>:</p> <pre><code>- job:\n    step: gold\n    topic: check\n    item: max_rows\n    options: { mode: complete }\n    check_options:\n      pre_run: true         # run checks before writing\n      post_run: true        # run checks after writing\n      min_rows: 10          # minimum expected row count\n      max_rows: 10000       # maximum expected row count\n      count_must_equal: fabricks.dummy  # enforce row count equality with another table\n      skip: false           # skip all checks if true\n</code></pre> <p>Supported options: - pre_run/post_run: Run checks around the execution of the job. - min_rows/max_rows: Row count bounds. - count_must_equal: Assert row count equals a reference table. - skip: Disable checks for this job.</p>"},{"location":"reference/checks-data-quality/#catalog-of-checks","title":"Catalog of checks","text":"<p>These are the built-in switches you can use in <code>check_options</code>. Built-in checks evaluate simple properties of the dataset and will fail the job on violation. Use SQL contracts when you need richer logic or to emit warnings instead of hard failures.</p> Name Type Default Required Description Failure behavior pre_run boolean false no Run SQL contracts and built-in checks before the write. Contract: <code>__action</code> controls outcome. post_run boolean false no Run SQL contracts and built-in checks after the write. Contract: <code>__action</code> controls outcome. min_rows integer unset no Minimum expected row count of the dataset being written. Fail if <code>count &lt; min_rows</code>. max_rows integer unset no Maximum allowed row count of the dataset being written. Fail if <code>count &gt; max_rows</code>. count_must_equal string (table/view) unset no Require row count to equal a reference table or view. Fail if counts differ. skip boolean false no Disable all checks for this job. No checks executed."},{"location":"reference/checks-data-quality/#examples","title":"Examples","text":"<p>Bounds only: <pre><code>check_options:\n  pre_run: true\n  min_rows: 10\n  max_rows: 100000\n</code></pre></p> <p>Equality against a reference: <pre><code>check_options:\n  post_run: true\n  count_must_equal: analytics.dim_customer\n</code></pre></p> <p>Mixing built-ins with SQL contracts: - Use <code>pre_run</code>/<code>post_run</code> to enable contract execution and add your contract SQL files (see \u201cSQL contracts\u201d below).</p>"},{"location":"reference/checks-data-quality/#sql-contracts","title":"SQL contracts","text":"<p>When <code>pre_run</code> or <code>post_run</code> checks are enabled, provide SQL files named after the table or job: - <code>table_name.pre_run.sql</code> - <code>table_name.post_run.sql</code></p> <p>Each SQL must return: - <code>__action</code>: either <code>'fail'</code> or <code>'warning'</code> - <code>__message</code>: human-readable message</p> <p>Example:</p> <pre><code>-- fail.pre_run.sql\nselect \"fail\" as __action, \"Please don't fail on me :(\" as __message;\n\n-- warning.post_run.sql\nselect \"warning\" as __action, \"I want you to warn me !\" as __message;\n</code></pre>"},{"location":"reference/checks-data-quality/#behavior-and-rollback","title":"Behavior and rollback","text":"<ul> <li>For physical tables (append/complete/update modes), a failed check triggers an automatic rollback to the previous successful version.</li> <li>For <code>memory</code> mode (views), results are not persisted and failures are logged only.</li> </ul>"},{"location":"reference/checks-data-quality/#ci-integration","title":"CI integration","text":"<p>Checks produce two observable signals that are useful in CI and orchestration systems:</p> <ul> <li>Exit code</li> <li>Any hard failure (built-in bound violations or a contract returning <code>__action = 'fail'</code>) causes the job to exit with a non\u2011zero status.</li> <li>For physical tables (append/complete/update modes), failures also trigger an automatic rollback to the previous successful version.</li> <li> <p>Most CI systems will mark the pipeline as failed automatically on non\u2011zero exit codes.</p> </li> <li> <p>Log messages</p> </li> <li>Contracts should emit a single row with <code>__action</code> and <code>__message</code>. These are logged so you can surface them in CI logs and artifacts.</li> <li>When <code>__action = 'warning'</code>, the job continues and exits with code 0. Ensure your CI surfaces logs so warnings are visible in pull requests.</li> </ul> <p>Example contract outputs (see \u201cSQL contracts\u201d): <pre><code>-- Fails the job and rolls back (physical tables)\nselect 'fail'    as __action, 'Row count below threshold' as __message;\n\n-- Logs a warning; job continues\nselect 'warning' as __action, 'Null rate above target'    as __message;\n</code></pre></p> <p>Tips: - Prefer <code>post_run</code> for checks that must consider the final persisted state. - Use built-in bounds for simple invariants; reserve contracts for multi-table logic and nuanced policies. - If you need to gate on warnings, implement a small CI step that scans logs for known warning markers and decides policy for your repository.</p>"},{"location":"reference/checks-data-quality/#examples_1","title":"Examples","text":"<ul> <li>Working scenarios are available in the sample runtime:</li> <li>Browsable: Sample runtime</li> <li>Rich integration scenarios (repository layout):</li> <li><code>framework/tests/integration/runtime/gold/gold/</code></li> </ul>"},{"location":"reference/checks-data-quality/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze \u2022 Silver \u2022 Gold</li> <li>Table properties and physical layout: Table Options</li> <li>Custom logic integration: Extenders, UDFs &amp; Parsers</li> </ul>"},{"location":"reference/extenders-udfs-parsers/","title":"Extenders, UDFs, and Parsers","text":"<p>This reference explains how to extend Fabricks with custom Python code and reusable SQL assets: - Extenders: Python functions that transform a Spark DataFrame before it is written. - UDFs: User-defined functions you register on the Spark session and use in SQL. - Parsers: Source-specific readers/cleaners that return a DataFrame (optional, advanced).</p> <p>Use these to encapsulate business logic, reuse patterns, and keep SQL jobs focused and readable.</p>"},{"location":"reference/extenders-udfs-parsers/#extenders","title":"Extenders","text":"<p>Extenders are Python functions that take a DataFrame and return a transformed DataFrame. They are applied during job execution (typically in Silver/Gold) right before write.</p> <ul> <li>Location: Put Python modules under your runtime, for example <code>fabricks/extenders</code>.</li> <li>Referencing: In job YAML use <code>extender: name</code> and optionally <code>extender_options: { ... }</code> to pass arguments.</li> </ul> <p>Example (adds a <code>country</code> column):</p> <pre><code>from pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom fabricks.core.extenders import extender\n\n@extender(name=\"add_country\")\ndef add_country(df: DataFrame, **kwargs) -&gt; DataFrame:\n    return df.withColumn(\"country\", lit(kwargs.get(\"country\", \"Unknown\")))\n</code></pre> <p>In a job:</p> <pre><code>- job:\n    step: silver\n    topic: sales_analytics\n    item: daily_summary\n    options:\n      mode: update\n    extender: add_country\n    extender_options:\n      country: CH\n</code></pre> <p>Notes: - Extenders should be idempotent and fast. - Avoid heavy I/O; handle source reading in Bronze or via a Parser. - Prefer small, composable transformations.</p>"},{"location":"reference/extenders-udfs-parsers/#udfs","title":"UDFs","text":"<p>UDFs are registered on the Spark session and then callable from SQL. Place the UDF registration code in your runtime (e.g., <code>fabricks/udfs</code>), and ensure it runs before jobs that need it (for example during initialization or via a notebook hook).</p> <p>Example (simple addition UDF):</p> <pre><code>from pyspark.sql import SparkSession\nfrom fabricks.core.udfs import udf\n\n@udf(name=\"addition\")\ndef addition(spark: SparkSession):\n    def _add(a: int, b: int) -&gt; int:\n        return a + b\n    spark.udf.register(\"udf_addition\", _add)\n</code></pre> <p>Using built-in helpers and a custom UDF in SQL:</p> <pre><code>select\n  udf_identity(s.id, id) as __identity,\n  udf_key(array(s.id)) as __key,\n  s.id as id,\n  s.name as monarch,\n  udf_addition(1, 2) as addition\nfrom silver.monarch_scd1__current s\n</code></pre> <p>Notes: - UDFs should validate inputs and avoid side-effects. - Prefer Spark SQL built-ins and DataFrame functions where possible for performance.</p>"},{"location":"reference/extenders-udfs-parsers/#parsers","title":"Parsers","text":"<p>Parsers read and lightly clean raw data. They return a DataFrame and should not write output or mutate state.</p> <p>What a parser should do: - Read raw data from <code>data_path</code> (batch or stream based on <code>stream</code> flag) - Optionally apply/validate a schema from <code>schema_path</code> - Perform light, source-specific cleanup (drops/renames/type fixes) - Return a Spark DataFrame (no writes, no side effects)</p> <p>Inputs: - <code>data_path</code>: source location (directory or file) - <code>schema_path</code>: optional schema location (e.g., JSON/DDL) - <code>spark</code>: active <code>SparkSession</code> - <code>stream</code>: when true, prefer <code>readStream</code> where supported</p> <p>Reference in a job:</p> <pre><code>- job:\n    step: bronze\n    topic: demo\n    item: source\n    options:\n      mode: append\n      uri: /mnt/demo/raw/demo\n      parser: monarch\n    parser_options:\n      file_format: parquet\n      read_options:\n        mergeSchema: true\n</code></pre> <p>Example implementation:</p> <pre><code>from pyspark.sql import DataFrame, SparkSession\nfrom fabricks.core.parsers import parser\nfrom fabricks.utils.path import Path\n\n@parser(name=\"monarch\")\nclass MonarchParser:\n    def parse(self, data_path: Path, schema_path: Path, spark: SparkSession, stream: bool) -&gt; DataFrame:\n        df = spark.read.parquet(data_path.string)\n        cols_to_drop = [c for c in df.columns if c.startswith(\"BEL_\")]\n        if cols_to_drop:\n            df = df.drop(*cols_to_drop)\n        return df\n</code></pre> <pre><code>from pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom fabricks.core.extenders import extender\n\n@extender(name=\"add_country\")\ndef add_country(df: DataFrame, **kwargs) -&gt; DataFrame:\n    return df.withColumn(\"country\", lit(kwargs.get(\"country\")))\n</code></pre>"},{"location":"reference/extenders-udfs-parsers/#related","title":"Related","text":"<ul> <li>Steps: Bronze \u2022 Silver \u2022 Gold</li> <li>Reference: Checks &amp; Data Quality \u2022 Table Options</li> <li>Runtime: Runtime Configuration</li> </ul>"},{"location":"reference/table-options/","title":"Table Options","text":"<p>Table options control how Fabricks creates and manages physical Delta tables across steps. Use these options to define properties, layout, constraints, identity, retention, and more. Options are generally provided under <code>table_options</code> at the job level (and may also be set as step defaults in your runtime config).</p>"},{"location":"reference/table-options/#common-options","title":"Common options","text":"<ul> <li>identity: Enables a Delta identity column on the target table. If <code>identity: true</code>, the identity column is auto-created when the table is created.</li> <li>liquid_clustering: Enables Delta Liquid Clustering where supported by the Databricks runtime.</li> <li>partition_by: List of partition columns (e.g., <code>[date_id, country]</code>).</li> <li>zorder_by: Columns used for Z-Ordering (improves data skipping).</li> <li>cluster_by: Logical clustering keys (materialization depends on runtime features).</li> <li>bloomfilter_by: Columns to maintain Bloom filters on (where supported).</li> <li>constraints: Map of table constraints (e.g., <code>primary key(__key)</code>).</li> <li>properties: Arbitrary Delta table properties (key/value pairs).</li> <li>comment: Table comment/description.</li> <li>calculated_columns: Map of computed columns populated on write.</li> <li>retention_days: Table VACUUM retention period (days).</li> <li>powerbi: true to apply Power BI\u2013specific metadata (if supported).</li> </ul>"},{"location":"reference/table-options/#option-matrix-types-defaults-compatibility","title":"Option matrix (types \u2022 defaults \u2022 compatibility)","text":"Option Type Default Applies to modes Notes identity boolean false append, complete, update Creates a Delta identity column on table create. liquid_clustering boolean false append, complete, update Requires Databricks runtime support. partition_by array[string] \u2014 append, complete, update Low-cardinality columns recommended. zorder_by array[string] \u2014 append, complete, update Improves data skipping on frequently filtered columns. cluster_by array[string] \u2014 append, complete, update Logical clustering; materialization depends on runtime features. bloomfilter_by array[string] \u2014 append, complete, update Use selectively for point-lookups. constraints map[string,string] \u2014 append, complete, update E.g., <code>primary key(__key)</code>. properties map[string,string] \u2014 append, complete, update Arbitrary Delta table properties. comment string \u2014 append, complete, update Table description. calculated_columns map[string,string] \u2014 append, complete, update Computed at write time. retention_days integer \u2014 append, complete, update VACUUM retention (days). powerbi boolean false append, complete, update Applies Power BI\u2013specific metadata where supported. <p>Notes: - table_options are ignored in memory mode (view-only). - Defaults may also be set at step level in your runtime and overridden per job.</p>"},{"location":"reference/table-options/#minimal-example","title":"Minimal example","text":"<pre><code>- job:\n    step: gold\n    topic: fact\n    item: option\n    options:\n      mode: complete\n    table_options:\n      identity: true\n      cluster_by: [monarch]\n      properties:\n        delta.enableChangeDataFeed: true\n      comment: Strength lies in unity\n</code></pre>"},{"location":"reference/table-options/#extended-example","title":"Extended example","text":"<pre><code>- job:\n    step: gold\n    topic: sales\n    item: curated\n    options:\n      mode: update\n      change_data_capture: scd1\n    table_options:\n      identity: true\n      partition_by: [date_id]\n      zorder_by: [customer_id, product_id]\n      bloomfilter_by: [order_id]\n      constraints:\n        primary_key: \"primary key(__key)\"\n      properties:\n        delta.minReaderVersion: 2\n        delta.minWriterVersion: 5\n        delta.enableChangeDataFeed: true\n      comment: \"Curated sales model with CDC\"\n      retention_days: 30\n      calculated_columns:\n        ingestion_date: \"current_date()\"\n</code></pre>"},{"location":"reference/table-options/#notes-and-guidance","title":"Notes and guidance","text":"<ul> <li>Scope: <code>table_options</code> apply to physical table modes (<code>append</code>, <code>complete</code>, <code>update</code>). They do not apply to <code>memory</code> mode (view-only).</li> <li>Identity: When <code>identity: true</code> is enabled, the identity column is defined at create time - See Identity.</li> <li>Layout vs performance: Start with <code>partition_by</code> on low-cardinality columns; add <code>zorder_by</code> for frequently filtered high-cardinality columns. Use Bloom filters selectively for point-lookups.</li> <li>Defaults: You can specify step-level defaults in your runtime config and override them per job.</li> </ul>"},{"location":"reference/table-options/#related-topics","title":"Related topics","text":"<ul> <li>Steps: Bronze \u2022 Silver \u2022 Gold</li> <li>Data quality checks: Checks &amp; Data Quality</li> <li>Custom logic integration: Extenders, UDFs &amp; Parsers</li> <li>Runtime configuration: Runtime</li> </ul>"},{"location":"steps/bronze/","title":"Bronze Step Reference","text":"<p>The Bronze step ingests raw data from files/streams/sources into the Lakehouse.</p> <p>What it does - Reads from a URI using a parser (e.g., parquet) or custom parser - Optionally filters, calculates, or encrypts columns - Appends or registers data for downstream steps</p>"},{"location":"steps/bronze/#modes","title":"Modes","text":"Mode Description memory Register a temporary view only; no Delta table is written. append Append records to the target table (no merge/upsert). register Register an existing Delta table/view at <code>uri</code>."},{"location":"steps/bronze/#required-fields","title":"Required fields","text":"<ul> <li>memory: Requires <code>options.mode: memory</code>. Typically specify <code>uri</code> and <code>parser</code> unless your extender or custom parser produces the DataFrame. No Delta table is written; a temporary view is registered.</li> <li>append: Requires <code>options.mode: append</code> and a readable source (<code>uri</code> + <code>parser</code>). <code>keys</code> are optional but recommended for de-duplication and downstream CDC.</li> <li>register: Requires <code>options.mode: register</code> and <code>uri</code> pointing to an existing Delta table or view. <code>parser</code> is not used in this mode.</li> </ul> <p>Minimal example <pre><code>- job:\n    step: bronze\n    topic: demo\n    item: source\n    options:\n      mode: append\n      uri: /mnt/demo/raw/demo\n      parser: parquet\n      keys: [id]\n</code></pre></p> <p>Additional examples</p> <p>Memory (temporary view only) <pre><code>- job:\n    step: bronze\n    topic: demo\n    item: mem_view\n    options:\n      mode: memory\n      uri: /mnt/demo/raw/demo\n      parser: parquet\n      keys: [id]   # optional, useful for downstream CDC\n</code></pre></p> <p>Register an existing table/view <pre><code>- job:\n    step: bronze\n    topic: demo\n    item: register_source\n    options:\n      mode: register\n      uri: analytics.raw_demo   # existing Delta table or view name\n</code></pre></p>"},{"location":"steps/bronze/#bronze-options","title":"Bronze options","text":""},{"location":"steps/bronze/#option-matrix-types-defaults-required","title":"Option matrix (types \u2022 defaults \u2022 required)","text":"Option Type Default Required Description type enum: default, manual default no <code>manual</code> disables auto DDL/DML; you manage persistence yourself. mode enum: memory, append, register \u2014 yes Controls behavior: register temp view, append to table, or register an existing table/view. uri string (path or table) \u2014 cond. Source location or existing table/view (for <code>register</code>). Required for <code>append</code>/<code>register</code>. parser string \u2014 cond. Input parser (e.g., parquet, csv, json) or custom parser. Not used in <code>register</code> mode. source string \u2014 no Logical source label for lineage/logging. keys array[string] \u2014 no Business keys used for dedup and downstream CDC. Recommended. parents array[string] \u2014 no Upstream job dependencies to enforce ordering. filter_where string (SQL predicate) \u2014 no Row-level filter applied during ingestion. calculated_columns map[string]string \u2014 no New columns defined as SQL expressions evaluated at load time. encrypted_columns array[string] \u2014 no Columns to encrypt during write. extender string \u2014 no Python extender to transform the DataFrame (see Extenders). extender_options map[string,any] {} no Arguments passed to the extender. operation enum: upsert, reload, delete \u2014 no Changelog semantics for certain feeds. timeout integer (seconds) \u2014 no Per-job timeout; overrides step default. <p>Notes: - <code>uri</code> is required for <code>append</code> and <code>register</code>; optional for <code>memory</code> when a custom source is produced by an extender or custom parser. - <code>parser</code> is required when reading files/directories; it is not used in <code>register</code> mode. - <code>keys</code> are optional but recommended for de-duplication and downstream CDC.</p>"},{"location":"steps/bronze/#all-options-at-a-glance","title":"All Options at a glance","text":"Option Purpose type <code>default</code> vs <code>manual</code> (manual disables auto DDL/DML; you manage persistence). mode One of: <code>memory</code>, <code>append</code>, <code>register</code>. uri Source location (e.g., <code>/mnt/...</code>, <code>abfss://...</code>) used by the parser. parser Input parser (e.g., <code>parquet</code>) or the name of a custom parser. source Logical source label for lineage/logging. keys Business keys used for dedup and downstream CDC. parents Upstream jobs to enforce dependencies and ordering. filter_where SQL predicate applied during ingestion. encrypted_columns Columns to encrypt at write time. calculated_columns Derived columns defined as SQL expressions. extender Name of a Python extender to apply (see Extenders). extender_options Arguments for the extender (mapping). operation Changelog semantics for certain feeds: <code>upsert</code> | <code>reload</code> | <code>delete</code>. timeout Per-job timeout seconds (overrides step default)."},{"location":"steps/bronze/#bronze-dedicated-options","title":"Bronze dedicated options","text":"<ul> <li> <p>Core</p> <ul> <li>type: <code>default</code> vs <code>manual</code>. Manual means Fabricks will not auto-generate DDL/DML; you control persistence.</li> <li>mode: Controls ingestion behavior (<code>memory</code>, <code>append</code>, <code>register</code>).</li> <li>timeout: Per-job timeout seconds; overrides step defaults.</li> </ul> </li> <li> <p>Source</p> <ul> <li>uri: Filesystem or table/view location resolved by the parser.</li> <li>parser: Name of the parser to read the source (e.g., <code>parquet</code>) or a custom parser.</li> <li>source: Optional logical label used for lineage/logging.</li> </ul> </li> <li> <p>Dependencies &amp; ordering</p> <ul> <li>parents: Explicit upstream jobs that must complete before this job runs.</li> <li>keys: Natural/business keys used for deduplication and for downstream CDC.</li> </ul> </li> <li> <p>Transformations &amp; security</p> <ul> <li>filter_where: SQL predicate to filter rows during ingestion.</li> <li>calculated_columns: Mapping of new columns to SQL expressions evaluated at load time.</li> <li>encrypted_columns: List of columns to encrypt during write.</li> </ul> </li> <li> <p>Changelog semantics</p> <ul> <li>operation: For change-log style feeds, indicates whether incoming rows should be treated (<code>upsert</code>, <code>reload</code>, <code>delete</code>).</li> </ul> </li> </ul>"},{"location":"steps/bronze/#extensibility","title":"Extensibility","text":"<ul> <li>extender: Apply a Python extender to the ingested DataFrame.</li> <li>extender_options: Mapping of arguments passed to the extender.</li> <li>See also: Extenders, UDFs &amp; Views</li> </ul>"},{"location":"steps/bronze/#related","title":"Related","text":"<ul> <li>Next steps: Silver Step, Table Options</li> <li>Data quality: Checks &amp; Data Quality</li> <li>Extensibility: Extenders, UDFs &amp; Views</li> <li>Sample runtime: Sample runtime</li> </ul>"},{"location":"steps/gold/","title":"Gold Step Reference","text":"<p>Gold steps produce consumption-ready models, usually via SQL. Semantic applies table properties/metadata.</p>"},{"location":"steps/gold/#modes","title":"Modes","text":"Mode Description memory View-only result; no table is written. append Append rows to the target table. complete Full refresh/overwrite of the target table. update Merge/upsert semantics (typically used with CDC). invoke Run a notebook instead of SQL (configure via <code>invoker_options</code>). <p>See Gold CDC input fields below for required <code>__</code> columns when using SCD.</p> <p>Quick CDC overview - Strategies: <code>nocdc</code>, <code>scd1</code>, <code>scd2</code> - SCD1: uses soft-delete flags <code>__is_current</code>, <code>__is_deleted</code> - SCD2: uses validity windows <code>__valid_from</code>, <code>__valid_to</code> - Gold inputs and operations: <code>__operation</code> values are <code>'upsert' | 'delete' | 'reload'</code>. If omitted for SCD in <code>mode: update</code>, Fabricks injects <code>'reload'</code> and enables rectification automatically</p> <p>See the CDC reference for details and examples: Change Data Capture (CDC)</p>"},{"location":"steps/gold/#options-at-a-glance","title":"Options at a glance","text":"Option Purpose type <code>default</code> vs <code>manual</code> (manual: you manage persistence yourself). mode One of: <code>memory</code>, <code>append</code>, <code>complete</code>, <code>update</code>, <code>invoke</code>. change_data_capture CDC strategy: <code>nocdc</code> | <code>scd1</code> | <code>scd2</code>. update_where Additional predicate limiting updates during merges. parents Explicit upstream dependencies for scheduling/recomputation. deduplicate Drop duplicate keys in the result before writing. persist_last_timestamp Persist the last processed timestamp for incremental loads. correct_valid_from Adjust SCD2 timestamps that would otherwise start at a sentinel date. table Target table override (useful for semantic/table-copy scenarios). table_options Delta table options and metadata (identity, clustering, properties, comments). spark_options Per-job Spark session/SQL options mapping. udfs Path to UDFs registry to load before executing the job. check_options Configure DQ checks (pre_run, post_run, max_rows, min_rows, count_must_equal, skip). notebook Mark job to run as a notebook (used with <code>mode: invoke</code>). invoker_options Configure <code>pre_run</code> / <code>run</code> / <code>post_run</code> notebook execution and arguments. requirements If true, install/resolve additional requirements for this job. timeout Per-job timeout seconds (overrides step defaults)."},{"location":"steps/gold/#option-matrix-types-defaults-required","title":"Option matrix (types \u2022 defaults \u2022 required)","text":"Option Type Default Required Description type enum: default, manual default no <code>manual</code> disables auto DDL/DML; you manage persistence yourself. mode enum: memory, append, complete, update, invoke \u2014 yes Processing behavior. change_data_capture enum: nocdc, scd1, scd2 nocdc no CDC strategy applied when writing (effective in <code>mode: update</code>). update_where string (SQL predicate) \u2014 no Additional predicate to limit rows affected during merge/upsert. parents array[string] \u2014 no Upstream dependencies to enforce scheduling/order. deduplicate boolean false no Drop duplicate keys in the result prior to write. persist_last_timestamp boolean false no Persist last processed timestamp for incremental loads. correct_valid_from boolean \u2014 no Adjust SCD2 start timestamps when needed to avoid sentinel starts. table string \u2014 no Target table override (useful for semantic/table-copy scenarios). table_options map \u2014 no Delta table options/metadata (identity, clustering, properties, comments). spark_options map \u2014 no Per-job Spark session/SQL options. udfs string (path) \u2014 no Path to UDFs registry to load before executing the job. check_options map \u2014 no Data quality checks (see Checks &amp; Data Quality). notebook boolean \u2014 no Mark job to run as a notebook (used with <code>mode: invoke</code>). invoker_options map \u2014 no Configure <code>pre_run</code> / <code>run</code> / <code>post_run</code> notebook execution and arguments. requirements boolean false no Resolve/install additional requirements for this job. timeout integer (seconds) \u2014 no Per-job timeout; overrides step defaults."},{"location":"steps/gold/#field-reference","title":"Field reference","text":"<ul> <li> <p>Core</p> <ul> <li>type: <code>default</code> vs <code>manual</code>. Manual means Fabricks will not auto-generate DDL/DML; you control persistence.</li> <li>mode: Processing behavior (<code>memory</code>, <code>append</code>, <code>complete</code>, <code>update</code>, <code>invoke</code>).</li> <li>timeout: Per-job timeout seconds; overrides step defaults.</li> <li>requirements: Resolve/install additional requirements for this job.</li> </ul> </li> <li> <p>CDC</p> <ul> <li>change_data_capture: <code>nocdc</code>, <code>scd1</code>, or <code>scd2</code>. Governs merge semantics when <code>mode: update</code>.</li> </ul> </li> <li> <p>Incremental &amp; merge</p> <ul> <li>update_where: Predicate that constrains rows affected during merge/upsert.</li> <li>deduplicate: Drop duplicate keys prior to write.</li> <li>persist_last_timestamp: Persist last processed timestamp to support incremental loads.</li> <li>correct_valid_from: Adjusts start timestamps for SCD2 validity windows when needed.</li> </ul> </li> <li> <p>Dependencies &amp; targets</p> <ul> <li>parents: Explicit upstream jobs that must complete before this job runs.</li> <li>table: Target table override (commonly used when coordinating with Semantic/table-copy flows).</li> </ul> </li> <li> <p>Table &amp; Spark</p> <ul> <li>table_options: Delta table options and metadata (e.g., identity, clustering, properties, comments).</li> <li>spark_options: Per-job Spark session/SQL options mapping.</li> </ul> </li> <li>UDFs<ul> <li>udfs: Path to UDFs registry to load before executing the job.</li> </ul> </li> <li>Checks<ul> <li>check_options: Configure DQ checks (e.g., <code>pre_run</code>, <code>post_run</code>, <code>max_rows</code>, <code>min_rows</code>, <code>count_must_equal</code>, <code>skip</code>).</li> </ul> </li> <li>Notebook invocation<ul> <li>notebook: When coupled with <code>mode: invoke</code>, mark this job to run a notebook.</li> <li>invoker_options: Configure <code>pre_run</code>, <code>run</code>, and <code>post_run</code> notebooks and pass arguments.</li> </ul> </li> </ul> <p>Notes:</p> <ul> <li>Memory outputs ignore columns starting with <code>__</code> (e.g., <code>__it_should_not_be_found</code>).</li> </ul> <p>Minimal examples</p> <p>Gold SCD1 update <pre><code>- job:\n    step: gold\n    topic: scd1\n    item: update\n    options:\n      change_data_capture: scd1\n      mode: update\n</code></pre></p> <p>Gold full refresh <pre><code>- job:\n    step: gold\n    topic: demo\n    item: hello_world\n    options:\n      mode: complete\n</code></pre></p> <p>Scoped merge with update_where <pre><code>- job:\n    step: gold\n    topic: scd1\n    item: scoped_update\n    options:\n      mode: update\n      change_data_capture: scd1\n      update_where: \"region = 'EMEA'\"   # limit merge to a subset\n</code></pre></p> <p>Dependencies <pre><code>with cte as (select d.time, d.hour from gold.dim_time d)\nselect\n  udf_key(array(f.id, d.time)) as __key,\n  f.id as id,\n  f.monarch as monarch,\n  s.__source as role,\n  f.value as value,\n  d.time as time\nfrom cte d\ncross join transf.fact_memory f\nleft join silver.king_and_queen_scd1__current s on f.id = s.id\nwhere d.hour = 10\n</code></pre></p> <p>Invoke (notebooks) <pre><code>- job:\n    step: gold\n    topic: invoke\n    item: post_run\n    options: { mode: memory }\n    invoker_options:\n      post_run:\n          - notebook: gold/gold/invoke/post_run/exe\n          arguments: { arg1: 1, arg2: 2, arg3: 3 }\n- job:\n    step: gold\n    topic: invoke\n    item: notebook\n    options: { mode: invoke }\n    invoker_options:\n      run:\n        notebook: gold/gold/invoke/notebook\n        arguments: { arg1: 1, arg2: 2, arg3: 3 }\n</code></pre></p> <p>Checks and DQ <pre><code>- job:\n    step: gold\n    topic: check\n    item: max_rows\n    options: { mode: complete }\n    check_options: { max_rows: 2 }\n</code></pre></p> <p>Check SQL contracts <pre><code>-- fail.pre_run.sql\nselect \"fail\" as __action, \"Please don't fail on me :(\" as __message\n\n-- warning.post_run.sql\nselect \"I want you to warn me !\" as __message, \"warning\" as __action\n</code></pre></p> <p>Additional examples <pre><code># Fact with table options and Spark options\n- job:\n    step: gold\n    topic: fact\n    item: option\n    options:\n      mode: complete\n    table_options:\n      identity: true\n      liquid_clustering: true\n      cluster_by: [monarch]\n      properties:\n        country: Belgium\n      comment: Strength lies in unity\n    spark_options:\n      sql:\n        spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite: false\n        spark.databricks.delta.properties.defaults.enableChangeDataFeed: true\n</code></pre></p> <pre><code>-- order_duplicate.sql\nselect 1 as __key, 1 as dummy, 1 as __order_duplicate_by_desc\nunion all\nselect 1 as __key, 2 as dummy, 2 as __order_duplicate_by_desc\n</code></pre>"},{"location":"steps/gold/#gold-options","title":"Gold options","text":"<p>See Options at a glance and Field reference above.</p>"},{"location":"steps/gold/#cdc-input-fields-for-gold-jobs","title":"CDC input fields for gold jobs","text":"<ul> <li>scd2 (required): <code>__key</code>, <code>__timestamp</code>, <code>__operation</code> ('upsert'|'delete'|'reload').</li> <li>scd1 (required): <code>__key</code>; optional <code>__timestamp</code>/<code>__operation</code> ('upsert'|'delete'|'reload') for delete/rectify handling. If <code>__operation</code> is omitted for SCD in <code>mode: update</code>, Fabricks injects <code>'reload'</code> and enables rectification.</li> <li>Optional helpers: <code>__order_duplicate_by_asc</code> / <code>__order_duplicate_by_desc</code>, <code>__identity</code>.</li> <li>See: Change Data Capture (CDC) for full details and examples.</li> </ul>"},{"location":"steps/gold/#gold-jobs","title":"Gold jobs","text":"<ul> <li>SCD1/SCD2 with modes and options:</li> </ul> <pre><code>- job:\n    step: gold\n    topic: scd1\n    item: update\n    options:\n      change_data_capture: scd1\n      mode: update\n- job:\n    step: gold\n    topic: scd1\n    item: last_timestamp\n    options:\n      change_data_capture: scd1\n      mode: update\n      persist_last_timestamp: true\n- job:\n    step: gold\n    topic: scd2\n    item: correct_valid_from\n    options:\n      change_data_capture: scd2\n      mode: memory\n      correct_valid_from: false\n</code></pre> <p>Example SCD2 SQL using <code>__key</code>, <code>__timestamp</code>, <code>__operation</code>:</p> <p>Note: Credit \u2014 Temporal Snapshot Fact Table (SQLBits 2012). Recommended to watch to understand SCD2 modeling in Gold. </p> <p>Slides: Temporal Snapshot Fact Tables (slides)</p> <pre><code>with dates as (\n  select id as id, __valid_from as __timestamp, 'upsert' as __operation from silver.monarch_scd2 where __valid_from &gt; '1900-01-02'\n  union\n  select id as id, __valid_to as __timestamp, 'delete' as __operation from silver.monarch_scd2 where __is_deleted\n)\nselect\n  d.id as __key,\n  s.id as id,\n  s.name as monarch,\n  s.doubleField as value,\n  d.__operation,\n  if(d.__operation = 'delete', d.__timestamp + interval 1 second, d.__timestamp) as __timestamp\nfrom dates d\nleft join silver.monarch_scd2 s on d.id = s.id and d.__timestamp between s.__valid_from and s.__valid_to\n</code></pre>"},{"location":"steps/gold/#when-to-model-scd2-in-gold-different-grain","title":"When to model SCD2 in Gold (different grain)","text":"<ul> <li>Use SCD2 in Gold when the base SCD2 tables do not align with the required consumption grain, and you need to derive a slowly changing history at a coarser/different grain (e.g., roll up line-item SCD2 to order-level SCD2, or derive a customer segment SCD2 from underlying attribute histories).</li> </ul> <p>Example: roll up line-item SCD2 (silver.order_item_scd2) to order-level SCD2 (total_amount by order_id) <pre><code>- job:\n    step: gold\n    topic: order\n    item: total_amount_scd2\n    options:\n      mode: update\n      change_data_capture: scd2\n</code></pre></p> <pre><code>-- Derive Gold SCD2 change points (upserts/deletes) at the order_id grain\nwith change_points as (\n  select order_id as __key, __valid_from as __timestamp, 'upsert' as __operation\n  from silver.order_item_scd2\n  union\n  select order_id as __key, __valid_to as __timestamp, 'delete' as __operation\n  from silver.order_item_scd2\n  where __is_deleted\n  union\n  -- article-level price change boundaries that fall within the order_item validity window\n  select oi.order_id as __key, a.__valid_from as __timestamp, 'upsert' as __operation\n  from silver.order_item_scd2 oi\n  join silver.article_scd2 a\n    on oi.article_id = a.article_id\n    and a.__valid_from between oi.__valid_from and oi.__valid_to\n  union\n  select oi.order_id as __key, a.__valid_to as __timestamp, 'upsert' as __operation\n  from silver.order_item_scd2 oi\n  join silver.article_scd2 a\n    on oi.article_id = a.article_id\n    and a.__valid_to between oi.__valid_from and oi.__valid_to\n)\n-- Compute the order-level attributes as of each change point\nselect\n  cp.__key,\n  oh.customer_id as customer_id,\n  seg.segment as customer_segment,\n  sum(oi.amount) as total_amount,\n  cp.__operation,\n  if(cp.__operation = 'delete', cp.__timestamp + interval 1 second, cp.__timestamp) as __timestamp\nfrom change_points cp\nleft join silver.order_item_scd2 oi\n  on cp.__key = oi.order_id\n  and cp.__timestamp between oi.__valid_from and oi.__valid_to\nleft join silver.order_header oh\n  on cp.__key = oh.order_id\nleft join silver.customer_segment_scd2 seg\n  on oh.customer_id = seg.customer_id\n  and cp.__timestamp between seg.__valid_from and seg.__valid_to\n</code></pre> <p>Notes</p> <ul> <li>The change_points CTE promotes underlying SCD2 intervals to the desired Gold grain by emitting 'upsert' at each interval start and 'delete' at interval end (or for deleted keys).</li> <li>At each change point, compute the Gold attributes as-of that timestamp. Fabricks will render the SCD2 merge using <code>__key</code>, <code>__timestamp</code>, and <code>__operation</code>.</li> <li>Article/price changes: extra unions in change_points add <code>silver.article_scd2</code> validity boundaries (within each item window) so totals recompute when article prices change.</li> <li>Joining extra tables:</li> <li>Static/non-SCD2 tables (e.g., <code>silver.order_header</code>) can be joined directly on business keys (e.g., <code>order_id</code>).</li> <li>SCD2 dimensions (e.g., <code>silver.customer_segment_scd2</code>) must be joined as-of the change point using a validity-window predicate: <code>cp.__timestamp between seg.__valid_from and seg.__valid_to</code>.</li> <li>If your derived attribute can also disappear (e.g., no remaining items), the 'delete' operation correctly closes the last interval for that Gold key.</li> </ul>"},{"location":"steps/gold/#scd2-with-aggregation-example-composite-keys-order_id-customer_id","title":"SCD2 with Aggregation Example (composite keys: order_id \u00d7 customer_id)","text":"<p>This example demonstrates how to derive a Gold SCD2 stream with aggregation at a different grain than the source, by: - Computing change points (upsert at interval starts, delete at terminal interval ends) at the target business key grain. - Aggregating measures as of each change point over all source SCD2 segments that are valid at that timestamp. - Zeroing out measures at delete timestamps to correctly close intervals.</p> <pre><code>with\n    -- 1) Establish target business key for Gold grain (order_id \u00d7 customer_id)\n    source_with_business_key as (\n        select\n            concat_ws('*', order_id, customer_id) as composite_business_key,\n            *\n        from silver.order_item_scd2\n    ),\n\n    -- 2) Identify terminal segments at the target grain\n    --    For terminal segments we will emit a 'delete' change point at __valid_to.\n    scd2_segments as (\n        select\n            if(\n                coalesce(\n                    lead(composite_business_key) over (\n                        partition by __key\n                        order by __valid_from\n                    ),\n                    composite_business_key\n                ) = composite_business_key,\n                __is_deleted,\n                true\n            ) as is_terminal_segment,\n            *\n        from source_with_business_key\n    ),\n\n    -- 3) Build change points at the Gold grain:\n    --    - 'upsert' at each __valid_from\n    --    - 'delete' at __valid_to only for terminal segments\n    change_points as (\n        select\n            order_id,\n            customer_id,\n            __valid_from as change_timestamp,\n            'upsert' as change_op\n        from scd2_segments\n\n        union\n\n        select\n            order_id,\n            customer_id,\n            __valid_to as change_timestamp,\n            'delete' as change_op\n        from scd2_segments\n        where is_terminal_segment\n    ),\n\n    -- 4) Aggregate measures as of each change point over active segments\n    snapshot_aggregates as (\n        select\n            concat_ws('*', d.order_id, d.customer_id) as __key,\n            d.order_id,\n            d.customer_id,\n            d.change_timestamp as __timestamp,\n            'upsert' as __operation,\n\n            sum(\n                if(d.change_op = 'delete' and d.change_timestamp = r.__valid_to, 0, r.item_qty)\n            ) as total_item_qty,\n\n            sum(\n                if(d.change_op = 'delete' and d.change_timestamp = r.__valid_to, 0, r.item_amount)\n            ) as total_item_amount,\n\n            sum(\n                if(d.change_op = 'delete' and d.change_timestamp = r.__valid_to, 0, r.item_cost)\n            ) as total_item_cost,\n\n            sum(\n                if(d.change_op = 'delete' and d.change_timestamp = r.__valid_to, 0, r.pending_item_qty)\n            ) as pending_total_item_qty,\n\n            sum(\n                if(d.change_op = 'delete' and d.change_timestamp = r.__valid_to, 0, r.pending_item_amount)\n            ) as pending_total_item_amount,\n\n            sum(\n                if(d.change_op = 'delete' and d.change_timestamp = r.__valid_to, 0, r.pending_item_cost)\n            ) as pending_total_item_cost\n        from change_points d\n        inner join scd2_segments r\n            on d.change_timestamp between r.__valid_from and r.__valid_to\n           and d.customer_id = r.customer_id\n           and d.order_id = r.order_id\n        group by d.order_id, d.customer_id, d.change_timestamp, d.change_op\n    )\nselect *\nfrom snapshot_aggregates;\n</code></pre> <p>Notes - composite_business_key ensures a clear target-grain identity for reasoning about terminal segments. - is_terminal_segment marks final intervals at the target grain, causing a 'delete' change point at __valid_to. - At delete timestamps, measures are zeroed to close the interval cleanly and avoid double counting. - The output conforms to Gold SCD2 inputs: (__key, __timestamp, __operation), with __operation fixed to 'upsert' as Fabricks handles rectification in update mode. - Adjust function names (if/concat_ws) to your SQL engine equivalents if needed.</p> <ul> <li>Fact options: table options, clustering, properties, and Spark options:</li> </ul> <pre><code>- job:\n    step: gold\n    topic: fact\n    item: option\n    options:\n      mode: complete\n    table_options:\n      identity: true\n      liquid_clustering: true\n      cluster_by: [monarch]\n      properties:\n        country: Belgium\n      comment: Strength lies in unity\n    spark_options:\n      sql:\n        spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite: false\n        spark.databricks.delta.properties.defaults.enableChangeDataFeed: true\n</code></pre> <p>Example SQL for this job:</p> <pre><code>select id as id, name as monarch, doubleField as value from silver.monarch_scd1__current\n</code></pre> <ul> <li>Manual, memory, append, overwrite:</li> </ul> <pre><code>- job: { step: gold, topic: fact, item: manual, options: { type: manual, mode: complete } }\n- job: { step: gold, topic: fact, item: memory, options: { mode: memory } }\n- job: { step: gold, topic: fact, item: append, options: { mode: append } }\n- job: { step: gold, topic: dim, item: overwrite, options: { change_data_capture: scd1, mode: update }, table_options: { identity: true } }\n</code></pre> <p>Memory outputs ignore columns starting with <code>__</code> (e.g., <code>__it_should_not_be_found</code>).</p> <ul> <li>Deduplicate and order-by-duplicate examples:</li> </ul> <pre><code>- job:\n    step: gold\n    topic: fact\n    item: deduplicate\n    options:\n      mode: complete\n      deduplicate: true\n</code></pre> <pre><code>-- deduplicate.sql\nselect 1 as __key, 2 as dummy\nunion all\nselect 1 as __key, 1 as dummy\n</code></pre> <pre><code>-- order_duplicate.sql\nselect 1 as __key, 1 as dummy, 1 as __order_duplicate_by_desc\nunion all\nselect 1 as __key, 2 as dummy, 2 as __order_duplicate_by_desc\n</code></pre>"},{"location":"steps/gold/#scd2-header-line-change-points-with-header-closure-order_id-order_line_id","title":"SCD2 header-line change points with header closure (order_id \u00d7 order_line_id)","text":"<p>This example shows how to generate SCD2 change points when combining header and line SCD2 sources, including: - Emitting upserts at each validity start across header and lines. - Emitting a delete for the header-only row as soon as a corresponding line exists (to close the sentinel header row). - Emitting deletes at validity end for deleted intervals. - Joining an additional SCD2 stream (e.g., item price) to ensure change points also reflect price boundaries.</p> <pre><code>with\n  -- 1) Gather validity boundaries across header, line, and related SCD2 (price)\n  hdr_line_dates as (\n    select h.order_id, l.order_line_id, h.__valid_to, h.__valid_from, h.__is_deleted\n    from silver.order_header_scd2 h\n    left join silver.order_line_scd2 l\n      on h.order_id = l.order_id\n      and h.__valid_from between l.__valid_from and l.__valid_to\n\n    union\n\n    select order_id, order_line_id, __valid_to, __valid_from, __is_deleted\n    from silver.order_line_scd2\n\n    union\n\n    -- ensure we have entries for the latest price window intersecting current header/line\n    select h.order_id, l.order_line_id, ip.__valid_to, ip.__valid_from, false\n    from silver.order_header_scd2__current h\n    inner join silver.order_line_scd2__current l\n      on h.order_id = l.order_id\n    inner join silver.item_price_scd2 ip\n      on ip.item_id = l.item_id\n     and ip.price_list_id = h.price_list_id\n     and ip.__is_current\n  ),\n\n  -- 2) Build change points\n  change_points as (\n    -- upsert at each validity start\n    select order_id, order_line_id, __valid_from as __timestamp, 'upsert' as __operation\n    from hdr_line_dates\n\n    union\n\n    -- as soon as a line exists, close the header-only row (order_line_id is NULL sentinel)\n    select order_id, null as order_line_id, __valid_from as __timestamp, 'delete' as __operation\n    from hdr_line_dates\n    where order_line_id is not null\n\n    union\n\n    -- delete at validity end for deleted intervals\n    select order_id, order_line_id, __valid_to as __timestamp, 'delete' as __operation\n    from hdr_line_dates\n    where __is_deleted\n  )\n\n-- 3) Project attributes as of each change point\nselect\n  concat_ws('*', p.order_id, coalesce(p.order_line_id, -1)) as __key,\n  p.order_id,\n  p.order_line_id,\n\n  -- header attributes (examples)\n  h.customer_id,\n  h.price_list_id,\n  h.order_date,\n\n  -- line attributes (examples)\n  l.item_id,\n  l.quantity,\n  l.unit_price,\n\n  -- CDC fields\n  p.__operation,\n  p.__timestamp\nfrom change_points p\nleft join silver.order_header_scd2 h\n  on p.order_id = h.order_id\n and p.__timestamp between h.__valid_from and h.__valid_to\nleft join silver.order_line_scd2 l\n  on p.order_id = l.order_id\n and coalesce(p.order_line_id, -1) = coalesce(l.order_line_id, -1)\n and p.__timestamp between l.__valid_from and l.__valid_to\n</code></pre> <p>Notes - The null-sentinel header row (order_line_id = null -&gt; coerced to -1 in __key) is closed via a 'delete' when any line appears at the same boundary. - Additional SCD2 inputs (e.g., item_price_scd2) can be unioned into the dates set to force change points whenever related dimensions change. - The output stream conforms to Gold SCD2 input fields: __key, __timestamp, __operation.</p>"},{"location":"steps/gold/#related","title":"Related","text":"<ul> <li>Next steps: Table Options</li> <li>Data quality: Checks &amp; Data Quality</li> <li>Extensibility: Extenders, UDFs &amp; Views</li> <li>Sample runtime: Sample runtime</li> </ul>"},{"location":"steps/silver/","title":"Silver Step Reference","text":"<p>The Silver step standardizes and enriches data, and applies CDC (SCD1/SCD2) if configured.</p>"},{"location":"steps/silver/#modes","title":"Modes","text":"Mode Description memory View-only; no table written. append Append-only table. latest Keep only the latest row per key within the batch. update Merge/upsert semantics; typically used with CDC. combine Combine/union outputs from parent jobs into one result."},{"location":"steps/silver/#behavior-notes","title":"Behavior notes","text":"<ul> <li>Deduplication and ordering</li> <li><code>deduplicate</code> drops duplicate keys within the current batch.</li> <li>If <code>order_duplicate_by</code> is provided, rows are ordered by the specified sort before deduplication; the first row per key is kept.</li> <li> <p>In <code>latest</code> mode, only within-batch keys are considered (historical rows are not consulted). In <code>update</code> mode, winners are merged against the existing table.</p> </li> <li> <p>Streaming constraints</p> </li> <li><code>stream: true</code> is supported where your connectors/environment support streaming.</li> <li>In <code>append</code> mode it appends micro-batches; in <code>latest</code>/<code>update</code> modes semantics rely on deterministic keys and, if used, stable ordering columns.</li> <li> <p>With streaming, <code>pre_run</code> counts can be zero; prefer <code>post_run</code> checks for row count bounds.</p> </li> <li> <p>Combine behavior</p> </li> <li><code>combine</code> mode unions all parent outputs into a single logical result.</li> <li>Parents should be schema-compatible; columns are aligned by name. No de-duplication is performed unless <code>deduplicate</code> is enabled.</li> </ul>"},{"location":"steps/silver/#cdc-strategies","title":"CDC strategies","text":"Strategy What it does Convenience views nocdc No CDC flags; writes the result as-is. \u2014 scd1 Tracks current vs deleted; adds flags <code>__is_current</code>, <code>__is_deleted</code>. <code>TABLE__current</code> scd2 Maintains validity windows with <code>__valid_from</code>, <code>__valid_to</code> and historical rows. <code>TABLE__current</code>"},{"location":"steps/silver/#options-at-a-glance","title":"Options at a glance","text":"Option Purpose type <code>default</code> vs <code>manual</code> (manual: you manage persistence yourself). mode One of the modes listed above. change_data_capture CDC strategy: <code>nocdc</code> | <code>scd1</code> | <code>scd2</code>. parents Upstream job identifiers (e.g., <code>bronze.demo_source</code>). filter_where SQL predicate applied at transform time. deduplicate Drop duplicate keys within the batch. stream Enable streaming semantics where supported. order_duplicate_by Choose preferred row when duplicates exist (e.g., <code>{ order_by: desc }</code>). timeout Per-job timeout seconds (overrides step default). extender Name of a Python extender to apply (see Extenders). extender_options Arguments for the extender (mapping). check_options Configure DQ checks (pre_run, post_run, max_rows, min_rows, count_must_equal, skip)."},{"location":"steps/silver/#option-matrix-types-defaults-required","title":"Option matrix (types \u2022 defaults \u2022 required)","text":"Option Type Default Required Description type enum: default, manual default no <code>manual</code> disables auto DDL/DML; you manage persistence yourself. mode enum: memory, append, latest, update, combine \u2014 yes Processing behavior. change_data_capture enum: nocdc, scd1, scd2 nocdc no CDC strategy applied when writing. parents array[string] \u2014 no Upstream job identifiers to enforce ordering. filter_where string (SQL predicate) \u2014 no Predicate applied at transform time. deduplicate boolean false no Drop duplicate keys within the current batch. order_duplicate_by map (e.g., { columns: [ts], order_by: desc }) \u2014 no Choose preferred row for duplicates before <code>deduplicate</code>. stream boolean false no Enable streaming semantics where supported. timeout integer (seconds) \u2014 no Per-job timeout; overrides step default. extender string \u2014 no Python extender to transform the DataFrame. extender_options map[string,any] {} no Arguments passed to the extender. check_options map \u2014 no Data quality checks (see Checks &amp; Data Quality). <p>Notes: - If both <code>order_duplicate_by</code> and <code>deduplicate</code> are set, ordering is applied first, then the first row per key is retained. - <code>latest</code> considers only the current batch; use <code>update</code> if you need to merge winners against an existing table.</p>"},{"location":"steps/silver/#field-reference","title":"Field reference","text":"<ul> <li> <p>Core</p> <ul> <li>type: <code>default</code> vs <code>manual</code>. Manual means Fabricks won\u2019t auto-generate DDL/DML; you control persistence.</li> <li>mode: Processing behavior (<code>memory</code>, <code>append</code>, <code>latest</code>, <code>update</code>, <code>combine</code>).</li> <li>timeout: Per-job timeout; overrides step defaults.</li> </ul> </li> <li> <p>CDC</p> <ul> <li>change_data_capture: <code>nocdc</code>, <code>scd1</code>, or <code>scd2</code>.</li> <li>scd1 flags: <code>__is_current</code>, <code>__is_deleted</code> and a <code>{table}__current</code> convenience view.</li> <li>scd2 windows: <code>__valid_from</code>, <code>__valid_to</code> and a <code>{table}__current</code> convenience view.</li> </ul> </li> <li> <p>Dependencies &amp; ordering</p> <ul> <li>parents: Explicit upstream jobs (e.g., <code>bronze.topic_item</code>).</li> <li>deduplicate: Drop duplicate keys within the batch.</li> <li>order_duplicate_by: Pick a preferred row when duplicates exist (e.g., <code>{ order_by: desc }</code>).</li> </ul> </li> <li> <p>Streaming</p> <ul> <li>stream: Where supported, use streaming semantics for incremental processing.</li> </ul> </li> <li> <p>Extensibility</p> <ul> <li>extender: Apply a Python extender to the DataFrame.</li> <li>extender_options: Mapping of arguments passed to the extender.</li> </ul> </li> <li> <p>See also: Extenders, UDFs &amp; Views</p> </li> <li> <p>Checks</p> </li> <li>check_options: Configure DQ checks (e.g., <code>pre_run</code>, <code>post_run</code>, <code>max_rows</code>, <code>min_rows</code>, <code>count_must_equal</code>, <code>skip</code>). See Checks &amp; Data Quality</li> </ul> <p>Minimal example <pre><code>- job:\n    step: silver\n    topic: demo\n    item: scd1\n    options:\n      mode: update\n      change_data_capture: scd1\n      parents: [bronze.demo_source]\n</code></pre></p>"},{"location":"steps/silver/#silver-options","title":"Silver options","text":"<p>See Options at a glance and Field reference above.</p>"},{"location":"steps/silver/#silver-jobs","title":"Silver jobs","text":"<ul> <li> <p>Combine outputs example:   <pre><code>- job:\n    step: silver\n    topic: princess\n    item: combine\n    options:\n      mode: combine\n      parents:\n        - silver.princess_extend\n        - silver.princess_latest\n</code></pre></p> </li> <li> <p>Monarchs with SCD1/SCD2 and delta/memory flavors:</p> </li> </ul> <pre><code>- job:\n    step: silver\n    topic: monarch\n    item: scd1\n    options:\n      mode: update\n      change_data_capture: scd1\n- job:\n    step: silver\n    topic: monarch\n    item: scd2\n    options:\n      mode: update\n      change_data_capture: scd2\n- job:\n    step: silver\n    topic: monarch\n    item: delta\n    options:\n      mode: update\n      change_data_capture: scd2\n- job:\n    step: silver\n    topic: monarch\n    item: memory\n    options:\n      mode: memory\n      change_data_capture: nocdc\n</code></pre> <ul> <li>Multi-parent example (kings and queens):</li> </ul> <pre><code>- job:\n    step: silver\n    topic: king_and_queen\n    item: scd1\n    options:\n      mode: update\n      change_data_capture: scd1\n      parents: [bronze.queen_scd1, bronze.king_scd1]\n</code></pre> <ul> <li>Princess patterns: latest, append, calculated columns, duplicate ordering, manual type, schema drift, extend with extenders:</li> </ul> <pre><code>- job:\n    step: silver\n    topic: princess\n    item: latest\n    options:\n      mode: latest\n- job:\n    step: silver\n    topic: princess\n    item: append\n    options:\n      mode: append\n- job:\n    step: silver\n    topic: princess\n    item: order_duplicate\n    options:\n      mode: update\n      change_data_capture: scd1\n      order_duplicate_by:\n        order_by: desc\n- job:\n    step: silver\n    topic: princess\n    item: calculated_column\n    options:\n      mode: update\n      change_data_capture: scd1\n      order_duplicate_by:\n        order_by: asc\n- job:\n    step: silver\n    topic: princess\n    item: manual\n    options:\n      type: manual\n      mode: update\n      change_data_capture: scd1\n- job:\n    step: silver\n    topic: princess\n    item: extend\n    options:\n      mode: update\n      change_data_capture: scd1\n    extender_options:\n        - extender: add_country\n        arguments:\n          country: Belgium\n</code></pre> <ul> <li>Quality gate with <code>check_options</code>:   <pre><code>- job:\n    step: silver\n    topic: princess\n    item: quality_gate\n    options:\n      mode: update\n      change_data_capture: scd1\n      parents: [silver.princess_latest]\n    check_options:\n      post_run: true\n      min_rows: 1\n      max_rows: 100000\n</code></pre></li> </ul>"},{"location":"steps/silver/#related","title":"Related","text":"<ul> <li>Next steps: Gold Step, Table Options</li> <li>Data quality: Checks &amp; Data Quality</li> <li>Extensibility: Extenders, UDFs &amp; Views</li> <li>Sample runtime: Sample runtime</li> </ul> <p>Special characters in column names are preserved (see <code>prince.special_char</code>).</p>"}]}