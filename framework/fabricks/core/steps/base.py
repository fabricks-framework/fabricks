import logging
from typing import List, Optional, Union, cast

from pyspark.sql import DataFrame
from pyspark.sql.functions import expr, md5
from pyspark.sql.types import Row

from fabricks.cdc import SCD1
from fabricks.context import CONF_RUNTIME, LOGLEVEL, PATHS_RUNTIME, PATHS_STORAGE, SPARK, STEPS
from fabricks.context.log import DEFAULT_LOGGER
from fabricks.core.jobs.base._types import Bronzes, Golds, Silvers, TStep
from fabricks.core.jobs.get_job import get_job
from fabricks.core.steps._types import Timeouts
from fabricks.core.steps.get_step_conf import get_step_conf
from fabricks.metastore.database import Database
from fabricks.metastore.table import Table
from fabricks.utils.helpers import concat_dfs, run_in_parallel
from fabricks.utils.read.read_yaml import read_yaml
from fabricks.utils.schema import get_schema_for_type


class BaseStep:
    def __init__(self, step: Union[TStep, str]):
        self.name = cast(str, step)

        if self.name in Bronzes:
            self.expand = "bronze"
        elif self.name in Silvers:
            self.expand = "silver"
        elif self.name in Golds:
            self.expand = "gold"

        else:
            raise ValueError(self.name, "does not expand a default step")

        _storage = PATHS_STORAGE.get(self.name)
        assert _storage
        _runtime = PATHS_RUNTIME.get(self.name)
        assert _runtime

        self.spark = SPARK
        self.storage = _storage
        self.runtime = _runtime
        self.database = Database(self.name)

    _conf: Optional[dict] = None
    _options: Optional[dict] = None

    _workers: Optional[int] = None
    _timeouts: Optional[Timeouts] = None

    @property
    def workers(self):
        if not self._workers:
            w = self.options.get("workers")
            if w is None:
                w = CONF_RUNTIME.get("options", {}).get("workers")
            assert w is not None
            self._workers = cast(int, w)

        return self._workers

    def _get_timeout(self, what: str) -> int:
        t = self.options.get("timeouts", {}).get(what, None)
        if t is None:
            t = CONF_RUNTIME.get("options", {}).get("timeouts", {}).get(what)
        assert t is not None

        return int(t)

    @property
    def timeouts(self) -> Timeouts:
        if not self._timeouts:
            self._timeouts = Timeouts(
                job=self._get_timeout("job"),
                step=self._get_timeout("step"),
            )

        return self._timeouts

    @property
    def conf(self) -> dict:
        if not self._conf:
            _conf = [s for s in STEPS if s.get("name") == self.name][0]
            assert _conf is not None
            self._conf = cast(dict[str, str], _conf)

        return self._conf

    @property
    def options(self) -> dict:
        if not self._options:
            o = self.conf.get("options")
            assert o is not None
            self._options = cast(dict[str, str], o)

        return self._options

    def drop(self):
        DEFAULT_LOGGER.warning("ðŸ’£ (drop)", extra={"step": self})

        fs = self.database.storage
        assert fs

        tmp = fs.join("tmp")
        if tmp.exists():
            tmp.rm()

        checkpoint = fs.join("checkpoints")
        if checkpoint.exists():
            checkpoint.rm()

        schema = fs.join("schemas")
        if schema.exists():
            schema.rm()

        for t in ["jobs", "tables", "dependencies", "views"]:
            tbl = Table("fabricks", self.name, t)
            tbl.drop()

        self.database.drop()

    def create(self):
        DEFAULT_LOGGER.info("ðŸŒŸ (create)", extra={"step": self})

        if not self.runtime.exists():
            DEFAULT_LOGGER.warning(f"{self.name} not found in runtime ({self.runtime})")
        else:
            self.update()

    def update(self, update_dependencies: Optional[bool] = True, progress_bar: Optional[bool] = False):
        if not self.runtime.exists():
            DEFAULT_LOGGER.warning(f"{self.name} not found in runtime ({self.runtime})")

        else:
            if not self.database.exists():
                self.database.create()

            self.update_jobs()
            self.create_jobs()

            if update_dependencies:
                self.update_dependencies(progress_bar=progress_bar)

            self.update_tables()
            self.update_views()

    def get_dependencies(
        self,
        progress_bar: Optional[bool] = False,
        topic: Optional[Union[str, List[str]]] = None,
    ) -> Optional[DataFrame]:
        DEFAULT_LOGGER.debug("get dependencies", extra={"step": self})

        errors = []

        def _get_dependencies(row: Row):
            job = get_job(step=self.name, job_id=row["job_id"])
            try:
                df = job.get_dependencies()
                return df

            except Exception as e:  # noqa E722
                DEFAULT_LOGGER.exception("failed to get dependencies", extra={"job": job})
                errors.append(job)

        job_df = self.get_jobs()
        if topic and job_df:
            if isinstance(topic, str):
                topic = [topic]

            where = ", ".join([f"'{t}'" for t in topic])
            DEFAULT_LOGGER.debug(f"where topic in {where}", extra={"step": self})
            job_df = job_df.where(f"topic in ({where})")

        if job_df:
            job_df = job_df.where("not options.type <=> 'manual'")

            DEFAULT_LOGGER.setLevel(logging.CRITICAL)
            dfs = run_in_parallel(_get_dependencies, job_df, workers=16, progress_bar=progress_bar)
            DEFAULT_LOGGER.setLevel(LOGLEVEL)

            for e in errors:
                DEFAULT_LOGGER.error("failed to get dependencies", extra={"step": e})

            if dfs:
                df = concat_dfs(dfs)
                return df

    def get_jobs_iter(self, topic: Optional[str] = None):
        return read_yaml(self.runtime, root="job", prio_file_name=topic)

    def get_jobs(self, topic: Optional[str] = None) -> Optional[DataFrame]:
        DEFAULT_LOGGER.debug("get jobs", extra={"step": self})

        try:
            conf = get_step_conf(self.name)
            schema = get_schema_for_type(conf)

            df = SPARK.createDataFrame(read_yaml(self.runtime, root="job", prio_file_name=topic), schema=schema)  # type: ignore

            df = df.withColumn("job_id", md5(expr("concat(step, '.' ,topic, '_', item)")))

            duplicated_df = df.groupBy("job_id", "step", "topic", "item").count().where("count > 1")
            duplicates = ",".join(f"{row.step}.{row.topic}_{row.item}" for row in duplicated_df.collect())
            assert duplicated_df.isEmpty(), f"duplicated job(s) ({duplicates})"

            return df if not df.isEmpty() else None

        except AssertionError as e:
            DEFAULT_LOGGER.exception("failed to get jobs", extra={"step": self})
            raise e

    def create_jobs(self, retry: Optional[bool] = True):
        DEFAULT_LOGGER.info("create jobs", extra={"step": self})

        errors = []

        def _create_job(row: Row):
            job = get_job(step=self.name, job_id=row["job_id"])
            try:
                job.create()
            except:  # noqa E722
                DEFAULT_LOGGER.exception("not created", extra={"job": self})
                errors.append(job)

        df = self.get_jobs()
        table_df = self.database.get_tables()
        view_df = self.database.get_views()

        if df:
            if table_df:
                table_df = table_df.withColumn("job_id", expr("md5(table)"))
                df = df.join(table_df, "job_id", how="left_anti")
            if view_df:
                view_df = view_df.withColumn("job_id", expr("md5(view)"))
                df = df.join(view_df, "job_id", how="left_anti")

            DEFAULT_LOGGER.setLevel(logging.CRITICAL)
            run_in_parallel(_create_job, df, workers=16, progress_bar=True)
            DEFAULT_LOGGER.setLevel(LOGLEVEL)

            if errors:
                for e in errors:
                    DEFAULT_LOGGER.error("not created", extra={"job": e})

                if retry:
                    DEFAULT_LOGGER.warning("retry create jobs", extra={"step": self})
                    self.update_tables()
                    self.update_views()

                    self.create_jobs(retry=False)

                else:
                    DEFAULT_LOGGER.warning("retry failed", extra={"step": self})

        else:
            DEFAULT_LOGGER.debug("no new job", extra={"step": self})

    def update_jobs(self, drop: Optional[bool] = False):
        df = self.get_jobs()
        if df:
            DEFAULT_LOGGER.info("update jobs", extra={"step": self})
            if drop:
                SCD1("fabricks", self.name, "jobs").table.drop()

            SCD1("fabricks", self.name, "jobs").delete_missing(df, keys=["job_id"])

        else:
            DEFAULT_LOGGER.debug("no job", extra={"step": self})

    def update_tables(self):
        df = self.database.get_tables()
        if df:
            DEFAULT_LOGGER.info("update tables", extra={"step": self})
            df = df.withColumn("job_id", expr("md5(table)"))
            SCD1("fabricks", self.name, "tables").delete_missing(df, keys=["job_id"])

        else:
            DEFAULT_LOGGER.debug("no table", extra={"step": self})

    def update_views(self):
        df = self.database.get_views()

        DEFAULT_LOGGER.info("update views", extra={"step": self})
        df = df.withColumn("job_id", expr("md5(view)"))
        SCD1("fabricks", self.name, "views").delete_missing(df, keys=["job_id"])

    def update_dependencies(self, progress_bar: Optional[bool] = False, topic: Optional[Union[str, List[str]]] = None):
        df = self.get_dependencies(progress_bar=progress_bar, topic=topic)
        if df:
            DEFAULT_LOGGER.info("update dependencies", extra={"step": self})
            df.cache()

            if topic is None:
                SCD1("fabricks", self.name, "dependencies").delete_missing(df, keys=["dependency_id"])
            else:
                if isinstance(topic, str):
                    topic = [topic]

                where = ", ".join([f"'{t}'" for t in topic])
                DEFAULT_LOGGER.debug(f"where topic in {where}", extra={"step": self})

                SCD1("fabricks", self.name, "dependencies").delete_missing(
                    df,
                    keys=["dependency_id"],
                    update_where=f"topic in ({where})",
                    uuid=True,
                )

        else:
            DEFAULT_LOGGER.debug("no dependency", extra={"step": self})

    def register(self, update: Optional[bool] = False, drop: Optional[bool] = False):
        def _register(row: Row):
            job = get_job(step=self.name, topic=row["topic"], item=row["item"])
            job.register()

        if drop:
            SPARK.sql(f"drop database if exists {self.name} cascade ")
            SPARK.sql(f"create database {self.name}")

        if update:
            self.update_jobs()

        df = self.get_jobs()
        if df:
            table_df = self.database.get_tables()
            if table_df:
                df = df.join(table_df, "job_id", how="left_anti")

        if df:
            DEFAULT_LOGGER.setLevel(logging.CRITICAL)
            run_in_parallel(_register, df, workers=16, progress_bar=True)
            DEFAULT_LOGGER.setLevel(LOGLEVEL)

    def __str__(self):
        return self.name
